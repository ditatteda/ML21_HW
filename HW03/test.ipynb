{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2nd\n",
      "1\n",
      "2\n",
      "title Harry Poter\n",
      "price 0.3\n",
      "h 3\n",
      "w 2\n"
     ]
    }
   ],
   "source": [
    "double_dict = {\n",
    "    \"paras\" : {\n",
    "        \"h\" : 3,\n",
    "        \"w\" : 2\n",
    "    }\n",
    "}\n",
    "\n",
    "def f(*args):\n",
    "    for arg in args:\n",
    "        print(arg)\n",
    "\n",
    "f(1)\n",
    "f(\"2nd\",1,2)\n",
    "\n",
    "def g(**kwargs):\n",
    "    for k in kwargs:\n",
    "        print(k, kwargs[k])\n",
    "\n",
    "test_dict = {\n",
    "    \"title\" : \"Harry Poter\",\n",
    "    \"price\" : 0.3\n",
    "}\n",
    "\n",
    "g(**test_dict)\n",
    "g(**double_dict[\"paras\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2., 3.],\n",
      "         [4., 5., 6.],\n",
      "         [7., 8., 9.]]])\n",
      "tensor([[[-3.2907]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "model = torch.nn.Conv2d(1, 1, 3, 1, padding=0)\n",
    "inputs = torch.Tensor([[[1,2,3],[4,5,6],[7,8,9]]])\n",
    "print(inputs)\n",
    "print(model(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.Size([1, 2, 3]) tensor([[[1., 2., 3.],\n",
      "         [4., 5., 6.]]])\n",
      "torch.float32 torch.Size([1, 6]) tensor([[1., 2., 3., 4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.Tensor([[[1,2,3], [4,5,6]]])\n",
    "print(t.dtype, t.shape, t)\n",
    "\n",
    "t_ = t.flatten(1)\n",
    "print(t_.dtype, t_.shape, t_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class A constructed\n",
      "<class '__main__.A'> <__main__.A object at 0x12e6c2f80>\n",
      "hey\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/complexa/ML2021/ML21_HW/HW03/test.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/complexa/ML2021/ML21_HW/HW03/test.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(a\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/complexa/ML2021/ML21_HW/HW03/test.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m c \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/complexa/ML2021/ML21_HW/HW03/test.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m c\u001b[39m.\u001b[39;49mdevice \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mho\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/complexa/ML2021/ML21_HW/HW03/test.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(c)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "class A:\n",
    "    def __init__(self):\n",
    "        print(\"class A constructed\")\n",
    "\n",
    "a = A()\n",
    "a.device = \"hey\"\n",
    "\n",
    "print(A, a)\n",
    "print(a.device)\n",
    "\n",
    "c = [1, 2]\n",
    "c.device = \"ho\"\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.7390, -0.3980, -0.1157],\n",
      "        [ 0.1020, -0.6843, -0.8908],\n",
      "        [-0.4062, -0.4279, -1.7917],\n",
      "        [-0.8333,  0.7010,  1.1794],\n",
      "        [-0.7411,  1.7690,  1.4698],\n",
      "        [-0.5558,  1.4585, -0.5355],\n",
      "        [ 0.4470,  0.2723, -0.8688],\n",
      "        [-0.5657, -0.9404,  1.1317],\n",
      "        [ 0.5942,  1.1422,  1.7809],\n",
      "        [ 0.6076,  0.0473,  1.7281]])\n",
      "tensor([False, False, False,  True,  True,  True, False,  True,  True,  True])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.randn((10, 3))\n",
    "print(a)\n",
    "\n",
    "b = (a.argmax(dim=-1) >= 1)\n",
    "print(b)\n",
    "\n",
    "c = torch.zeros((10, 3))\n",
    "print(c)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 2., 3., 4.],\n",
      "        [5., 6., 7., 8.]])\n",
      "[5. 8.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6., 9.], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.Tensor([[5, 2, 3, 4], [5, 6, 7, 8]])\n",
    "print(a)\n",
    "b = a.argmax(dim=-1).tolist()\n",
    "c = a.max(dim=-1)\n",
    "d = {\"value\" : c[0], \"index\" : c[1]}\n",
    "\n",
    "e = d[\"value\"].numpy()\n",
    "\n",
    "print(e)\n",
    "e += 1\n",
    "e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (\"aaa\", 1)\n",
    "print(type(a))\n",
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/complexa/anaconda3/envs/complexa_ML21_3a/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8a245932194e17a7df91dc4217b4f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('./food-11/training/unlabeled/00/0000.jpg', './food-11/training/unlabeled/00/0001.jpg', './food-11/training/unlabeled/00/0002.jpg')\n",
      "tensor([0, 0, 0])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b968071c0aa4bc8880d2d3a09a8d4c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('./food-11/training/unlabeled/00/0000.jpg',)\n",
      "tensor([999])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import ConcatDataset, Dataset, DataLoader, Subset\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torchvision.transforms import v2\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "xs = torch.Tensor([1, 2, 3, 4])\n",
    "ys = torch.Tensor([5, 6, 7, 8])\n",
    "\n",
    "t = TestDataset(xs, ys)\n",
    "\n",
    "t_loader = DataLoader(t)\n",
    "\n",
    "for i, (x, y) in enumerate(t_loader):\n",
    "    t.y[i] = 9\n",
    "\n",
    "\n",
    "tfm =v2.Compose([\n",
    "    v2.Resize((128, 128)),\n",
    "    v2.ToTensor()\n",
    "])\n",
    "train_unlb_dataset = DatasetFolder(\"./food-11/training/unlabeled\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=tfm)\n",
    "train_unlb_loader = DataLoader(train_unlb_dataset)\n",
    "\n",
    "class Custom_DatasetFolder(DatasetFolder):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]\n",
    "    \n",
    "    def add_sample(self, data, label):\n",
    "        self.data.append(data)\n",
    "        self.label.append(label)\n",
    "\n",
    "class Custom_Dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]\n",
    "    \n",
    "    def datasetFolder_feed(self, datasetFolder):\n",
    "        for i in range(len(datasetFolder)):\n",
    "            self.data.append(datasetFolder.samples[i][0])\n",
    "            self.label.append(datasetFolder.samples[i][1])\n",
    "    \n",
    "    def add_samples(self, sample):\n",
    "        self.data.append(sample[0])\n",
    "        self.label.append(sample[1])\n",
    "    \n",
    "    def set_label(self, index, label):\n",
    "        if index in range(len(self.label)):\n",
    "            self.label[index] = label\n",
    "\n",
    "\n",
    "train_set = Custom_Dataset()\n",
    "train_set.datasetFolder_feed(train_unlb_dataset)\n",
    "\n",
    "subset = Subset(train_set, [0, 1])\n",
    "\n",
    "pseudo_set = Custom_Dataset()\n",
    "pseudo_set.add_samples(train_set.__getitem__(2))\n",
    "\n",
    "concatset = ConcatDataset([subset, pseudo_set])\n",
    "\n",
    "c_loader = DataLoader(concatset, batch_size=3)\n",
    "\n",
    "for batch in tqdm(c_loader):\n",
    "    data, label = batch\n",
    "    print(data)\n",
    "    print(label)\n",
    "\n",
    "c_datasetFolder = Custom_DatasetFolder()\n",
    "c_datasetFolder.add_sample(train_unlb_dataset.samples[0][0], 999)\n",
    "\n",
    "cf_loader = DataLoader(c_datasetFolder)\n",
    "\n",
    "for batch in tqdm(cf_loader):\n",
    "    data, label = batch\n",
    "    print(data)\n",
    "    print(label)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 3, 5, 7, 8, 9]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "lst = [2, 4, 6]\n",
    "\n",
    "a = [i for i in range(10) if i not in lst]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.8471, 0.8627, 0.8627,  ..., 0.0706, 0.0667, 0.0588],\n",
      "         [0.8706, 0.8941, 0.8510,  ..., 0.0745, 0.0667, 0.0588],\n",
      "         [0.9137, 0.9137, 0.8863,  ..., 0.0980, 0.0627, 0.0549],\n",
      "         ...,\n",
      "         [0.7882, 0.8157, 0.8235,  ..., 0.9529, 0.9647, 0.9608],\n",
      "         [0.8039, 0.8196, 0.8157,  ..., 0.9608, 0.9765, 0.9804],\n",
      "         [0.8118, 0.8118, 0.8000,  ..., 0.9647, 0.9804, 0.9843]],\n",
      "\n",
      "        [[0.7333, 0.7451, 0.6980,  ..., 0.0902, 0.0824, 0.0824],\n",
      "         [0.7529, 0.7529, 0.6588,  ..., 0.1020, 0.0863, 0.0863],\n",
      "         [0.9255, 0.8784, 0.7373,  ..., 0.1373, 0.0902, 0.0902],\n",
      "         ...,\n",
      "         [0.8275, 0.8549, 0.8588,  ..., 0.9882, 1.0000, 0.9961],\n",
      "         [0.8392, 0.8588, 0.8588,  ..., 0.9922, 1.0000, 1.0000],\n",
      "         [0.8392, 0.8471, 0.8431,  ..., 0.9882, 0.9961, 1.0000]],\n",
      "\n",
      "        [[0.7333, 0.7451, 0.6941,  ..., 0.0902, 0.0784, 0.0784],\n",
      "         [0.7569, 0.7647, 0.6627,  ..., 0.1098, 0.0863, 0.0824],\n",
      "         [0.9059, 0.8706, 0.7412,  ..., 0.1529, 0.0941, 0.0863],\n",
      "         ...,\n",
      "         [0.8078, 0.8392, 0.8471,  ..., 0.9922, 1.0000, 0.9961],\n",
      "         [0.8118, 0.8353, 0.8392,  ..., 0.9922, 1.0000, 1.0000],\n",
      "         [0.8118, 0.8196, 0.8196,  ..., 0.9961, 1.0000, 1.0000]]]), 0)\n",
      "(tensor([[[0.8471, 0.8627, 0.8627,  ..., 0.0706, 0.0667, 0.0588],\n",
      "         [0.8706, 0.8941, 0.8510,  ..., 0.0745, 0.0667, 0.0588],\n",
      "         [0.9137, 0.9137, 0.8863,  ..., 0.0980, 0.0627, 0.0549],\n",
      "         ...,\n",
      "         [0.7882, 0.8157, 0.8235,  ..., 0.9529, 0.9647, 0.9608],\n",
      "         [0.8039, 0.8196, 0.8157,  ..., 0.9608, 0.9765, 0.9804],\n",
      "         [0.8118, 0.8118, 0.8000,  ..., 0.9647, 0.9804, 0.9843]],\n",
      "\n",
      "        [[0.7333, 0.7451, 0.6980,  ..., 0.0902, 0.0824, 0.0824],\n",
      "         [0.7529, 0.7529, 0.6588,  ..., 0.1020, 0.0863, 0.0863],\n",
      "         [0.9255, 0.8784, 0.7373,  ..., 0.1373, 0.0902, 0.0902],\n",
      "         ...,\n",
      "         [0.8275, 0.8549, 0.8588,  ..., 0.9882, 1.0000, 0.9961],\n",
      "         [0.8392, 0.8588, 0.8588,  ..., 0.9922, 1.0000, 1.0000],\n",
      "         [0.8392, 0.8471, 0.8431,  ..., 0.9882, 0.9961, 1.0000]],\n",
      "\n",
      "        [[0.7333, 0.7451, 0.6941,  ..., 0.0902, 0.0784, 0.0784],\n",
      "         [0.7569, 0.7647, 0.6627,  ..., 0.1098, 0.0863, 0.0824],\n",
      "         [0.9059, 0.8706, 0.7412,  ..., 0.1529, 0.0941, 0.0863],\n",
      "         ...,\n",
      "         [0.8078, 0.8392, 0.8471,  ..., 0.9922, 1.0000, 0.9961],\n",
      "         [0.8118, 0.8353, 0.8392,  ..., 0.9922, 1.0000, 1.0000],\n",
      "         [0.8118, 0.8196, 0.8196,  ..., 0.9961, 1.0000, 1.0000]]]), 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/complexa/anaconda3/envs/complexa_ML21_3a/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import ConcatDataset, Dataset, DataLoader, Subset\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torchvision.transforms import v2\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "\n",
    "tfm =v2.Compose([\n",
    "    v2.Resize((128, 128)),\n",
    "    v2.ToTensor()\n",
    "])\n",
    "\n",
    "train_lb_dataset = DatasetFolder(\"./food-11/training/labeled\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=tfm)\n",
    "\n",
    "\n",
    "class ddd(DatasetFolder):\n",
    "    def __init__(self, root, loader, extension, transform):\n",
    "        super().__init__(root=root, loader=loader, extensions=extension, transform=transform)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (sample, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, self.targets[index]\n",
    "    \n",
    "    def set_label(self, index, label):\n",
    "        self.targets[index] = label\n",
    "    \n",
    "a = ddd(\"./food-11/training/unlabeled\", lambda x: Image.open(x), \"jpg\", tfm)\n",
    "a.set_label(0, 999)\n",
    "\n",
    "sub = Subset(a, [2, 5, 9, 11])\n",
    "\n",
    "c = ConcatDataset([train_lb_dataset, sub])\n",
    "dl = DataLoader(c, batch_size=128, shuffle=True, pin_memory=True)\n",
    "\n",
    "'''\n",
    "for i in tqdm(dl):\n",
    "    print(i)\n",
    "'''\n",
    "\n",
    "print(sub[0])\n",
    "print(a[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "probs = torch.randint(0, 10, (5, 8))\n",
    "\n",
    "max_prob_idx = probs.max(dim=-1)\n",
    "max_prob = max_prob_idx[0]\n",
    "max_idx = max_prob_idx[1]\n",
    "\n",
    "max_idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 999, 2, 3, 4, 5, 6, 999, 8, 999]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(range(10))\n",
    "\n",
    "for i in range(len(a)):\n",
    "    if a[i] % 2 != 0:\n",
    "        if i in [3, 4, 5]:\n",
    "            continue\n",
    "        a[i] = 999\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('str', 1)\n",
      "['str', 1]\n",
      "('str', 2) <class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "a = (\"str\", 1)\n",
    "print(a)\n",
    "b = list(a)\n",
    "print(b)\n",
    "\n",
    "b[1] = 2\n",
    "\n",
    "a = tuple(b)\n",
    "print(a, type(a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complexa_ML21_3a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
