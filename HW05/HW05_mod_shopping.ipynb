{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTv4XN2qB_fz"
      },
      "source": [
        "part 1 load file (finish)\n",
        "part 2 clean data (finish on 12/5)\n",
        "part 3 tokenize : sentencepiece (finish on 12/6)\n",
        "part 4 build model encoder decoder (finish on 12/7)\n",
        "part 5 beam search (finish on 12/8)\n",
        "part 6 label smooth (finish on 12/9)\n",
        "\n",
        "part 7 test (finish on 12/10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwhMkDjdvARr"
      },
      "outputs": [],
      "source": [
        "# !pip install 'torch>=1.6.0' editdistance matplotlib sacrebleu sacremoses sentencepiece tqdm wandb\n",
        "# !pip install --upgrade jupyter ipywidgets\n",
        "# !git clone https://github.com/pytorch/fairseq.git\n",
        "# !cd fairseq && git checkout 9a1c497\n",
        "# !pip install --upgrade ./fairseq/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgIV_VBavJ2K"
      },
      "outputs": [],
      "source": [
        "# import sys\n",
        "# import pdb\n",
        "# import pprint\n",
        "# import logging\n",
        "# import os\n",
        "# import random\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from torch.utils import data\n",
        "# import numpy as np\n",
        "# import tqdm.auto as tqdm\n",
        "# from pathlib import Path\n",
        "# from argparse import Namespace\n",
        "# #from fairseq import utils\n",
        "\n",
        "# import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVHTBvhfZyZY"
      },
      "outputs": [],
      "source": [
        "# seed = 1\n",
        "# random.seed(seed)\n",
        "# torch.manual_seed(seed)\n",
        "# if torch.cuda.is_available():\n",
        "#     torch.cuda.manual_seed(seed)\n",
        "#     torch.cuda.manual_seed_all(seed)\n",
        "# np.random.seed(seed)\n",
        "# torch.backends.cudnn.benchmark = False\n",
        "# torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "X23RaTWe9hoU",
        "outputId": "38d19e30-40ec-46d5-ef8a-63abbf77fb34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/ted2020.tgz'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import shutil\n",
        "shutil.copyfile('/content/drive/MyDrive/ted2020.tgz','/content/ted2020.tgz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2GhVMRgifeKi"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "\n",
        "rawdata_file = \"/content/ted2020.tgz\"\n",
        "unzip_path = \"/content/train_dev/\"\n",
        "# open file\n",
        "file = tarfile.open(rawdata_file)\n",
        "\n",
        "# extracting file\n",
        "file.extractall(unzip_path)\n",
        "\n",
        "file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH2bCPFbvhK2",
        "outputId": "579b0347-61ab-4801-c329-9ce02684f25d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/train_dev/raw.zh\n"
          ]
        }
      ],
      "source": [
        "src_lang = \"raw.en\"\n",
        "tgt_lang = \"raw.zh\"\n",
        "\n",
        "src_path = f\"{unzip_path}{src_lang}\"\n",
        "tgt_path = f\"{unzip_path}{tgt_lang}\"\n",
        "print(tgt_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjIUpASA076k",
        "outputId": "44418f71-84b4-4348-dd31-e9114e18cb64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'> ['Thank you so much, Chris.', \"And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\", 'I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.', 'And I say that sincerely, partly because  I need that.', 'Put yourselves in my position.'] 394066\n",
            "<class 'list'> ['非常謝謝你，克里斯。能有這個機會第二度踏上這個演講台', '真是一大榮幸。我非常感激。', '這個研討會給我留下了極為深刻的印象，我想感謝大家 對我之前演講的好評。', '我是由衷的想這麼說，有部份原因是因為 —— 我真的有需要!', '請你們設身處地為我想一想！'] 394066\n"
          ]
        }
      ],
      "source": [
        "for path in [src_path,tgt_path]:\n",
        "  with open(path, \"r\") as f:\n",
        "  #  data = f.readlines()\n",
        "  #  print(type(data),data[0:5])#<class 'list'> ['Thank you so much, Chris.\\n', \"And it'......on.\\n']\n",
        "    data = f.read().splitlines()\n",
        "    print(type(data),data[0:5],len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhEDa7SN6us_",
        "outputId": "0b69e44b-af47-4771-e0d8-0eadfd4a72b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65292\n",
            "，\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "print(ord(\"，\"))\n",
        "print(chr(65292))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFgPn-1i1EZR",
        "outputId": "b35a60f8-c4f2-4100-b2cd-6b7d5decb53f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ASDFFFFQ\n"
          ]
        }
      ],
      "source": [
        "import unicodedata\n",
        "def to_halfwidth(string):\n",
        "  return \"\".join(unicodedata.normalize('NFKC',letter) for letter in string)\n",
        "print(to_halfwidth(\"ＡＳＤＦＦＦＦＱ\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5dwWeVz-sPI2"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "def clean_s_zh(s):\n",
        "    s = to_halfwidth(s)\n",
        "    # step 1 : delete — _\n",
        "    delete = \" _()[]\"\n",
        "    delete_rules = s.maketrans(\"\",\"\",delete)\n",
        "    s = s.translate(delete_rules)\n",
        "\n",
        "    # step 2 : replace “” with \"\"\n",
        "    to_be_replace = '“”'\n",
        "    replace = '\"\"'\n",
        "    replace_dict = dict(zip(to_be_replace,replace))\n",
        "    # print(replace_dict.items()) : dict_items([('“', '\"'), ('”', '\"')])\n",
        "\n",
        "    # step 3 : add **END** before and after punctuation\n",
        "\n",
        "    \"\"\"\n",
        "    The number of sentences in one line may be different\n",
        "    in line pairs of source and target set.\n",
        "    so I try to use \"。!?\" or \".!?\" to split sentences.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "#    punctuation = \"。,;!?()\\\"~「」\"\n",
        "    punctuation = \"。!?\"\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "\n",
        "    replace_rules = s.maketrans(replace_dict)\n",
        "    s = s.translate(replace_rules)\n",
        "\n",
        "    zh_list = s.strip(\"\\n\").split(\"\\n\")\n",
        "\n",
        "    return zh_list\n",
        "\n",
        "def clean_s_en(s):\n",
        "    s = to_halfwidth(s)\n",
        "\n",
        "    replace_dict = {}\n",
        "\n",
        "    delete = \"-()[]\"\n",
        "    for char in delete:\n",
        "      replace_dict[char] = \"\"\n",
        "\n",
        "    punctuation = \"!?\"\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "    replace_rules = s.maketrans(replace_dict)\n",
        "    s = s.translate(replace_rules)\n",
        "\n",
        "    pattern = re.compile(r\"(?<!([.\\s\\r\\n\\f][a-zA-Z]))[.]\")\n",
        "    s = pattern.sub(\"**END**\",s)\n",
        "\n",
        "    en_list = s.strip(\"\\n\").split(\"\\n\")\n",
        "\n",
        "    return en_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt8FnBUBrSZb",
        "outputId": "d13c416c-53e1-44c0-92fe-a4779276fbc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are many people in U.S. w.r.t. in Taiwan**END**Thank you**END**\n"
          ]
        }
      ],
      "source": [
        "pattern = re.compile(r\"(?<!([.\\s\\r\\n\\f][a-zA-Z]))[.]\")\n",
        "result = pattern.sub(\"**END**\",\"There are many people in U.S. w.r.t. in Taiwan.Thank you.\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yeSGu6mwmbzh"
      },
      "outputs": [],
      "source": [
        "def load_file(path,fuction):\n",
        "  with open(path, \"r\") as f:\n",
        "    data = f.read()\n",
        "    return(fuction(data))\n",
        "src_list = load_file(src_path,clean_s_en)\n",
        "tgt_list = load_file(tgt_path,clean_s_zh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXOFN_BQN7ru",
        "outputId": "854aa5ac-1328-4a97-ecf9-c1cea5387f22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "我們下了交流道後就開始找餐廳 — 我們發現了一家 Shoney' s餐館。\n",
            "\n",
            "我們下了交流道後就開始找餐廳—我們發現了一家Shoney's餐館。**END**\n"
          ]
        }
      ],
      "source": [
        "with open(tgt_path, \"r\") as f:\n",
        "    data = f.readlines()\n",
        "    print(data[17])\n",
        "print(tgt_list[17])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YH2tx-ShwhAS"
      },
      "outputs": [],
      "source": [
        "    # \"\"\"\n",
        "    # Please ensure that space char \" \" has bee remove in previous step ,\n",
        "    # it's the reason s must be translated twice.\n",
        "\n",
        "    # \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    # print(len(replace_dict.items())) : 13\n",
        "#    punctuation = \"(.,;!?()\\\"])\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "0RRwmoBw59gS"
      },
      "outputs": [],
      "source": [
        "def divide_by_END(s):\n",
        "    return(s.strip(\"**END**\").split(\"**END**\"))\n",
        "\n",
        "def devide_en_again(s,punctuation = \":;\"):\n",
        "    replace_dict = {}\n",
        "    for char in punctuation:\n",
        "      replace_dict[char] = char + \"**END**\"\n",
        "\n",
        "    replace_rules_src = s.maketrans(replace_dict)\n",
        "    new_s = divide_by_END(s.translate(replace_rules_src))\n",
        "    return new_s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVYvYCpuDQDa",
        "outputId": "d93397dd-f938-488d-f6af-a87caa8c1ffa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "394066 350481 12188 2972 28425\n",
            "Thank you so much, Chris 非常謝謝你,克里斯。\n"
          ]
        }
      ],
      "source": [
        "def check_data_pairs(src_list,tgt_list):\n",
        "    index = 0\n",
        "    new_src_list = []\n",
        "    new_tgt_list = []\n",
        "\n",
        "    same = 0\n",
        "    add_next = 0\n",
        "    split_again = 0\n",
        "    not_use = 0\n",
        "\n",
        "    while(index < len(src_list)):\n",
        "\n",
        "      src = divide_by_END(src_list[index])\n",
        "      tgt = divide_by_END(tgt_list[index])\n",
        "      # case 1 : src is as long as tgt , finished.\n",
        "      if len(src) == len(tgt):\n",
        "        new_src_list += src\n",
        "        new_tgt_list += tgt\n",
        "        same += 1\n",
        "        index += 1\n",
        "\n",
        "      else :\n",
        "        # if it is not the last one : both src and tgt add next sentence\n",
        "        if index != len(src_list)-1:\n",
        "          src_add_next = divide_by_END(src_list[index] + src_list[index+1])\n",
        "          tgt_add_next = divide_by_END(tgt_list[index] + tgt_list[index+1])\n",
        "          # case 2 : src_add_next is as long as tgt_add_next , finished.\n",
        "          if len(src_add_next) == len(tgt_add_next):\n",
        "            new_src_list += src_add_next\n",
        "            new_tgt_list += tgt_add_next\n",
        "            add_next += 2\n",
        "            index += 2\n",
        "\n",
        "          # using new punctuation to divide tgt (english) sentence.\n",
        "          else :\n",
        "            src_add_next = devide_en_again(src_list[index] + src_list[index+1])\n",
        "            # case 3 : src_add_next is as long as tgt_add_next , finished.\n",
        "            if len(src_add_next) == len(tgt_add_next):\n",
        "              new_src_list += src_add_next\n",
        "              new_tgt_list += tgt_add_next\n",
        "              split_again +=2\n",
        "              index += 2\n",
        "\n",
        "            # case 4 : sentence will not be used.\n",
        "            else :\n",
        "              not_use += 1\n",
        "              # if to_do == 1 :\n",
        "              #   print(index,src_add_next,tgt_add_next,len(src_add_next),len(tgt_add_next))\n",
        "              index += 1\n",
        "\n",
        "        # if it is the last one\n",
        "        else :\n",
        "          not_use += 1\n",
        "          index += 1\n",
        "\n",
        "    print(index,same,add_next,split_again,not_use)\n",
        "\n",
        "    return(new_src_list,new_tgt_list)\n",
        "dataset = check_data_pairs(src_list,tgt_list)\n",
        "print(dataset[0][0],dataset[1][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoNi7CeA3P9G"
      },
      "outputs": [],
      "source": [
        "# s = \"測試,測試,測試。\"\n",
        "# print(s)\n",
        "# s = ' '.join(s.strip().split())\n",
        "# print(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wS-Tl_5JhUVo"
      },
      "outputs": [],
      "source": [
        "# def len_s(s, lang):\n",
        "#     if lang == 'zh':\n",
        "#         return len(s)\n",
        "#     return len(s.split())\n",
        "\n",
        "# def clean_corpus(prefix, l1, l2, ratio=9, max_len=1000, min_len=1):\n",
        "\n",
        "#   for s1 in l1_in_f:\n",
        "#       s1 = s1.strip()\n",
        "#       s2 = l2_in_f.readline().strip()\n",
        "#       s1 = clean_s(s1, l1)\n",
        "#       s2 = clean_s(s2, l2)\n",
        "#       s1_len = len_s(s1, l1)\n",
        "#       s2_len = len_s(s2, l2)\n",
        "#       if min_len > 0: # remove short sentence\n",
        "#           if s1_len < min_len or s2_len < min_len:\n",
        "#               continue\n",
        "#       if max_len > 0: # remove long sentence\n",
        "#           if s1_len > max_len or s2_len > max_len:\n",
        "#               continue\n",
        "#       if ratio > 0: # remove by ratio of length\n",
        "#           if s1_len/s2_len > ratio or s2_len/s1_len > ratio:\n",
        "#               continue\n",
        "#       print(s1, file=l1_out_f)\n",
        "#       print(s2, file=l2_out_f)\n",
        "\n",
        "# if (prefix/f'train.clean.{src_lang}').exists() \\\n",
        "# and (prefix/f'train.clean.{tgt_lang}').exists() \\\n",
        "# and (prefix/f'valid.clean.{src_lang}').exists() \\\n",
        "# and (prefix/f'valid.clean.{tgt_lang}').exists():\n",
        "#     print(f'train/valid splits exists. skipping split.')\n",
        "# else:\n",
        "#     line_num = sum(1 for line in open(f'{data_prefix}.clean.{src_lang}'))\n",
        "#     labels = list(range(line_num))\n",
        "#     random.shuffle(labels)\n",
        "#     for lang in [src_lang, tgt_lang]:\n",
        "#         train_f = open(os.path.join(data_dir, dataset_name, f'train.clean.{lang}'), 'w')\n",
        "#         valid_f = open(os.path.join(data_dir, dataset_name, f'valid.clean.{lang}'), 'w')\n",
        "#         count = 0\n",
        "#         for line in open(f'{data_prefix}.clean.{lang}', 'r'):\n",
        "#             if labels[count]/line_num < train_ratio:\n",
        "#                 train_f.write(line)\n",
        "#             else:\n",
        "#                 valid_f.write(line)\n",
        "#             count += 1\n",
        "#         train_f.close()\n",
        "#         valid_f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oO1-anUThuCQ"
      },
      "outputs": [],
      "source": [
        "# import sentencepiece as spm\n",
        "# vocab_size = 8000\n",
        "# if (prefix/f'spm{vocab_size}.model').exists():\n",
        "#     print(f'{prefix}/spm{vocab_size}.model exists. skipping spm_train.')\n",
        "# else:\n",
        "#     spm.SentencePieceTrainer.train(\n",
        "#         input=','.join([f'{prefix}/train.clean.{src_lang}',\n",
        "#                         f'{prefix}/valid.clean.{src_lang}',\n",
        "#                         f'{prefix}/train.clean.{tgt_lang}',\n",
        "#                         f'{prefix}/valid.clean.{tgt_lang}']),\n",
        "#         model_prefix=prefix/f'spm{vocab_size}',\n",
        "#         vocab_size=vocab_size,\n",
        "#         character_coverage=1,\n",
        "#         model_type='unigram', # 'bpe' 也可\n",
        "#         input_sentence_size=1e6,\n",
        "#         shuffle_input_sentence=True,\n",
        "#         normalization_rule_name='nmt_nfkc_cf',\n",
        "#     )\n",
        "# spm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))\n",
        "# in_tag = {\n",
        "#     'train': 'train.clean',\n",
        "#     'valid': 'valid.clean',\n",
        "#     'test': 'test.raw.clean',\n",
        "# }\n",
        "# for split in ['train', 'valid', 'test']:\n",
        "#     for lang in [src_lang, tgt_lang]:\n",
        "#         out_path = prefix/f'{split}.{lang}'\n",
        "#         if out_path.exists():\n",
        "#             print(f\"{out_path} exists. skipping spm_encode.\")\n",
        "#         else:\n",
        "#             with open(prefix/f'{split}.{lang}', 'w') as out_f:\n",
        "#                 with open(prefix/f'{in_tag[split]}.{lang}', 'r') as in_f:\n",
        "#                     for line in in_f:\n",
        "#                         line = line.strip()\n",
        "#                         tok = spm_model.encode(line, out_type=str)\n",
        "#                         print(' '.join(tok), file=out_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xIWAdyf6h0bY"
      },
      "outputs": [],
      "source": [
        "# !head {data_dir+'/'+dataset_name+'/train.'+src_lang} -n 5\n",
        "# !head {data_dir+'/'+dataset_name+'/train.'+tgt_lang} -n 5\n",
        "# binpath = Path('./DATA/data-bin', dataset_name)\n",
        "# if binpath.exists():\n",
        "#     print(binpath, \"exists, will not overwrite!\")\n",
        "# else:\n",
        "#     !python -m fairseq_cli.preprocess \\\n",
        "#         --source-lang {src_lang}\\\n",
        "#         --target-lang {tgt_lang}\\\n",
        "#         --trainpref {prefix/'train'}\\\n",
        "#         --validpref {prefix/'valid'}\\\n",
        "#         --testpref {prefix/'test'}\\\n",
        "#         --destdir {binpath}\\\n",
        "#         --joined-dictionary\\\n",
        "#         --workers 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbfi-DvrlDhI"
      },
      "outputs": [],
      "source": [
        "# def load_data_iterator(task, split, epoch=1, max_tokens=4000, num_workers=1, cached=True):\n",
        "#     batch_iterator = task.get_batch_iterator(\n",
        "#         dataset=task.dataset(split),\n",
        "#         max_tokens=max_tokens,\n",
        "#         max_sentences=None,\n",
        "#         max_positions=utils.resolve_max_positions(\n",
        "#             task.max_positions(),\n",
        "#             max_tokens,\n",
        "#         ),\n",
        "#         ignore_invalid_inputs=True,\n",
        "#         seed=seed,\n",
        "#         num_workers=num_workers,\n",
        "#         epoch=epoch,\n",
        "#         disable_iterator_cache=not cached,\n",
        "#         # Set this to False to speed up. However, if set to False, changing max_tokens beyond\n",
        "#         # first call of this method has no effect.\n",
        "#     )\n",
        "#     return batch_iterator\n",
        "\n",
        "# demo_epoch_obj = load_data_iterator(task, \"valid\", epoch=1, max_tokens=20, num_workers=1, cached=False)\n",
        "# demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True)\n",
        "# sample = next(demo_iter)\n",
        "# sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VgNDQMVlGLZ"
      },
      "outputs": [],
      "source": [
        "# batch = {\n",
        "#   \"id\": id, # 每個 example 的 id\n",
        "#   \"nsentences\": len(samples), # batch size 句子數\n",
        "#   \"ntokens\": ntokens, # batch size 字數\n",
        "#   \"net_input\": {\n",
        "#       \"src_tokens\": src_tokens, # 來源語言的序列\n",
        "#       \"src_lengths\": src_lengths, # 每句話沒有 pad 過的長度\n",
        "#       \"prev_output_tokens\": prev_output_tokens, # 上面提到右 shift 一格後的目標序列\n",
        "#   },\n",
        "#   \"target\": target, # 目標序列\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYgg7zwHk1jn"
      },
      "outputs": [],
      "source": [
        "# cuda_env = utils.CudaEnvironment()\n",
        "# utils.CudaEnvironment.pretty_print_cuda_env_list([cuda_env])\n",
        "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Zy7vAVXlTAW"
      },
      "outputs": [],
      "source": [
        "# # # HINT: transformer 架構\n",
        "# # from fairseq.models.transformer import (\n",
        "# #     TransformerEncoder,\n",
        "# #     TransformerDecoder,\n",
        "# # )\n",
        "\n",
        "# def build_model(args, task):\n",
        "#     \"\"\" 按照參數設定建置模型 \"\"\"\n",
        "#     src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
        "\n",
        "#     # 詞嵌入\n",
        "#     encoder_embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, src_dict.pad())\n",
        "#     decoder_embed_tokens = nn.Embedding(len(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())\n",
        "\n",
        "#     # 編碼器與解碼器\n",
        "#     # TODO: 替換成 TransformerEncoder 和 TransformerDecoder\n",
        "#     encoder = RNNEncoder(args, src_dict, encoder_embed_tokens)\n",
        "#     decoder = RNNDecoder(args, tgt_dict, decoder_embed_tokens)\n",
        "\n",
        "#     # 序列到序列模型\n",
        "#     model = Seq2Seq(args, encoder, decoder)\n",
        "\n",
        "#     # 序列到序列模型的初始化很重要 需要特別處理\n",
        "#     def init_params(module):\n",
        "#         from fairseq.modules import MultiheadAttention\n",
        "#         if isinstance(module, nn.Linear):\n",
        "#             module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "#             if module.bias is not None:\n",
        "#                 module.bias.data.zero_()\n",
        "#         if isinstance(module, nn.Embedding):\n",
        "#             module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "#             if module.padding_idx is not None:\n",
        "#                 module.weight.data[module.padding_idx].zero_()\n",
        "#         if isinstance(module, MultiheadAttention):\n",
        "#             module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "#             module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "#             module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "#         if isinstance(module, nn.RNNBase):\n",
        "#             for name, param in module.named_parameters():\n",
        "#                 if \"weight\" in name or \"bias\" in name:\n",
        "#                     param.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "#     # 初始化模型\n",
        "#     model.apply(init_params)\n",
        "#     return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBNhGpgS08gI"
      },
      "outputs": [],
      "source": [
        "# class Seq2Seq(FairseqEncoderDecoderModel):\n",
        "#     def __init__(self, args, encoder, decoder):\n",
        "#         super().__init__(encoder, decoder)\n",
        "#         self.args = args\n",
        "\n",
        "#     def forward(\n",
        "#         self,\n",
        "#         src_tokens,\n",
        "#         src_lengths,\n",
        "#         prev_output_tokens,\n",
        "#         return_all_hiddens: bool = True,\n",
        "#     ):\n",
        "#         \"\"\"\n",
        "#         Run the forward pass for an encoder-decoder model.\n",
        "#         \"\"\"\n",
        "#         encoder_out = self.encoder(\n",
        "#             src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
        "#         )\n",
        "#         logits, extra = self.decoder(\n",
        "#             prev_output_tokens,\n",
        "#             encoder_out=encoder_out,\n",
        "#             src_lengths=src_lengths,\n",
        "#             return_all_hiddens=return_all_hiddens,\n",
        "#         )\n",
        "#         return logits, extra"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
