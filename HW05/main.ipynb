{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML21-HW05-Translation(Seq2Seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Jump direct to 'Preparing Data - Colab' if running on colab.\n",
    "\n",
    "Reference:\n",
    "\n",
    "- main\n",
    "    - https://zhuanlan.zhihu.com/p/106902569\n",
    "    - https://zhuanlan.zhihu.com/p/347061440\n",
    "    - https://colab.research.google.com/github/ga642381/ML2021-Spring/blob/main/HW05/HW05.ipynb\n",
    "    - https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "    \n",
    "- utils\n",
    "    - https://docs.python.org/3/howto/logging.html#logging-basic-tutorial\n",
    "\n",
    "Architecture:\n",
    "\n",
    "- Preparing Data\n",
    "- Token\n",
    "- Logging\n",
    "- Dataset & Dataloader\n",
    "- Model\n",
    "- Criterion & Optimizer\n",
    "- Beam Search\n",
    "- Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data\n",
    "\n",
    "The data has already been downloaded to local. Check original code from TA's for download information.\n",
    "\n",
    "- Extract the .tgz files `./data/ted2020.tgz` to `./data/raw/`.\n",
    "\n",
    "- Clean raw data and save the result to `./data/clean/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip `.tgz` File\n",
    "\n",
    "(These code blocks credit to `shopping le macaca`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "src_path = './data/ted2020.tgz'\n",
    "unzip_dir = './data/raw'\n",
    "Path(unzip_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with tarfile.open(src_path) as tf:\n",
    "    tf.extractall(unzip_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some data\n",
    "with open(unzip_dir + '/raw.en') as en_file:\n",
    "    en_data = en_file.read().splitlines()\n",
    "with open(unzip_dir + '/raw.zh') as zh_file:\n",
    "    zh_data = zh_file.read().splitlines()\n",
    "for i in range(5):\n",
    "    print(en_data[i])\n",
    "    print(zh_data[i])\n",
    "with open(unzip_dir + '/raw.en') as en_r:\n",
    "    endata = en_r.readline()\n",
    "    print(endata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "- clean data\n",
    "- save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class DataPreprocessor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    - en_filepath (:obj:`str`) :\n",
    "        filepath of raw en-corpus\n",
    "    - zh_filepath (:obj:`str`) :\n",
    "        filepath of raw zh-corpus\n",
    "    - is_test (:obj:`bool` | False) :\n",
    "        whether the input files are for testing\n",
    "    - ratio (:obj:`int` | 9) :\n",
    "        threshold ratio between senctence lengths of en and zh\n",
    "    - max_len (:obj:`int` | 1000) :\n",
    "        maximum length of each sentence\n",
    "    - min_len (:obj:`int` | 1) :\n",
    "        minimum length of each sentence\n",
    "\n",
    "    function call:\n",
    "    - cleans en- and zh-corpus\n",
    "    - return tuple of lists (en-corpus, zh-corpus)\n",
    "    \"\"\"\n",
    "    def __call__(self, en_filepath : str, zh_filepath : str, is_test=False, ratio=9, max_len=1000, min_len=1):\n",
    "        en_path = Path(en_filepath)\n",
    "        zh_path = Path(zh_filepath)     \n",
    "        en_clean_corpus = []\n",
    "        zh_clean_corpus = []\n",
    "        # start clean\n",
    "        if is_test:\n",
    "            with en_path.open(mode='r') as r:\n",
    "                for s in r:\n",
    "                    s = s.strip()\n",
    "                    s = self.__clean_string__(s, 'en')\n",
    "                    en_clean_corpus.append(s)\n",
    "        else:\n",
    "            with en_path.open(mode='r') as ren:\n",
    "                with zh_path.open(mode='r') as rzh:\n",
    "                    for s_en in ren:\n",
    "                        s_en = s_en.strip()\n",
    "                        s_zh = rzh.readline().strip()\n",
    "                        s_en = self.__clean_string__(s_en, 'en')\n",
    "                        s_zh = self.__clean_string__(s_zh, 'zh')\n",
    "                        if self.__drop__(\n",
    "                            self.__get_strlen__(s_en, 'en'),\n",
    "                            self.__get_strlen__(s_zh, 'zh'),\n",
    "                            min_len, max_len, ratio):\n",
    "                            continue\n",
    "                        en_clean_corpus.append(s_en)\n",
    "                        zh_clean_corpus.append(s_zh)\n",
    "        return en_clean_corpus, zh_clean_corpus\n",
    "    \n",
    "    def __drop__(self, en_len, zh_len, min_len, max_len, ratio):\n",
    "        \"\"\"check whether the input strings pass the given thresholds\"\"\"\n",
    "        if en_len < min_len or zh_len < min_len:\n",
    "            return True\n",
    "        if en_len > max_len or zh_len > max_len:\n",
    "            return True\n",
    "        if en_len/zh_len > ratio or zh_len/en_len > ratio:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def __f2h__(self, fstr : str):\n",
    "        \"\"\"convert full-width string to half-width string\"\"\"\n",
    "        # reference: https://www.cnblogs.com/kaituorensheng/p/3554571.html\n",
    "        # half-width string splitter\n",
    "        hss = []\n",
    "        for s in fstr:\n",
    "            num = ord(s)\n",
    "            if num == 12288:\n",
    "                num = 32\n",
    "            elif 65281 <= num <= 65374:\n",
    "                num -= 65248\n",
    "            hss.append(chr(num))\n",
    "        return ''.join(hss)\n",
    "    \n",
    "    def __get_strlen__(self, s : str, lang : str):\n",
    "        if lang == \"zh\":\n",
    "            return len(s)\n",
    "        return len(s.split())\n",
    "        \n",
    "    def __clean_string__(self, s : str, lang : str):\n",
    "        if lang == 'en':\n",
    "            s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
    "            s = s.replace('-', '') # remove '-'\n",
    "            # Each word in en-string is separated by space, and in order\n",
    "            #   to keep punctuation for training, we also add space between\n",
    "            #   word and punctuation.\n",
    "            # For example,\n",
    "            #   \"hello, this is an example!\" becomes\n",
    "            #   \"hello ,  this is an example ! \"\n",
    "            s = re.sub('([.,;:!?()\\\"])', r\" \\1 \", s)\n",
    "            # Continue the above example, the result becomes\n",
    "            #   \"hello , this is an example !\"\n",
    "            s = ' '.join(s.strip().split())\n",
    "        elif lang == 'zh':\n",
    "            s = self.__f2h__(s)\n",
    "            s = re.sub(r\"\\([^()]*\\)\", \"\", s)\n",
    "            s = s.replace(' ', '')\n",
    "            s = s.replace('-', '')\n",
    "            s = s.replace('“', '\"')\n",
    "            s = s.replace('”', '\"')\n",
    "            s = s.replace('_', '')\n",
    "            # For zh-string, each character is a word, and hence either we\n",
    "            #   add space between each of the word and punctuation like\n",
    "            #   we deal with en-string( which will cause a lot of memory waste)\n",
    "            #   or we deal the zh-string compactly( no sapce between word and punctuation).\n",
    "            # We choose the second solution, and the example would be\n",
    "            #   \"你好,這是一個例子!\"\n",
    "            #s = re.sub('([。,;:!?()\\\"~「」])', r' \\1 ', s)\n",
    "            s = s.strip()\n",
    "        return s\n",
    "    \n",
    "def save_data(save_dir, en_corpus, zh_corpus):\n",
    "    odir = Path(save_dir)\n",
    "    odir.mkdir(parents=True, exist_ok=True)\n",
    "    with open(odir / \"clean.en\", 'w') as ofile:\n",
    "        for s in en_corpus:\n",
    "            print(s, file=ofile)\n",
    "    with open(odir / \"clean.zh\", 'w') as ofile:\n",
    "        for s in zh_corpus:\n",
    "            print(s, file=ofile)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean raw data and save result\n",
    "dp = DataPreprocessor()\n",
    "en_corpus, zh_corpus = dp(\"./data/raw/raw.en\", \"./data/raw/raw.zh\")\n",
    "for i in range(5):\n",
    "    print(en_corpus[i])\n",
    "    print(zh_corpus[i])\n",
    "save_data(\"./data/clean\", en_corpus, zh_corpus)\n",
    "\n",
    "# check cleaned data\n",
    "clean_en = []\n",
    "clean_zh = []\n",
    "with open(\"./data/clean/clean.en\", 'r') as ifile:\n",
    "    clean_en = ifile.readlines()\n",
    "with open(\"./data/clean/clean.zh\", 'r') as ifile:\n",
    "    clean_zh = ifile.readlines()\n",
    "print(f\"en sentences : {len(clean_en)}\")\n",
    "print(f\"zh sentences : {len(clean_zh)}\")\n",
    "for i in range(5):\n",
    "    print(clean_en[i].strip())\n",
    "    print(clean_zh[i].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import shutil\n",
    "shutil.copyfile('/content/drive/MyDrive/ML2021_data/hw5/token.tgz','/content/token.tgz')\n",
    "\n",
    "import tarfile\n",
    "with tarfile.open('/content/token.tgz','r') as tar_ref:\n",
    "  tar_ref.extractall('/content/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token (sentencepiece)\n",
    "\n",
    "- Train tokenizers and save models under `./token/tokenizer/`.\n",
    "\n",
    "- Tokenize data from\n",
    "\n",
    "  - `./data/clean/clean.en`\n",
    "\n",
    "  - `./data/clean/clean.zh`\n",
    "  \n",
    "  to `./token/data/train_token.json`.\n",
    "\n",
    "  In order to speed up training process, we save tokenized data in this block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sentencepiece as spm\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"datapath_config\" : {\n",
    "        \"clean\" : [\"./data/clean/clean.en\", \"./data/clean/clean.zh\"],\n",
    "    },\n",
    "    \"spm_config\" : {\n",
    "        \"vocab_size\" : 8000,\n",
    "        \"dir\" : \"./token\",\n",
    "        \"model_type\" : \"bpe\",\n",
    "        \"nm_rule\" : \"nmt_nfkc_cf\",\n",
    "        \"character_coverage\" : 1,\n",
    "        \"input_sentence_size\" : 1e6,\n",
    "        \"shuffle_input_sentence\" : True,\n",
    "        \"pad_id\" : 0,\n",
    "        \"unk_id\" : 1,\n",
    "        \"bos_id\" : 2,\n",
    "        \"eos_id\" : 3,\n",
    "    },\n",
    "}\n",
    "\n",
    "class SpmTokenHandler:\n",
    "    def __init__(self, spm_config, datapath_config):\n",
    "        self.spm_config = spm_config\n",
    "        self.datapath = datapath_config['clean']\n",
    "        # make directory for models\n",
    "        model_dir = Path(spm_config['dir']) / \"tokenizer\"\n",
    "        model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        # get spm model prefix\n",
    "        self.en_model_prefix = model_dir / f\"spm{spm_config['vocab_size']}en\"\n",
    "        self.zh_model_prefix = model_dir / f\"spm{spm_config['vocab_size']}zh\"\n",
    "    \n",
    "    def __get_model_path__(self):\n",
    "        \"\"\"get spm model path\"\"\"\n",
    "        return Path(f\"{self.en_model_prefix}.model\"), Path(f\"{self.zh_model_prefix}.model\")\n",
    "    \n",
    "    def __check_spm_model__(self):\n",
    "        \"\"\"check whether spm-models exist, if not, create them\"\"\"\n",
    "        en_model_path, zh_model_path = self.__get_model_path__()\n",
    "        if not en_model_path.exists():\n",
    "            self.__train_tokenizer__(filepath=self.datapath[0], model_prefix=self.en_model_prefix, **self.spm_config)\n",
    "        if not zh_model_path.exists():\n",
    "            self.__train_tokenizer__(filepath=self.datapath[1], model_prefix=self.zh_model_prefix, **self.spm_config)\n",
    "\n",
    "    def __train_tokenizer__(self, filepath, model_prefix, vocab_size, character_coverage,\n",
    "                        model_type, input_sentence_size,\n",
    "                        shuffle_input_sentence, nm_rule,\n",
    "                        pad_id, unk_id, bos_id, eos_id, **kwargs):\n",
    "        \"\"\"\n",
    "        make sentence piece tokenizer\n",
    "        \"\"\"\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input=filepath,\n",
    "            model_prefix=model_prefix,\n",
    "            vocab_size=vocab_size,\n",
    "            character_coverage=character_coverage,\n",
    "            model_type=model_type,\n",
    "            input_sentence_size=input_sentence_size,\n",
    "            shuffle_input_sentence=shuffle_input_sentence,\n",
    "            normalization_rule_name=nm_rule,\n",
    "            pad_id=pad_id,\n",
    "            unk_id=unk_id,\n",
    "            bos_id=bos_id,\n",
    "            eos_id=eos_id,\n",
    "        )\n",
    "    \n",
    "    def tokenize(self, threshold):\n",
    "        \"\"\"\n",
    "        Do tokenization and return tokenized data and max length of tokens.\n",
    "\n",
    "        Args:\n",
    "            - threshold (:obj:`int`):\n",
    "                maximun number of tokens of one sentence may have\n",
    "\n",
    "        Return:\n",
    "            (english tokens set, chinese tokens set, maximum length of tokens)\n",
    "        \"\"\"\n",
    "        self.__check_spm_model__()\n",
    "        # load data\n",
    "        en_corpus = []\n",
    "        zh_corpus = []\n",
    "        with open(self.datapath[0], 'r') as ifile:\n",
    "            for s in ifile:\n",
    "                en_corpus.append(s.strip())\n",
    "        with open(self.datapath[1], 'r') as ifile:\n",
    "            for s in ifile:\n",
    "                zh_corpus.append(s.strip())\n",
    "        # tokenize\n",
    "        en_tokenizer, zh_tokenizer = self.get_tokenizers()\n",
    "        bos_id = self.spm_config['bos_id']\n",
    "        eos_id = self.spm_config['eos_id']\n",
    "        en_tokens = []\n",
    "        zh_tokens = []\n",
    "        for i in range(len(en_corpus)):\n",
    "            en_tks = [bos_id] + en_tokenizer.encode(en_corpus[i], out_type=int) + [eos_id]\n",
    "            zh_tks = [bos_id] + zh_tokenizer.encode(zh_corpus[i], out_type=int) + [eos_id]\n",
    "            if len(en_tks) > threshold or len(zh_tks) > threshold:\n",
    "                continue\n",
    "            en_tokens.append(en_tks)\n",
    "            zh_tokens.append(zh_tks)\n",
    "        max_tokens_len = len(max([max(en_tokens, key=len), max(zh_tokens, key=len)], key=len))\n",
    "        return en_tokens, zh_tokens, max_tokens_len\n",
    "\n",
    "    def get_tokenizers(self):\n",
    "        \"\"\"\n",
    "        Return: (english tokenizer, chinese tokenizer)\n",
    "        \"\"\"\n",
    "        self.__check_spm_model__()\n",
    "        en_model_path, zh_model_path = self.__get_model_path__()    \n",
    "        return spm.SentencePieceProcessor(model_file=str(en_model_path)), \\\n",
    "               spm.SentencePieceProcessor(model_file=str(zh_model_path))\n",
    "        \n",
    "def save_tokens_set(save_dir, src_data, tgt_data, length):\n",
    "    \"\"\"save tokens set into :obj:`train.json` file\"\"\"\n",
    "    file_dir = Path(save_dir)\n",
    "    file_dir.mkdir(parents=True, exist_ok=True)\n",
    "    js_obj = {\n",
    "        \"length\" : length,\n",
    "        \"src_data\" : src_data,\n",
    "        \"tgt_data\" : tgt_data\n",
    "    }\n",
    "    with open(file_dir / \"train.json\", 'w') as ofile:\n",
    "        json.dump(js_obj, ofile)\n",
    "\n",
    "\n",
    "# Do this block if 'train.json' file is not saved.\n",
    "\"\"\"\n",
    "# train tokenizer\n",
    "spmTokenHandler = SpmTokenHandler(**config)\n",
    "en_tokens_set, zh_tokens_set, max_len = spmTokenHandler.tokenize(400)\n",
    "print(f\"max tokens length {max_len}\")\n",
    "en_tokenizer, zh_tokenizer = spmTokenHandler.get_tokenizers()\n",
    "s = 'hello world , this is for test .'\n",
    "print(en_tokenizer.EncodeAsPieces(s))\n",
    "print(en_tokenizer.EncodeAsIds(s))\n",
    "print(en_tokenizer.DecodeIds([5788, 277, 10, 74, 64, 85, 1149, 14]))\n",
    "sz = '椰奶不是產自椰羊所以請不要騷擾椰羊！'\n",
    "print(zh_tokenizer.EncodeAsPieces(sz))\n",
    "print(zh_tokenizer.EncodeAsIds(sz))\n",
    "print(zh_tokenizer.DecodeIds([2043, 4827, 3526, 28, 2279, 2107, 4827, 3594, 15, 2577, 458, 4096, 3354, 4827, 3594, 2410]))\n",
    "\n",
    "# save/load\n",
    "save_tokens_set(\"./token/data\", en_tokens_set, zh_tokens_set, max_len)\n",
    "js_obj = {}\n",
    "with open(\"./token/data/train.json\", 'r') as ifile:\n",
    "    js_obj = json.load(ifile)\n",
    "print(f\"tokens length : {js_obj['length']}\")\n",
    "for i in range(5):\n",
    "    print(en_tokenizer.decode(js_obj['src_data'][i]))\n",
    "    print(zh_tokenizer.decode(js_obj['tgt_data'][i]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def set_logger(log_name : str):\n",
    "    \"\"\"write log under given log_path\"\"\"\n",
    "    log_dir = Path(\"./log\")\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    log_path = log_dir / f\"{datetime.datetime.now().date()}.log\"\n",
    "\n",
    "    logger = logging.getLogger(log_name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formater_s = logging.Formatter(\"%(name)s [%(levelname)s] %(message)s\")\n",
    "    formater_f = logging.Formatter(\"%(asctime)s - %(name)s [%(levelname)s] %(message)s\")\n",
    "    \n",
    "    if not logger.handlers:\n",
    "        \"\"\"\n",
    "        # logging to file\n",
    "        file_handler = logging.FileHandler(log_path)\n",
    "        file_handler.setFormatter(formater_f)\n",
    "        logger.addHandler(file_handler)\n",
    "        \"\"\"\n",
    "        # logging to console\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        stream_handler.setFormatter(formater_s)\n",
    "        logger.addHandler(stream_handler)\n",
    "    \n",
    "    return logger\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset & DataLoader\n",
    "\n",
    "Data (`./token/data/train.json`) structure:\n",
    "\n",
    "- `length` : (int) length of each data\n",
    "- `src_data` : (list of int-list) english data\n",
    "- `tgt_data` : (list of int-list) chinese data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "#from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, filepath, pad_id):\n",
    "        js_obj = None\n",
    "        with open(filepath, 'r') as fp:\n",
    "            js_obj = json.load(fp)\n",
    "        self.feature_len = js_obj['length']\n",
    "        self.src_data = js_obj['src_data']\n",
    "        self.tgt_data = js_obj['tgt_data']\n",
    "        self.__padding__(pad_id)\n",
    "\n",
    "    def __padding__(self, pad_id):\n",
    "        \"\"\"padding data to specific length\"\"\"\n",
    "        # shorten the names\n",
    "        src = self.src_data\n",
    "        tgt = self.tgt_data\n",
    "        for i in range(len(src)):\n",
    "            src[i] = src[i] + [pad_id for j in range(self.feature_len - len(src[i]))]\n",
    "            tgt[i] = tgt[i] + [pad_id for j in range(self.feature_len - len(tgt[i]))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.src_data[idx], self.tgt_data[idx]\n",
    "    \n",
    "    def get_features_len(self):\n",
    "        return self.feature_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Container\n",
    "\n",
    "Container to hold data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"collect data in batch, used in collate_fn\"\"\"\n",
    "    def __init__(self, src, tgt, src_key_padding_mask, tgt_key_padding_mask):\n",
    "        self.src = src\n",
    "        self.tgt = tgt\n",
    "        self.src_key_padding_mask = src_key_padding_mask\n",
    "        self.tgt_key_padding_mask = tgt_key_padding_mask\n",
    "    \n",
    "    def to(self, device):\n",
    "        return Batch(self.src.to(device), self.tgt.to(device), self.src_key_padding_mask.to(device), self.tgt_key_padding_mask.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "def collate_fn(batch, pad_id):\n",
    "    # batch = [(src-0, tgt-0), (src-1, tgt-1), ... , (src-n, tgt-n)]\n",
    "    #   where batch_size = n + 1.\n",
    "    # src-i = tensor([tk-0, tk-1, ... , tk-n])\n",
    "    src = torch.tensor([x[0] for x in batch])\n",
    "    tgt = torch.tensor([x[1] for x in batch])\n",
    "    src_key_padding_mask = (src == pad_id)\n",
    "    tgt_key_padding_mask = (tgt == pad_id)\n",
    "    return Batch(src, tgt, src_key_padding_mask, tgt_key_padding_mask)\n",
    "\n",
    "def get_dataloader(filepath, batch_size, n_workers, pad_id, collate_fn):\n",
    "    \"\"\"\n",
    "    Get dataloaders for training and validating.\n",
    "    \n",
    "    Return: train_loader, valid_loader, features_len\n",
    "    \"\"\"\n",
    "    dataset = myDataset(filepath, pad_id)\n",
    "    features_len = dataset.get_features_len()\n",
    "    # split dataset into training and validating\n",
    "    trainlen = int(0.9 * len(dataset))\n",
    "    lengths = [trainlen, len(dataset)-trainlen]\n",
    "    trainset, validset = random_split(dataset, lengths)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        trainset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=n_workers,\n",
    "        collate_fn=lambda x : collate_fn(x, pad_id),\n",
    "        drop_last=True,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        validset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=n_workers,\n",
    "        collate_fn=lambda x : collate_fn(x, pad_id),\n",
    "        drop_last=True,\n",
    "    )\n",
    "    return train_loader, valid_loader, features_len\n",
    "\n",
    "\"\"\"\n",
    "train_loader, valid_loader, flen = get_dataloader(\"./token/data/train.json\", 32, 0, 0, collate_fn)\n",
    "for i, batch in enumerate(train_loader):\n",
    "    if i == 1:\n",
    "        break\n",
    "    print(batch.src.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Reference:\n",
    "- https://zhuanlan.zhihu.com/p/106902569\n",
    "- https://github.com/hemingkx/ChineseNMT/blob/master/model.py\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
    "- https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 400, 800])\n",
      "torch.Size([400, 32, 128])\n",
      "torch.Size([32, 400, 800])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torchinfo import summary\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super().__init__()\n",
    "        # position encoding matrix : (length, dim)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # position index\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # division term\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.) / d_model))\n",
    "        # assign calculated number into the matrix\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # add batch dimension to pe : (batch, length, dim)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : (batch, length, dim)\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return x\n",
    "    \n",
    "class Preprocessor(nn.Module):\n",
    "    def __init__(self, pos_encode, vocab_sz, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedder = nn.Embedding(vocab_sz, d_model)\n",
    "        self.pos_encoder = pos_encode(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : (batch, length, dim)\n",
    "        # embedding\n",
    "        x = self.embedder(x).squeeze()\n",
    "        # positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        return x\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead=2, encoder_layer=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        encoder_norm = nn.LayerNorm(d_model)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, encoder_layer, encoder_norm)\n",
    "\n",
    "    def forward(self, src, src_key_padding_mask=None):\n",
    "        # src : (batch, length, dim)\n",
    "        # src : (length, batch, dim)\n",
    "        src = src.transpose(0, 1)\n",
    "        src = self.encoder(src, src_key_padding_mask=src_key_padding_mask)\n",
    "        return src\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, batch_size, length, d_model, nhead=2, decoder_layer=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nhead = nhead\n",
    "        # attention-mask\n",
    "        attn_mask = self.generate_attn_mask(batch_size, length)\n",
    "        self.register_buffer('attn_mask', attn_mask)\n",
    "        # decoder\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead)\n",
    "        decoder_norm = nn.LayerNorm(d_model)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, decoder_layer, decoder_norm)\n",
    "        # generator(feedforward)\n",
    "        self.generator = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def generate_attn_mask(self, batch_size, mask_size):\n",
    "        \"\"\"Generate attention mask.\"\"\"\n",
    "        mask = torch.ones((mask_size, mask_size))\n",
    "        mask = (mask.triu(diagonal=1) == 1)\n",
    "        return mask.expand(batch_size * self.nhead, -1, -1)\n",
    "    \n",
    "    def forward(self, tgt, memory, tgt_mask=None, tgt_key_padding_mask=None):\n",
    "        # memory : (length, batch, dim)\n",
    "        # tgt : (batch, length, dim)\n",
    "        # tgt : (length, batch, dim)\n",
    "        tgt = tgt.transpose(0, 1)\n",
    "        # output : (length, batch, dim)\n",
    "        if tgt_mask is None:\n",
    "            output = self.decoder(tgt, memory, tgt_mask=self.attn_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        else:\n",
    "            output = self.decoder(tgt, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        # output : (batch, length, dim)\n",
    "        output = output.transpose(0, 1)\n",
    "        # feedforward\n",
    "        output = self.generator(output)\n",
    "        output = F.log_softmax(output, -1)\n",
    "        return output\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, preprocessor_src : Preprocessor, preprocessor_tgt : Preprocessor,\n",
    "                 encoder : Encoder, decoder : Decoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.preprocessor_src = preprocessor_src\n",
    "        self.preprocessor_tgt = preprocessor_tgt\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def encode(self, src, src_key_padding_mask=None):\n",
    "        src = self.preprocessor_src(src)\n",
    "        src = self.encoder(src=src, src_key_padding_mask=src_key_padding_mask)\n",
    "        return src\n",
    "    \n",
    "    def decode(self, tgt, memory, tgt_mask=None, tgt_key_padding_mask=None):\n",
    "        tgt = self.preprocessor_tgt(tgt)\n",
    "        tgt = self.decoder(tgt=tgt, memory=memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        return tgt\n",
    "\n",
    "    def forward(self, src, tgt, src_key_padding_mask=None,tgt_mask=None, tgt_key_padding_mask=None):\n",
    "        # src : (batch, length, dim)\n",
    "        # tgt : (batch, length, dim)\n",
    "        # memory : (length, batch, dim)\n",
    "        memory = self.encode(src, src_key_padding_mask)\n",
    "        # output : (batch, length, dim)\n",
    "        output = self.decode(tgt=tgt, memory=memory, tgt_mask=tgt_mask,  tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        return output\n",
    "\n",
    "\n",
    "def mk_model(pos_encode, vocab_size, batch_size, length, d_model, nhead_encoder=2, nhead_decoder=2, encoder_layer=2, decoder_layer=2, **kwargs):\n",
    "    \"\"\"making model\"\"\"\n",
    "    preprocessor_src = Preprocessor(pos_encode, vocab_size, d_model)\n",
    "    preprocessor_tgt = Preprocessor(pos_encode, vocab_size, d_model)\n",
    "    encoder = Encoder(d_model, nhead_encoder, encoder_layer)\n",
    "    decoder = Decoder(vocab_size, batch_size, length, d_model, nhead_decoder, decoder_layer)\n",
    "    transformer = Transformer(preprocessor_src, preprocessor_tgt, encoder, decoder)\n",
    "    return transformer\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "vocab_size = 800\n",
    "batch_size = 32\n",
    "length = 400\n",
    "d_model = 128\n",
    "\n",
    "model = mk_model(PositionalEncoding, vocab_size, batch_size, length, d_model)\n",
    "#src = torch.randint(0, vocab_size, (batch_size, length, 1), dtype=torch.long)\n",
    "#tgt = torch.randint(0, vocab_size, (batch_size, length, 1), dtype=torch.long)\n",
    "src = torch.randint(0, vocab_size, (batch_size, length), dtype=torch.long)\n",
    "tgt = torch.randint(0, vocab_size, (batch_size, length), dtype=torch.long)\n",
    "src_key_padding_mask = torch.cat(((torch.FloatTensor(batch_size, 200).uniform_() > 1),\n",
    "                                  (torch.FloatTensor(batch_size, 200).uniform_() > 0.15)), dim=1)\n",
    "tgt_key_padding_mask = torch.cat(((torch.FloatTensor(batch_size, 100).uniform_() > 1),\n",
    "                                  (torch.FloatTensor(batch_size, 300).uniform_() > 0.15)), dim=1)\n",
    "#summary(modelTest, src=src, tgt=tgt, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "#summary(modelTest.preprocessor, src=src, tgt=tgt)\n",
    "#print(modelTest.state_dict())\n",
    "\n",
    "#print(src.shape)\n",
    "#src = model.preprocessor_src(src)\n",
    "#print(src.shape)\n",
    "\n",
    "prob = model(src=src, tgt=tgt, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "print(prob.shape)\n",
    "\n",
    "memory = model.encode(src=src, src_key_padding_mask=src_key_padding_mask)\n",
    "print(memory.shape)\n",
    "output = model.decode(tgt=tgt, memory=memory, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "print(output.shape)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criterion & Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criterion : Label-Smoothing Cross Entropy Loss\n",
    "\n",
    "Reference:\n",
    "- https://zhuanlan.zhihu.com/p/518262647\n",
    "- https://github.com/hemingkx/ChineseNMT/blob/master/model.py\n",
    "- https://blog.csdn.net/lihuanyu520/article/details/132164972\n",
    "- HW05\n",
    "\n",
    "Also refers `7. Model Regularization via Label Smoothing`: https://arxiv.org/pdf/1512.00567.pdf\n",
    "- $p$ : output prediction\n",
    "- $q$ : target label (one-hot vector)\n",
    "- $q'$ : label-smoothing\n",
    "\n",
    "$$ H(q', p) = -\\sum_{k=1}^K\\log p(k)q'(k) = (1-\\epsilon)H(q, p) + \\epsilon H(u, p) $$\n",
    "\n",
    "where\n",
    "- $K$ is total labels\n",
    "- $\\epsilon=\\frac{smoothing\\ rate}{K}$\n",
    "- $u(k)=\\frac{1}{K}$\n",
    "- $q'(k) = (1 - \\epsilon)\\delta_{k, y} + \\frac{\\epsilon}{K}$, $y$ is true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LabelSmoothingCrossEntropyLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Brief \n",
    "    y^ = < y_1^, y_2^, y_3^, ... , y_n^ > is an n-dimensional one-hot vector, for which\n",
    "\n",
    "        y_i^ =\n",
    "\n",
    "         1  if i = target\n",
    "\n",
    "         0  if i != target\n",
    "\n",
    "    Instead of using above one-hot vector to compute cross entropy, we use smoothed\n",
    "    y~ = < y_1~, y_2~, ... , y_n~ >, which\n",
    "\n",
    "        y_i~ =\n",
    "\n",
    "          1 - e     if i = target\n",
    "\n",
    "          e / (n-1) if i != target\n",
    "\n",
    "    where e is a small number between 0 and 1. (usually named e as 'smoothing').\n",
    "    \n",
    "    Note y_i~ may differ, for example, in `7. Model Regularization via Label Smoothing`,\n",
    "        y_i~ =\n",
    "\n",
    "         1 - e + e/n    if i = target\n",
    "\n",
    "         e/n            if i != target\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, padding_idx=None, smoothing=0.0, **kwargs):\n",
    "        \"\"\"\n",
    "        padding_idx :\n",
    "            the index in spm-dictionary that used for padding. \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.padding_idx = padding_idx\n",
    "        self.smoothing = smoothing\n",
    "        self.labels_num = vocab_size\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        \"\"\"\n",
    "        Calculate the loss based on https://arxiv.org/pdf/1512.00567.pdf\n",
    "\n",
    "        Note that the loss should exclude those words after the padding index\n",
    "        found in target. for example: padding_idx = 0\n",
    "\n",
    "        x = torch.tensor([[[0.001, 0.002, 0.003, 0.004],\n",
    "                           [0.010, 0.020, 0.030, 0.040],\n",
    "                           [0.100, 0.200, 0.300, 0.400],\n",
    "                           [1.000, 2.000, 3.000, 4.000]],\n",
    "\n",
    "                          [[10.00, 20.00, 30.00, 40.00],\n",
    "                           [100.0, 200.0, 300.0, 400.0],\n",
    "                           [1000., 2000., 3000., 4000.],\n",
    "                           [10000, 20000, 30000, 40000]]])\n",
    "\n",
    "        target = torch.tensor([[2,\n",
    "                                1,\n",
    "                                0,\n",
    "                                0],\n",
    "                                \n",
    "                                [2,\n",
    "                                3,\n",
    "                                1,\n",
    "                                0]])\n",
    "        \n",
    "        values in x should be count into loss are:\n",
    "\n",
    "        x = torch.tensor([[[0.001, 0.002, 0.003, 0.004],\n",
    "                           [0.010, 0.020, 0.030, 0.040],\n",
    "                           [-----, -----, -----, -----],\n",
    "                           [-----, -----, -----, -----]],\n",
    "\n",
    "                          [[10.00, 20.00, 30.00, 40.00],\n",
    "                           [100.0, 200.0, 300.0, 400.0],\n",
    "                           [1000., 2000., 3000., 4000.],\n",
    "                           [-----, -----, -----, -----]]])\n",
    "\n",
    "        Args:\n",
    "        - x  :\n",
    "            (batch, length, dim = vocab_sz), predictions made by model\n",
    "        - target :\n",
    "            (batch, length)\n",
    "        \"\"\"\n",
    "        # expand dim of target\n",
    "        if target.dim() == x.dim() - 1:\n",
    "            target = target.unsqueeze(-1)\n",
    "\n",
    "        # gather values in x for Hqp\n",
    "        Hqp = -x.gather(dim=-1, index=target)\n",
    "        # sum up dimensional values of x to Hup\n",
    "        Hup = -x.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        # exclude those padded values, i.e, set them as 0\n",
    "        if self.padding_idx is not None:\n",
    "            pad_mask = target.eq(self.padding_idx)\n",
    "            Hqp.masked_fill_(pad_mask, 0.0)\n",
    "            Hup.masked_fill_(pad_mask, 0.0)\n",
    "\n",
    "        Hqp = Hqp.sum()\n",
    "        Hup = Hup.sum() / self.labels_num\n",
    "        batch_loss = ( (1 - self.smoothing) * Hqp + self.smoothing * Hup )\n",
    "        return batch_loss / target.size(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer : NoamOpt\n",
    "\n",
    "Reference:\n",
    "- https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "- https://github.com/hemingkx/ChineseNMT/blob/master/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    def __init__(self, vocab_size, factor, warmup, optimizer, **kwargs):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = vocab_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def __rate__(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "\n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        self._rate = self.__rate__()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = self._rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    \n",
    "# plot NoamOpt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "opts = [NoamOpt(512, 1, 4000, None), \n",
    "        NoamOpt(512, 1, 8000, None),\n",
    "        NoamOpt(256, 1, 4000, None)]\n",
    "plt.plot(np.arange(1, 20000), [[opt.__rate__(i) for opt in opts] for i in range(1, 20000)])\n",
    "plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class BeamHandler:\n",
    "    \"\"\"\n",
    "    Container for beams and scores.\n",
    "\n",
    "    For n+1 (batch)sentences:\n",
    "\n",
    "        [sentence-0],\n",
    "        [sentence-1],\n",
    "        ... ,\n",
    "        [sentence-n].\n",
    "\n",
    "    Repeating k+1 times of each sentence:\n",
    "\n",
    "        [beam-00],\n",
    "        [beam-01],\n",
    "        ... ,\n",
    "        [beam-0k],\n",
    "\n",
    "        [beam-10],\n",
    "        [beam-11],\n",
    "        ... ,\n",
    "        [beam-1k],\n",
    "\n",
    "        ... ,\n",
    "\n",
    "        [beam-n0],\n",
    "        [beam-n1],\n",
    "        ... ,\n",
    "        [beam-nk]\n",
    "\n",
    "    where beam-ij = sentence-ij, which is one of k+1 possible\n",
    "    outputs of sentence-i from sequence-to-sequence model.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_beams, batch_size, length, pad_id, bos_id, eos_id, device):\n",
    "        self.n_beams = n_beams\n",
    "        self.pad_id = pad_id\n",
    "        self.bos_id = bos_id\n",
    "\n",
    "        # create beams\n",
    "        self.beams = torch.tensor([[[0] for j in range(length)]\n",
    "                                   for i in range(batch_size * n_beams)], dtype=torch.long).to(device)\n",
    "        # create beams score\n",
    "        self.beams_score = torch.tensor([[[0.0]]\n",
    "                                         for i in range(batch_size * n_beams)], dtype=torch.float).to(device)\n",
    "\n",
    "        # create best beam\n",
    "        self.best_beam = torch.tensor([[[0] for j in range(length)]\n",
    "                                       for i in range(batch_size)], dtype=torch.long).to(device)\n",
    "\n",
    "    def init(self):\n",
    "        self.beams[:, 0] = self.bos_id\n",
    "        self.beams[:, 1:] = self.pad_id\n",
    "        self.beams_score[:] = 0.0\n",
    "        self.best_beam[:, 0] = self.bos_id\n",
    "        self.best_beam[:, 1:] = self.pad_id\n",
    "\n",
    "    def update(self, ranked_beam_ids, ranked_spm_ids, ranked_score, sentence_id, update_position):\n",
    "        # get corresponding beams-bundle and beams-score bundle of 'sentence_id'\n",
    "        bgn_idx = sentence_id * self.n_beams\n",
    "        beams_bundle = self.beams[bgn_idx : bgn_idx + self.n_beams]\n",
    "        beams_score_bundle = self.beams_score[bgn_idx : bgn_idx + self.n_beams]\n",
    "        # update best beam\n",
    "        self.best_beam[sentence_id] = beams_bundle[ranked_beam_ids[0]]\n",
    "        self.best_beam[sentence_id, update_position] = ranked_spm_ids[0]\n",
    "        # sort in ascending order with respect to beam ids\n",
    "        beam_ids, sort_rule = torch.sort(ranked_beam_ids)\n",
    "        spm_ids = torch.index_select(ranked_spm_ids, -1, sort_rule)\n",
    "        score_ = torch.index_select(ranked_score, -1, sort_rule)\n",
    "        # update beams bundle and beams-score bundle\n",
    "        for i in range(self.n_beams):\n",
    "            new_i = beam_ids[i]\n",
    "            if i != new_i:\n",
    "                beams_bundle[i] = beams_bundle[new_i]\n",
    "            beams_bundle[i, update_position] = spm_ids[i]\n",
    "            beams_score_bundle[i] = score_[i]\n",
    "\n",
    "\n",
    "class BeamSearch:\n",
    "    def __init__(self, model, n_beams, batch_size, length, vocab_size, pad_id, bos_id, eos_id, device):\n",
    "        self.model = model\n",
    "        self.n_beams = n_beams\n",
    "        self.batch_size = batch_size\n",
    "        self.length = length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.pad_id = pad_id\n",
    "        self.bos_id = bos_id\n",
    "        self.eos_id = eos_id\n",
    "        self.beams_handler = BeamHandler(n_beams, batch_size, length, pad_id, bos_id, eos_id, device)\n",
    "\n",
    "    def __update_beams__(self, beams_prob, word_idx):\n",
    "        \"\"\"\n",
    "        Update beams of each sentence by selecting top-k of accumulatted score.\n",
    "\n",
    "        Args:\n",
    "\n",
    "            beams_prob =\n",
    "\n",
    "                (batch size * number of beams, sentence length, vocabulary size)\n",
    "\n",
    "            where word_idx specifies the position of new word in sentence, and spm-id range\n",
    "            from 0 to (vocabulary size - 1).\n",
    "\n",
    "        Local variables:\n",
    "\n",
    "            competitions[i] =\n",
    "\n",
    "                [ score of (beam-i0 + spm-0), ... , score of (beam-i0 + spm-7999),\n",
    "                  score of (beam-i1 + spm-0), ... , score of (beam-i1 + spm-7999),\n",
    "                  ... ,\n",
    "                  score of (beam-ik + spm-0), ... , score of (beam-ik + spm-7999) ]\n",
    "\n",
    "            with vocabulary size = 8000.\n",
    "        \"\"\"\n",
    "        # beams_score = (batch_size * n_beams, 1, 1)\n",
    "        beams_score = self.beams_handler.beams_score\n",
    "        beams_spm_score = (beams_prob + beams_score)[:, word_idx]\n",
    "        competitions = []\n",
    "        for i in range(0, self.batch_size * self.n_beams, self.n_beams):   \n",
    "            competitions.append(torch.cat(\n",
    "                [spm_score for spm_score in beams_spm_score[i: i + self.n_beams]]))\n",
    "        # select top-(n_beams) score in competitions\n",
    "        for sentence_id in range(len(competitions)):\n",
    "            ranked_score, idx = torch.topk(competitions[sentence_id], self.n_beams)\n",
    "            ranked_beam_ids = idx // self.vocab_size\n",
    "            ranked_spm_ids = idx % self.vocab_size\n",
    "            self.beams_handler.update(ranked_beam_ids, ranked_spm_ids, ranked_score, sentence_id, word_idx + 1)\n",
    "\n",
    "    def prepare_beams_memory(self, memory):\n",
    "        \"\"\"Return beams-memory for decoding.\"\"\"\n",
    "        # memory = (length, batch size, dim)\n",
    "        return torch.repeat_interleave(memory, self.n_beams, 1)\n",
    "\n",
    "    def generate_sentence(self, beams_memory, beams_attn_mask):\n",
    "        \"\"\"Generate best sentence for score evaluation.\"\"\"\n",
    "        self.beams_handler.init()\n",
    "        # Exclude first bos_id in every beams_tgt, so our\n",
    "        #   loop only goes up to length - 1.\n",
    "        for word_idx in range(self.length - 1):\n",
    "            print(word_idx)\n",
    "            beams_tgt = self.beams_handler.beams\n",
    "            beams_prob = self.model.decode(tgt=beams_tgt, memory=beams_memory, tgt_mask=beams_attn_mask)\n",
    "            self.__update_beams__(beams_prob, word_idx)\n",
    "        return self.beams_handler.best_beam.squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "- loss/score recording\n",
    "- main functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/Score Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Records:\n",
    "  def __init__(self):\n",
    "    self.train_accus = []\n",
    "    self.train_losses = []\n",
    "    self.valid_accus = []\n",
    "    self.valid_losses = []\n",
    "\n",
    "  def record_train(self, loss, accu):\n",
    "    self.train_accus.append(accu)\n",
    "    self.train_losses.append(loss)\n",
    "\n",
    "  def record_valid(self, loss, accu):\n",
    "    self.valid_accus.append(accu)\n",
    "    self.valid_losses.append(loss)\n",
    "\n",
    "  def get_mean_train_records(self, length=1000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      length (:obj:`int`):\n",
    "        length of records to mean\n",
    "    Return:\n",
    "      mean loss, mean accuracy\n",
    "    \"\"\"\n",
    "    train_accus = np.array(self.train_accus)\n",
    "    train_losses = np.array(self.train_losses)\n",
    "    if len(train_accus) > length:\n",
    "      return train_losses[-length : -1].mean(), train_accus[-length : -1].mean()\n",
    "    else:\n",
    "      return train_losses[:-1].mean(), train_accus[:-1].mean()\n",
    "\n",
    "  def get_best_accu(self):\n",
    "    \"\"\"Get the best accuracies of training and validating stored.\"\"\"\n",
    "    return max(self.train_accus), max(self.valid_accus)\n",
    "\n",
    "  def get_records_for_plot(self):\n",
    "    \"\"\"\n",
    "    Get records for plotting.\n",
    "\n",
    "    Return:\n",
    "      gap between consecutive records, records of train (:obj:`dict`), records of validation (:obj:`dict`)\n",
    "    \"\"\"\n",
    "    train_accus = np.array(self.train_accus)\n",
    "    train_losses = np.array(self.train_losses)\n",
    "    valid_accus = np.array(self.valid_accus)\n",
    "    valid_losses = np.array(self.valid_losses)\n",
    "    # The length of training-accuracies (or losses) to mean so that the output lengths of train-\n",
    "    #   and valid-accuracies (or losses) be the same.\n",
    "    gap = (train_accus.size // valid_accus.size) + 1\n",
    "\n",
    "    out_train = {\"accuracy\": [], \"loss\": []}\n",
    "    out_valid = {\"accuracy\": valid_accus.tolist(), \"loss\": valid_losses.tolist()}\n",
    "\n",
    "    for i in range(valid_accus.size - 1):\n",
    "      out_train[\"accuracy\"].append(train_accus[i * gap : (i + 1) * gap].mean())\n",
    "      out_train[\"loss\"].append(train_losses[i * gap : (i + 1) * gap].mean())\n",
    "    out_train[\"accuracy\"].append(train_accus[(valid_accus.size - 1) * gap :].mean())\n",
    "    out_train[\"loss\"].append(train_losses[(valid_losses.size - 1) * gap :].mean())\n",
    "\n",
    "    return gap, out_train, out_valid\n",
    "\n",
    "# instanization\n",
    "records = Records()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Functions\n",
    "\n",
    "(*To run this part, make sure `/token/data/train.json` has been prepared in `Token`-part.*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch, model, device, criterion, optimizer, pad_id):\n",
    "    \"\"\"\n",
    "    Return:\n",
    "        loss\n",
    "    \"\"\"\n",
    "    batch = batch.to(device)\n",
    "    model.train()\n",
    "    preds = model(src=batch.src, tgt=batch.tgt,\n",
    "                  src_key_padding_mask=batch.src_key_padding_mask,\n",
    "                  tgt_key_padding_mask=batch.tgt_key_padding_mask)\n",
    "        # Create target by shifting batch.tgt 1 index right, because the first\n",
    "        #   token of each sentence is not 'bos_id'.\n",
    "    target = torch.full(batch.tgt.size(), pad_id).to(device)\n",
    "    target[:, : -1] = batch.tgt[:, 1:]\n",
    "    loss = criterion(preds, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics.functional.text import bleu_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def valid(dataloader, model, device, beam_search, beams_attn_mask, n_gram, tokenizer):\n",
    "    model.eval()\n",
    "    running_score = 0.0\n",
    "\n",
    "    pbar = tqdm(total=len(dataloader.dataset), ncols=0, desc='Valid', unit=' uttr')\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        batch = batch.to(device) \n",
    "        with torch.no_grad():\n",
    "            memory = model.encode(src=batch.src, src_key_padding_mask=batch.src_key_padding_mask)\n",
    "            beams_memory = beam_search.prepare_beams_memory(memory)\n",
    "            best_beam = beam_search.generate_sentence(beams_memory, beams_attn_mask)\n",
    "            # compute bleu score\n",
    "            #   note: each sentence in best beam starts with bos_id\n",
    "            candidates = best_beam.tolist()\n",
    "            references = batch.tgt.tolist()\n",
    "            candidates_text = tokenizer.decode(candidates)\n",
    "            references_text = tokenizer.decode(references)\n",
    "            candidates = [' '.join(list(s)) for s in candidates_text]\n",
    "            references = [' '.join(list(s)) for s in references_text]\n",
    "            running_score += float(bleu_score(candidates, references, n_gram))\n",
    "\n",
    "        pbar.update(dataloader.batch_size)\n",
    "        pbar.set_postfix(\n",
    "            score=f\"{running_score / (i+1):.2f}\" \n",
    "        )\n",
    "\n",
    "    pbar.close()\n",
    "    return running_score / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torcheval.metrics.functional.text import bleu\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"arguements\"\"\"\n",
    "    config = {\n",
    "        \"batch_size\" : 32,\n",
    "        \"n_workers\" : 0,\n",
    "        \"n_beams\" : 2,\n",
    "        \"n_gram\" : 4,\n",
    "        \"valid_steps\": 20,\n",
    "        \"save_steps\": 100,\n",
    "        \"total_steps\": 200,\n",
    "        \"model_path\": \"./model.ckpt\",\n",
    "        \"datapath_config\" : {\n",
    "            \"clean\" : [\"./data/clean/clean.en\", \"./data/clean/clean.zh\"],\n",
    "            \"token\" : \"./token/data/train.json\",\n",
    "        },\n",
    "        \"spm_config\" : {\n",
    "            \"vocab_size\" : 8000,\n",
    "            \"dir\" : \"./token\",\n",
    "            \"model_type\" : \"bpe\",\n",
    "            \"nm_rule\" : \"nmt_nfkc_cf\",\n",
    "            \"character_coverage\" : 1,\n",
    "            \"input_sentence_size\" : 1e6,\n",
    "            \"shuffle_input_sentence\" : True,\n",
    "            \"pad_id\" : 0,\n",
    "            \"unk_id\" : 1,\n",
    "            \"bos_id\" : 2,\n",
    "            \"eos_id\" : 3,\n",
    "        },\n",
    "        \"criterion_config\" : {\n",
    "            \"smoothing\" : 0.1,\n",
    "        },\n",
    "        \"noamOpt_config\" : {\n",
    "            \"factor\" : 1,\n",
    "            \"warmup\" : 1000,\n",
    "        },\n",
    "        \"model_config\" : {\n",
    "            \"pos_encode\" : PositionalEncoding,\n",
    "            \"d_model\" : 128,\n",
    "            \"nhead_encoder\" : 2,\n",
    "            \"nhead_decoder\" : 2,\n",
    "            \"encoder_layer\" : 2,\n",
    "            \"decoder_layer\" : 2,\n",
    "        },\n",
    "    }\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    device = (\n",
    "        \"cuda\" if torch.cuda.is_available()\n",
    "        else \"mps\" if torch.backends.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "    return device\n",
    "\n",
    "def main(\n",
    "        batch_size,\n",
    "        n_workers,\n",
    "        datapath_config,\n",
    "        spm_config,        \n",
    "        criterion_config,\n",
    "        noamOpt_config,\n",
    "        model_config,\n",
    "        n_beams,\n",
    "        n_gram,\n",
    "        valid_steps,\n",
    "        save_steps,\n",
    "        total_steps,\n",
    "        model_path,\n",
    "):\n",
    "    logger = set_logger(\"s2s-transformer\")\n",
    "\n",
    "    device = get_device()\n",
    "    logger.info(f\"{device} is used.\")\n",
    "\n",
    "    vocab_size = spm_config['vocab_size']\n",
    "    \n",
    "    train_loader, valid_loader, features_len = get_dataloader(datapath_config['token'], batch_size, n_workers, spm_config['pad_id'], collate_fn)\n",
    "    logger.info(\"Training/Validating data loaded.\")\n",
    "    \n",
    "    model = mk_model(vocab_size=vocab_size, batch_size=batch_size, length=features_len, **model_config).to(device)\n",
    "    logger.info(\"Model created.\")\n",
    "\n",
    "    criterion = LabelSmoothingCrossEntropyLoss(vocab_size=vocab_size, padding_idx=spm_config['pad_id'], **criterion_config)\n",
    "    optimizer = AdamW(params=model.parameters(), lr=0.0, betas=(0.9, 0.98), eps=1e-9)\n",
    "    noamOpt = NoamOpt(vocab_size=vocab_size, optimizer=optimizer, **noamOpt_config)\n",
    "    logger.info(\"Criterion set.\")\n",
    "\n",
    "    beam_search = BeamSearch(model, n_beams, batch_size, features_len, vocab_size,\n",
    "                             spm_config['pad_id'], spm_config['bos_id'], spm_config['eos_id'],\n",
    "                             device)\n",
    "    beams_attn_mask = model.decoder.generate_attn_mask(batch_size * n_beams, features_len).to(device)\n",
    "    logger.info(\"Beam search up.\")\n",
    "\n",
    "    spmToken_handler = SpmTokenHandler(spm_config, datapath_config)\n",
    "    en_tokenizer, zh_tokenizer = spmToken_handler.get_tokenizers()\n",
    "    logger.info(\"Tokenizer ready.\")\n",
    "\n",
    "    # training/validating\n",
    "    best_score = -1.0\n",
    "    best_state_dict = None\n",
    "\n",
    "    train_iter = iter(train_loader)\n",
    "\n",
    "    pbar = tqdm(total=valid_steps, ncols=0, desc='Train', unit=' step')\n",
    "    logger.info(\"===Start training===\")\n",
    "\n",
    "    for step in range(total_steps):\n",
    "        try:\n",
    "            batch = next(train_iter)\n",
    "        except StopIteration:\n",
    "            train_iter = iter(train_loader)\n",
    "            batch = next(train_iter)\n",
    "        \n",
    "        # train\n",
    "        mean_loss = train(batch, model, device, criterion, noamOpt, spm_config['pad_id'])\n",
    "\n",
    "        # log\n",
    "        pbar.update()\n",
    "        pbar.set_postfix(\n",
    "            loss=f\"{mean_loss:.2f}\",\n",
    "            step=step + 1\n",
    "        )\n",
    "\n",
    "        # validate\n",
    "        if (step + 1) % valid_steps == 0:\n",
    "            pbar.close()\n",
    "\n",
    "            score = valid(valid_loader, model, device, beam_search, beams_attn_mask, n_gram, zh_tokenizer)\n",
    "\n",
    "            # keep the best model\n",
    "            if best_score < score:\n",
    "                best_score = score\n",
    "                best_state_dict = model.state_dict()\n",
    "            \n",
    "            pbar = tqdm(total=valid_steps, ncols=0, desc='Train', unit=' step')\n",
    "        \n",
    "        # save best model\n",
    "        if (step + 1) % save_steps == 0 and best_state_dict is not None:\n",
    "            torch.save(best_state_dict, model_path)\n",
    "            pbar.write(f\"Step {step + 1}, best model saved. (score={best_score:.4f})\")\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(**parse_args())\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complexa_ML21_3a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
