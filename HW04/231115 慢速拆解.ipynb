{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 認識一下這些資料 ~ Penguin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json \n",
    "data_dir=r\"c:\\Users\\User\\Desktop\\my_ML\\2021\\HW4\\Dataset\"\n",
    "metadata_path = Path(data_dir) / \"metadata.json\"\n",
    "metadata = json.load(open(metadata_path))[\"speakers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 'n_mels', 40, 'speakers', 600),\n",
       " ['id10473',\n",
       "  'id10328',\n",
       "  'id10382',\n",
       "  'id11088',\n",
       "  'id10553',\n",
       "  'id11063',\n",
       "  'id10358',\n",
       "  'id10218',\n",
       "  'id10368',\n",
       "  'id10209'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(data_dir)   \n",
    "# 答:  WindowsPath('c:/Users/User/Desktop/my_ML/2021/HW4/Dataset')\n",
    "# 去研究 Path 跟 WindowsPath 類應該又歪掉了\n",
    "# 反正就是弄出網址的手段，我都用 import os , os.path.其他東西  \n",
    "\n",
    "metadata0=json.load(open(metadata_path))\n",
    "(   len(metadata0),list(metadata0)[0],metadata0['n_mels'],list(metadata0)[1],len(metadata)   ), list(metadata)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'feature_path': 'uttr-5c88b2f1803449789c36f14fb4d3c1eb.pt', 'mel_len': 652},\n",
       " {'feature_path': 'uttr-022a67baccc54bfda3567a7ac282a7b8.pt', 'mel_len': 564},\n",
       " {'feature_path': 'uttr-6a5c6e7231d642568633db13b6e429e1.pt', 'mel_len': 952},\n",
       " {'feature_path': 'uttr-e316c0cfdc8843c3ba3cfbb86af687b8.pt', 'mel_len': 1214},\n",
       " {'feature_path': 'uttr-3060b2b96bfc4614bb518914e557fea6.pt', 'mel_len': 353},\n",
       " {'feature_path': 'uttr-f1ac81adf7ba44c691ba389c9d171a05.pt', 'mel_len': 864},\n",
       " {'feature_path': 'uttr-930c993be52a4843b27f4961a44ecfe9.pt', 'mel_len': 352},\n",
       " {'feature_path': 'uttr-dbf99fe8f6b64cbb99e868e298813a4a.pt', 'mel_len': 429},\n",
       " {'feature_path': 'uttr-788cb6b47da94b34af1afb7e3a10f3f7.pt', 'mel_len': 276},\n",
       " {'feature_path': 'uttr-f0ff720eaad94ca9a383518c8e2fef59.pt', 'mel_len': 1560},\n",
       " {'feature_path': 'uttr-fcba6892138544d9ae2b199e709d327c.pt', 'mel_len': 594},\n",
       " {'feature_path': 'uttr-87460fee9ab44adba2b8b20c2f735686.pt', 'mel_len': 1352},\n",
       " {'feature_path': 'uttr-52bfb12f2ace4aa98f9799d1577fc2d9.pt', 'mel_len': 536},\n",
       " {'feature_path': 'uttr-0cf940452af647cb874c2ef04b7c921a.pt', 'mel_len': 658},\n",
       " {'feature_path': 'uttr-f4c8a81ff7fc46b68406cea18e2e1bff.pt', 'mel_len': 1024},\n",
       " {'feature_path': 'uttr-ed2b12279a404d34adf9489d9b827ae3.pt', 'mel_len': 442},\n",
       " {'feature_path': 'uttr-910dd65b0c944d11a7da767615403c58.pt', 'mel_len': 400},\n",
       " {'feature_path': 'uttr-6afc18d82c354898a6a8b1a75c393969.pt', 'mel_len': 404},\n",
       " {'feature_path': 'uttr-b7b9caac5dcd430f919d1dc2282856b8.pt', 'mel_len': 741},\n",
       " {'feature_path': 'uttr-55d4e07583a544bdb1eb1597b892379f.pt', 'mel_len': 1528},\n",
       " {'feature_path': 'uttr-71e99fa9a6f9401db9f8f05d88167188.pt', 'mel_len': 783},\n",
       " {'feature_path': 'uttr-99d98bf3ee7144909e93248706b89a3b.pt', 'mel_len': 504},\n",
       " {'feature_path': 'uttr-e73a28525c9b4b788f15fb776df7730b.pt', 'mel_len': 648},\n",
       " {'feature_path': 'uttr-ceb34c4f3d1149acbc3c2e6114305f4a.pt', 'mel_len': 1357},\n",
       " {'feature_path': 'uttr-3051dbf10ec3467abdf68223e98f21fa.pt', 'mel_len': 664},\n",
       " {'feature_path': 'uttr-7ed2439f5c3c4ec3bfbc97d7a7b99947.pt', 'mel_len': 1213},\n",
       " {'feature_path': 'uttr-a677791147dc483f9d6eb09baab4d73f.pt', 'mel_len': 549},\n",
       " {'feature_path': 'uttr-e1d0605393e742d3bb2e5d79fd93aa05.pt', 'mel_len': 758},\n",
       " {'feature_path': 'uttr-c0390ced58b94aebba4cce5317bef4f7.pt', 'mel_len': 1801},\n",
       " {'feature_path': 'uttr-e8005c02c46e4d7e878ba331cd232367.pt', 'mel_len': 699},\n",
       " {'feature_path': 'uttr-1a8d6a28037b4e10ada135a3b31d2179.pt', 'mel_len': 407},\n",
       " {'feature_path': 'uttr-9c54a3da69004826bbcc33b42be1df7c.pt', 'mel_len': 436},\n",
       " {'feature_path': 'uttr-a90ba548b1be487eb0612371ed64afcf.pt', 'mel_len': 338},\n",
       " {'feature_path': 'uttr-b9453ef0fae0413c9556f5e22e485924.pt', 'mel_len': 520},\n",
       " {'feature_path': 'uttr-fd16fe6f9f984beda3ef65db75512451.pt', 'mel_len': 453},\n",
       " {'feature_path': 'uttr-21da62f43a6545a7b1d7461aba8ffb11.pt', 'mel_len': 467},\n",
       " {'feature_path': 'uttr-067b8a999f76490fb7a5ef19fc2bdcdb.pt', 'mel_len': 702},\n",
       " {'feature_path': 'uttr-a6c00d1d2ec3466cbf4d6912f8eb1d37.pt', 'mel_len': 521},\n",
       " {'feature_path': 'uttr-b6c39611bacf4424af70dc33b6871d91.pt', 'mel_len': 715},\n",
       " {'feature_path': 'uttr-5452fffaf0464429b6659294b382f9d3.pt', 'mel_len': 354},\n",
       " {'feature_path': 'uttr-c83793bbca4f414688e1004c7b0597f5.pt', 'mel_len': 811},\n",
       " {'feature_path': 'uttr-308d7d02c5a042a794a76bf2165b0510.pt', 'mel_len': 538},\n",
       " {'feature_path': 'uttr-3210a601903d4313b19ec049e8248f1a.pt', 'mel_len': 667},\n",
       " {'feature_path': 'uttr-f1152365fbca4520806ab81cf13d4f43.pt', 'mel_len': 1268},\n",
       " {'feature_path': 'uttr-51fab252605a4dfc886da51b30c845a6.pt', 'mel_len': 536},\n",
       " {'feature_path': 'uttr-fa59567ab0ec4286981bc6d999ac658c.pt', 'mel_len': 464},\n",
       " {'feature_path': 'uttr-13c8bf94825d4f3c8c8fd118cbdfaffe.pt', 'mel_len': 390},\n",
       " {'feature_path': 'uttr-97a003bcb67c438d8a134b96e7e2ec31.pt', 'mel_len': 577},\n",
       " {'feature_path': 'uttr-01b8c9be911148389c9755f49a203b67.pt', 'mel_len': 1074},\n",
       " {'feature_path': 'uttr-d8615a9e615a44c79595c192efae519f.pt', 'mel_len': 1763},\n",
       " {'feature_path': 'uttr-578115eb2a9243d28cf8ac9fc36a340d.pt', 'mel_len': 404},\n",
       " {'feature_path': 'uttr-0d6fc2be594a45cb9e91d4c4370641d7.pt', 'mel_len': 502},\n",
       " {'feature_path': 'uttr-75137e81b2db48d0982347f762adc8a9.pt', 'mel_len': 1588},\n",
       " {'feature_path': 'uttr-4528aafb82054536b5b1d80de8dd13a9.pt', 'mel_len': 650},\n",
       " {'feature_path': 'uttr-38cc959f8a7f470a9b8a059e4b670e86.pt', 'mel_len': 408},\n",
       " {'feature_path': 'uttr-d333db7dd7864e78b03d3c0f0fee9ae7.pt', 'mel_len': 1352},\n",
       " {'feature_path': 'uttr-2d242ac87f7746b89207c291f42fb5ce.pt', 'mel_len': 488},\n",
       " {'feature_path': 'uttr-0823230fbcae41b4967ce1a38f5ed75b.pt', 'mel_len': 481},\n",
       " {'feature_path': 'uttr-b0b4ae5a6e304b77997ca01b4d0767c6.pt', 'mel_len': 424},\n",
       " {'feature_path': 'uttr-18d070782e1a438dabc4ab5b6419569b.pt', 'mel_len': 1033},\n",
       " {'feature_path': 'uttr-635f6df196774918affc5fcfa5c5fee6.pt', 'mel_len': 330},\n",
       " {'feature_path': 'uttr-fc1d6585d9694146988e857fd8f0297e.pt', 'mel_len': 1621},\n",
       " {'feature_path': 'uttr-ec94a906285f431387a043596ce92811.pt', 'mel_len': 1603},\n",
       " {'feature_path': 'uttr-40d3cf309e48482d856175d4de960777.pt', 'mel_len': 520},\n",
       " {'feature_path': 'uttr-de9c6a207735443db4ffa4db8cef1530.pt', 'mel_len': 1200},\n",
       " {'feature_path': 'uttr-32a8199f16484a798d2e7f74b63e808a.pt', 'mel_len': 1392},\n",
       " {'feature_path': 'uttr-cb738c94238040f39653dded72378ae4.pt', 'mel_len': 677},\n",
       " {'feature_path': 'uttr-89e09e2603d348df8f25915bfc8b191d.pt', 'mel_len': 425},\n",
       " {'feature_path': 'uttr-82a1f3c9fb984265a835086cecb6b0d6.pt', 'mel_len': 616},\n",
       " {'feature_path': 'uttr-cb4d382092f347a0afc2c10898e2e33f.pt', 'mel_len': 864},\n",
       " {'feature_path': 'uttr-6047eb80a6bc43b6a7cd2473edb96076.pt', 'mel_len': 696},\n",
       " {'feature_path': 'uttr-6f477d1baa2241b2a3fa7f6aaa56ad88.pt', 'mel_len': 692},\n",
       " {'feature_path': 'uttr-7786318955a94f85a0faf80ecfb47e21.pt', 'mel_len': 994},\n",
       " {'feature_path': 'uttr-cfeeb5e3fe9a425c8b6a2a0a47350de3.pt', 'mel_len': 668},\n",
       " {'feature_path': 'uttr-cdafc190f1774d81873a873e07314137.pt', 'mel_len': 599},\n",
       " {'feature_path': 'uttr-988afa1899a943edbe3bdba4e79d9148.pt', 'mel_len': 1328},\n",
       " {'feature_path': 'uttr-3f511ce5a96c4f12a1019b94718a6ed4.pt', 'mel_len': 1356},\n",
       " {'feature_path': 'uttr-02e770d4cc574f1c802e9c0fc6627fb8.pt', 'mel_len': 664},\n",
       " {'feature_path': 'uttr-d6cba1ab1a1e450ba3ca28484dc85330.pt', 'mel_len': 534},\n",
       " {'feature_path': 'uttr-7c47677b99ed4e50ac7cb4df97f1c05d.pt', 'mel_len': 421},\n",
       " {'feature_path': 'uttr-e58a3e7ef68a4faa9ae3d2a723953331.pt', 'mel_len': 1800},\n",
       " {'feature_path': 'uttr-2674a55c98f5472ab45c18b45f188f5e.pt', 'mel_len': 249},\n",
       " {'feature_path': 'uttr-c9b6b72152cd423e97b17fd07acb33c2.pt', 'mel_len': 540},\n",
       " {'feature_path': 'uttr-bdfeb92fc1d14726a9e6e7bf00465ab2.pt', 'mel_len': 709},\n",
       " {'feature_path': 'uttr-4abbef670a19463997da9882b065bac4.pt', 'mel_len': 350},\n",
       " {'feature_path': 'uttr-be93976e6b674d95ae7f46e59e0136ed.pt', 'mel_len': 675},\n",
       " {'feature_path': 'uttr-f9df2b88385340d7b601fd5e96a823af.pt', 'mel_len': 883},\n",
       " {'feature_path': 'uttr-a8fb6409145b4b308eeecdbee116480c.pt', 'mel_len': 1156},\n",
       " {'feature_path': 'uttr-713f09d406b04484bc8de414b1c422a8.pt', 'mel_len': 1471},\n",
       " {'feature_path': 'uttr-f055378288124f81a198f844f513e5eb.pt', 'mel_len': 684},\n",
       " {'feature_path': 'uttr-e97f00d94c9d4a5ca71591bd8fdfa180.pt', 'mel_len': 516},\n",
       " {'feature_path': 'uttr-b2549f50f6d641b78a5151bbc8e2a2c4.pt', 'mel_len': 617},\n",
       " {'feature_path': 'uttr-8f937154e6dc4ea7a099b9a7c8f87d72.pt', 'mel_len': 591},\n",
       " {'feature_path': 'uttr-42a49789e52640cba696841510a18bda.pt', 'mel_len': 581},\n",
       " {'feature_path': 'uttr-2687e1f4f7b040e2b6cc35ecf639838e.pt', 'mel_len': 1039},\n",
       " {'feature_path': 'uttr-bd4506528f414ecca2c2ff42540d99b5.pt', 'mel_len': 358},\n",
       " {'feature_path': 'uttr-3e48e069e43445e287b865b68b124fc0.pt', 'mel_len': 488},\n",
       " {'feature_path': 'uttr-b62e91dd6be040cc9b3cb23a360a393c.pt', 'mel_len': 322},\n",
       " {'feature_path': 'uttr-a9eb81691aed4a278691bd9c6ebd71b4.pt', 'mel_len': 378},\n",
       " {'feature_path': 'uttr-617cd86c59fa49d1b8800ef2ff94a465.pt', 'mel_len': 512},\n",
       " {'feature_path': 'uttr-9e538ce726314887a57cfc497209b560.pt', 'mel_len': 3372},\n",
       " {'feature_path': 'uttr-105c636c6a574fc7922b3973e16d1797.pt', 'mel_len': 760},\n",
       " {'feature_path': 'uttr-63398b02334a47529cb2b26d06c4e55e.pt', 'mel_len': 520},\n",
       " {'feature_path': 'uttr-e6cdec28bb1a47a2ae3031392ba99e0c.pt', 'mel_len': 409},\n",
       " {'feature_path': 'uttr-d784055211b54ddabcf9d8328080c9bc.pt', 'mel_len': 975},\n",
       " {'feature_path': 'uttr-761fcf14685149baa3b4dc3d7c90b61f.pt', 'mel_len': 837},\n",
       " {'feature_path': 'uttr-15ab3efce3384b9e8cf324a8f23f658f.pt', 'mel_len': 739},\n",
       " {'feature_path': 'uttr-e6fd7cc54d864c74849a0cfac80bf5d4.pt', 'mel_len': 428},\n",
       " {'feature_path': 'uttr-1e7d65ae70744cc38b60fa0f0c039574.pt', 'mel_len': 486},\n",
       " {'feature_path': 'uttr-4d1977cd18b54e9f889be49aefe1193e.pt', 'mel_len': 933},\n",
       " {'feature_path': 'uttr-c0ea38f91a354d59b3e8d7ff8f3b0e59.pt', 'mel_len': 643},\n",
       " {'feature_path': 'uttr-9856d5fdc3014593b82808f81c607567.pt', 'mel_len': 412},\n",
       " {'feature_path': 'uttr-597bf01090b6422aaae805d85cf73af4.pt', 'mel_len': 644},\n",
       " {'feature_path': 'uttr-221a853ac6e5412c952b9910f36a17ee.pt', 'mel_len': 307},\n",
       " {'feature_path': 'uttr-ee1949ba5db24c29bbae0954cbe383b8.pt', 'mel_len': 418},\n",
       " {'feature_path': 'uttr-deb9a2b3cb9043db9b56ed1697210abe.pt', 'mel_len': 2093},\n",
       " {'feature_path': 'uttr-15ef7567a162442997acfa47f263637b.pt', 'mel_len': 703},\n",
       " {'feature_path': 'uttr-ce3865a7d8aa4dcc9684176531daf9d3.pt', 'mel_len': 481},\n",
       " {'feature_path': 'uttr-cf10d4eb564c4d038e8caa17143009a8.pt', 'mel_len': 833},\n",
       " {'feature_path': 'uttr-ead168526abe4a8e9bad91a04efdae3f.pt', 'mel_len': 495},\n",
       " {'feature_path': 'uttr-ad253fd175b647e3adf7d1ea00fa759d.pt', 'mel_len': 469},\n",
       " {'feature_path': 'uttr-ef0e98d320e0455c9ca5a4b779cb87a8.pt', 'mel_len': 1228},\n",
       " {'feature_path': 'uttr-207eef0e209740c9b5869260a83b47eb.pt', 'mel_len': 2265},\n",
       " {'feature_path': 'uttr-31f4e4270bef4711a6a848e8bb3ad918.pt', 'mel_len': 548},\n",
       " {'feature_path': 'uttr-fc485ec799c74c43b03bc222984797f9.pt', 'mel_len': 1314},\n",
       " {'feature_path': 'uttr-10cc3245f316451fb258017dbd07130b.pt', 'mel_len': 613},\n",
       " {'feature_path': 'uttr-d1c06be240f147f99fec32c62b9a5a16.pt', 'mel_len': 497},\n",
       " {'feature_path': 'uttr-1f92f867731c465f8bbe7498499605ca.pt', 'mel_len': 858},\n",
       " {'feature_path': 'uttr-9fef099134eb4db6987777de73a75b7b.pt', 'mel_len': 448},\n",
       " {'feature_path': 'uttr-e38828f5806244d89feda3fab9260a8c.pt', 'mel_len': 1247},\n",
       " {'feature_path': 'uttr-1f3cb19c018446188ce3ad699614b7f6.pt', 'mel_len': 568},\n",
       " {'feature_path': 'uttr-4d645fb3d68643e68f4a5db907b2f685.pt', 'mel_len': 1068},\n",
       " {'feature_path': 'uttr-c600faa447f44465a3aa7d60369ad554.pt', 'mel_len': 700},\n",
       " {'feature_path': 'uttr-56238e8875234f51be657050fae21b87.pt', 'mel_len': 577},\n",
       " {'feature_path': 'uttr-5e7926fa336c420cb202ff05b15db671.pt', 'mel_len': 844},\n",
       " {'feature_path': 'uttr-e78733b99a9c467797bd762d5a5f2ee3.pt', 'mel_len': 428},\n",
       " {'feature_path': 'uttr-5c88b5d8cc6e4ac3850632737391b979.pt', 'mel_len': 2588},\n",
       " {'feature_path': 'uttr-3ab4ece7fc024601b987c7eeb115fd2d.pt', 'mel_len': 401},\n",
       " {'feature_path': 'uttr-3187c0c09725430494ef0f169c067e91.pt', 'mel_len': 652},\n",
       " {'feature_path': 'uttr-3c5d5b6027c7484095e448a4d359b211.pt', 'mel_len': 552},\n",
       " {'feature_path': 'uttr-0c3db689e9ab46bc89afc756566a8f88.pt', 'mel_len': 1243},\n",
       " {'feature_path': 'uttr-978f6ba6f3c9453080f76ae13a69ad4c.pt', 'mel_len': 392},\n",
       " {'feature_path': 'uttr-1a40826e20994b049570ada06c4ef8f1.pt', 'mel_len': 596},\n",
       " {'feature_path': 'uttr-eb85a9747a9b4f9bbdd3924c3d6ee652.pt', 'mel_len': 525},\n",
       " {'feature_path': 'uttr-613b42dcc5b744a1a8fdca61cfb5808e.pt', 'mel_len': 1011},\n",
       " {'feature_path': 'uttr-440cef5b4902436d9a81dde894393c41.pt', 'mel_len': 413},\n",
       " {'feature_path': 'uttr-e8c8721f4b7146a2b0d57c6debb719a7.pt', 'mel_len': 1002},\n",
       " {'feature_path': 'uttr-e23591c0957b4ebd87609670f2285ebd.pt', 'mel_len': 415},\n",
       " {'feature_path': 'uttr-dc4e46b60527428daf17b999a96759e2.pt', 'mel_len': 412},\n",
       " {'feature_path': 'uttr-ab55e382dc074fc08742964d5d00949e.pt', 'mel_len': 759},\n",
       " {'feature_path': 'uttr-98492e286fae4d4c96dd25a1d9a09ad8.pt', 'mel_len': 530},\n",
       " {'feature_path': 'uttr-fda4a682ffb54833ac5aeb51da10d9cf.pt', 'mel_len': 440},\n",
       " {'feature_path': 'uttr-b053188426014a86b68e0737783029b7.pt', 'mel_len': 752},\n",
       " {'feature_path': 'uttr-3dea74974ee94b6dadadbe510606f0da.pt', 'mel_len': 402},\n",
       " {'feature_path': 'uttr-f98d7dc33fed4d88baf96c072797a0f6.pt', 'mel_len': 664},\n",
       " {'feature_path': 'uttr-94de73e804a046c49b692be74cdd4d35.pt', 'mel_len': 765},\n",
       " {'feature_path': 'uttr-8f14504b6f1b47f8b69a5708460cb0e3.pt', 'mel_len': 944},\n",
       " {'feature_path': 'uttr-9a085b02508d47088a3a03918cf59a00.pt', 'mel_len': 820},\n",
       " {'feature_path': 'uttr-c7bea7174df2445cb95647cea7bf95b0.pt', 'mel_len': 880},\n",
       " {'feature_path': 'uttr-cf5453c06048480094d2ff6fea0cd657.pt', 'mel_len': 1696},\n",
       " {'feature_path': 'uttr-522d0a342c7b4a0c88a5089e39ea03de.pt', 'mel_len': 580},\n",
       " {'feature_path': 'uttr-895d9d41754d47d1ad57b58f3dfff973.pt', 'mel_len': 412},\n",
       " {'feature_path': 'uttr-c8fcd5853aaa4fcea27ebf39eae644d6.pt', 'mel_len': 898},\n",
       " {'feature_path': 'uttr-78ea947b84a7478e85baf450d8c34737.pt', 'mel_len': 913},\n",
       " {'feature_path': 'uttr-197cf0b08b4e4ce8a6408a22c15e1eb4.pt', 'mel_len': 943},\n",
       " {'feature_path': 'uttr-f62f1fcd16c8453684c6023cdefad39d.pt', 'mel_len': 634},\n",
       " {'feature_path': 'uttr-2c5bd833f62b42f8bbab7dbbfd58f51e.pt', 'mel_len': 508},\n",
       " {'feature_path': 'uttr-7fa3a71c09354cf78756aa73e115024c.pt', 'mel_len': 1838},\n",
       " {'feature_path': 'uttr-89e8de51c69743489b71200db9a89e77.pt', 'mel_len': 444},\n",
       " {'feature_path': 'uttr-03ab588d0aa44cd88b0afa9f9231fec8.pt', 'mel_len': 1504},\n",
       " {'feature_path': 'uttr-d6c7878fe7424fac8b499d56df740d9e.pt', 'mel_len': 420},\n",
       " {'feature_path': 'uttr-fbb6c0cc40544b5caea3749f1c7f597e.pt', 'mel_len': 2197},\n",
       " {'feature_path': 'uttr-ea678364d8124b72bb95e8a95cec13ef.pt', 'mel_len': 683},\n",
       " {'feature_path': 'uttr-3b8e7f0fca6b4ad28958cf4e8ed535ba.pt', 'mel_len': 540},\n",
       " {'feature_path': 'uttr-a02849fe278e44c0965900e5bee01805.pt', 'mel_len': 674},\n",
       " {'feature_path': 'uttr-caf385f0032140e0add0bbde9a6595cd.pt', 'mel_len': 414}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[      list(metadata)[0]    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一樣的要建立 myDataset , __len__, __getitem__ \n",
    "\n",
    ".key() 是什麼意思？ 是dict_key 物件， 跟 list(  metadata) 很像\n",
    "\n",
    "mel 是什麼？ 把 mel 理解為一段聲音的數字記錄\n",
    "\n",
    "json 檔案格式是什麼？ 對了，看一下助教影片吧！它就是記錄 dict 的東西，只要 import json 就能讀進來變成 dict\n",
    "\n",
    "助教講的 d_model 不知道是幹嘛的，是那個隨機 sample 舉例的 2 嗎？  .pt 檔裡面到底裝什麼？ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker=list(metadata.keys())[0] \n",
    "metadata.keys()\n",
    "type(metadata) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['speaker2id', 'id2speaker'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker=list(metadata.keys())[0]     \n",
    "#  第一名講者\n",
    "#  奇怪的資料型態  type(metadata.keys())==list  --> False \n",
    "\n",
    "mapping_path=r\"c:\\Users\\User\\Desktop\\my_ML\\2021\\HW4\\Dataset\\mapping.json\"\n",
    "mapping = json.load(open(mapping_path))\n",
    "speaker2id = mapping[\"speaker2id\"]\n",
    "mapping.keys() \n",
    "# speaker2id  # {'id10001': 0, 'id10005': 1,   'id10006': 2,  'id10007': 3,  'id10008': 4, .... }\n",
    "# mapping['id2speaker']  # '0': 'id10001', '1': 'id10005', '2': 'id10006', '3': 'id10007', '4': 'id10008',...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['uttr-5c88b2f1803449789c36f14fb4d3c1eb.pt', 217],\n",
       " ['uttr-022a67baccc54bfda3567a7ac282a7b8.pt', 217],\n",
       " ['uttr-6a5c6e7231d642568633db13b6e429e1.pt', 217],\n",
       " ['uttr-e316c0cfdc8843c3ba3cfbb86af687b8.pt', 217],\n",
       " ['uttr-3060b2b96bfc4614bb518914e557fea6.pt', 217]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data=[]\n",
    "for utterances in metadata[speaker]:\n",
    "    data.append([utterances[\"feature_path\"], speaker2id[speaker]])\n",
    "data[0:5]\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到目前為止 data  就是針對某一個講者的所有錄音紀錄，含有他的編號(id)，如上。 在底下每一個 .pt 檔讀出來都是一個向量的序列，會看到 shape 是 [652,40] ，代表是 652 項的 $\\mathbb R^{40}$ 的元素。 其中的 652 理解為上面的 mel_len ："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([652, 40])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0 \n",
    "feat_path, speaker = data[index]\n",
    "\n",
    "\n",
    "import torch \n",
    "import os \n",
    "mel = torch.load(os.path.join(data_dir, feat_path))\n",
    "mel.shape\n",
    "#len(mel)\n",
    "\n",
    "# ------------\n",
    "# 執行比較久, 4.2秒 \n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mel)   #torch.Tensor\n",
    "mel = torch.FloatTensor(mel)\n",
    "type(mel)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把長度超過 segment_len=128 的截取其中的 128 的一段出來： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 40]), tensor([217]), 128)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import random\n",
    "segment_len=128\n",
    "if len(mel) > segment_len:\n",
    "  # Randomly get the starting point of the segment.\n",
    "  start = random.randint(0, len(mel) - segment_len)\n",
    "  # Get a segment with \"segment_len\" frames.\n",
    "  mel = torch.FloatTensor(mel[start:start+segment_len])\n",
    "else:\n",
    "  mel = torch.FloatTensor(mel)\n",
    "# Turn the speaker id into long for computing loss later.\n",
    "speaker = torch.FloatTensor([speaker]).long()\n",
    "\n",
    "\n",
    "mel.shape, speaker, len(mel) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上截取出了一組訓練資料 $(\\vec{x},y)$=(mel,speaker)。底下取出一個批次： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 40])\n",
      "torch.Size([128, 40])\n",
      "torch.Size([128, 40])\n",
      "torch.Size([128, 40])\n",
      "torch.Size([128, 40])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, tuple, 2, torch.Size([128, 40]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import os \n",
    "import random\n",
    "\n",
    "\n",
    "batch=[] \n",
    "batch_size=5\n",
    "segment_len=128\n",
    "\n",
    "for index in range(batch_size):\n",
    "    feat_path, speaker = data[index]\n",
    "    mel = torch.load(os.path.join(data_dir, feat_path))\n",
    "    \n",
    "    if len(mel) > segment_len:\n",
    "    # Randomly get the starting point of the segment.\n",
    "        start = random.randint(0, len(mel) - segment_len)\n",
    "    # Get a segment with \"segment_len\" frames.\n",
    "        mel = torch.FloatTensor(mel[start:start+segment_len])\n",
    "    else:\n",
    "        mel = torch.FloatTensor(mel)\n",
    "    # Turn the speaker id into long for computing loss later.\n",
    "    speaker = torch.FloatTensor([speaker]).long()\n",
    "\n",
    "    print(mel.shape)\n",
    "    batch.append((mel,speaker))\n",
    "\n",
    "len(batch), type(batch[0]),len(batch[0]),batch[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader 的問題\n",
    "\n",
    "### collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在底下的 DataLoader 的部份多了 collate_fn 這個項要寫，即 \n",
    "\n",
    "        DataLoader(     \n",
    "            trainset,    \n",
    "            batch_size=batch_size,    \n",
    "            collate_fn=collate_batch\n",
    "            )\n",
    "  \n",
    "所以先來理解與它相關的種種。 \n",
    "\n",
    "從[pytorch之深入理解collate_fn](https://blog.csdn.net/qq_43391414/article/details/120462055) 知道，DataLoader 會先讀進 batch_size 這麼多筆資料，形成一個批次，接著把這個批次送進 collate_fn 函式再處理，才成為真正的批次。 如果 collate_fn 沒特別指定，那就是torch 內定的函式。 \n",
    "\n",
    "而這裡的 collate_fn 我們指定為 collate_batch。  \n",
    "\n",
    "(這是看了看就忘了的參考資料(collate_fn)： https://zhuanlan.zhihu.com/p/346332974 ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collate_batch 裡的問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "  # Process features within a batch.\n",
    "  \"\"\"Collate a batch of data.\"\"\"\n",
    "  mel, speaker = zip(*batch)\n",
    "  # Because we train the model batch by batch, we need to pad the features in the same batch to make their lengths the same.\n",
    "  mel = pad_sequence(mel, batch_first=True, padding_value=-20)    # pad log 10^(-20) which is very small value.\n",
    "  # mel: (batch size, length, 40)\n",
    "  return mel, torch.FloatTensor(speaker).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[ 3.7789,  5.0095,  6.2048,  ..., -1.7800, -2.6364, -4.1402],\n",
       "          [ 3.1319,  4.0267,  5.5865,  ..., -2.6058, -3.0292, -3.5199],\n",
       "          [ 3.5766,  3.6933,  5.1692,  ..., -1.9463, -2.5324, -3.1807],\n",
       "          ...,\n",
       "          [ 3.5081,  1.7898,  1.3294,  ..., -3.6261, -4.8100, -6.2782],\n",
       "          [ 2.3507,  1.5578, -0.2451,  ..., -4.6477, -4.9963, -6.1467],\n",
       "          [ 2.8042,  1.7832,  0.5915,  ..., -4.9164, -5.5058, -6.6358]]),\n",
       "  tensor([[ 1.2233,  0.7965, -0.3329,  ..., -6.3155, -6.4495, -6.1057],\n",
       "          [-0.4542, -0.6395, -1.0690,  ..., -5.9986, -5.3997, -5.7342],\n",
       "          [-1.3774, -1.0288, -0.4304,  ..., -5.8196, -5.7786, -6.4971],\n",
       "          ...,\n",
       "          [-0.1968, -0.0100, -0.8153,  ..., -2.9740, -3.0045, -2.9300],\n",
       "          [-0.7895,  0.5788,  0.4039,  ..., -3.2665, -2.4843, -2.6286],\n",
       "          [ 1.5114,  2.0339,  1.9580,  ..., -2.4905, -2.5871, -2.4693]]),\n",
       "  tensor([[-1.1504, -0.4570,  1.2115,  ..., -4.6025, -5.6709, -6.5668],\n",
       "          [ 0.2562,  0.5291,  2.2269,  ..., -4.9264, -5.4243, -6.8790],\n",
       "          [ 0.8475, -0.2891,  1.4659,  ..., -5.2508, -6.3791, -6.9436],\n",
       "          ...,\n",
       "          [ 0.4230,  0.8821,  2.1619,  ..., -4.6211, -5.6649, -6.4936],\n",
       "          [ 0.0813,  0.5449,  1.2372,  ..., -5.1480, -5.4092, -6.2040],\n",
       "          [ 0.3100,  0.5373,  1.5951,  ..., -5.3427, -5.4666, -6.3122]]),\n",
       "  tensor([[-1.1391,  0.5342,  0.4770,  ..., -7.6684, -8.4135, -9.6001],\n",
       "          [-0.0488,  2.1598,  2.6323,  ..., -8.3001, -7.9115, -8.3590],\n",
       "          [ 0.2327,  2.7612,  3.3464,  ..., -7.0602, -7.5906, -7.6639],\n",
       "          ...,\n",
       "          [ 2.5338,  4.1727,  3.6269,  ..., -3.4749, -2.8261, -5.1588],\n",
       "          [ 1.8630,  3.9744,  3.5512,  ..., -4.0058, -4.0809, -5.4301],\n",
       "          [ 1.7108,  3.8698,  3.6950,  ..., -3.4403, -3.4093, -4.7040]]),\n",
       "  tensor([[ 2.1870,  3.9600,  3.5551,  ..., -5.4312, -5.7194, -6.2710],\n",
       "          [ 0.9236,  3.7710,  3.7089,  ..., -5.2895, -5.9937, -7.3748],\n",
       "          [ 2.2035,  4.3751,  4.1984,  ..., -5.6110, -6.0130, -7.4296],\n",
       "          ...,\n",
       "          [ 3.3396,  1.6952, -0.0341,  ..., -3.3890, -4.7032, -5.9844],\n",
       "          [ 1.3674,  1.7887,  2.9879,  ..., -3.2609, -5.3836, -6.6823],\n",
       "          [ 1.8450,  2.3602,  2.6808,  ..., -4.1693, -5.5094, -7.2378]])),\n",
       " (tensor([217]), tensor([217]), tensor([217]), tensor([217]), tensor([217])))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel, speaker = zip(*batch)\n",
    "mel, speaker \n",
    "\n",
    "# zip(*batch) 看不出來 zip 在哪裡\n",
    "# 實在是看不出 pad_sequence 的效果。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以需要做點實測。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### padding = -20 的部份\n",
    "\n",
    "https://blog.csdn.net/m0_37586991/article/details/89467955 https://blog.csdn.net/HUSTHY/article/details/123916820\n",
    "\n",
    "\n",
    "#### zip(*batch) 的部份: \n",
    "\n",
    "(ref:  zip(*batch) https://blog.csdn.net/kz_java/article/details/124650719 https://ithelp.ithome.com.tw/articles/10218029)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "底下隨意用個 $4\\times 3$ 的矩陣造出資料集，當然造了一個 mytest 資料集進行測試，collate_fn 設成 my_fn，定義為 identity。 如此一來便試試看 zip 會得到什麼東西： \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.71511142 0.73934188 0.19947609]\n",
      " [0.64381424 0.03567123 0.29782309]\n",
      " [0.60832829 0.25268904 0.93861829]\n",
      " [0.25464282 0.71424791 0.5423858 ]]\n",
      "\n",
      "[(array([0.71511142, 0.73934188]), 100.1994760943883), (array([0.64381424, 0.03567123]), 100.29782309383232)]\n",
      "(array([0.71511142, 0.73934188]), array([0.64381424, 0.03567123])) (100.1994760943883, 100.29782309383232)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.7151, 0.7393],\n",
       "        [0.6438, 0.0357]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch ,numpy \n",
    "from torch.utils.data import Dataset ,DataLoader \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class mytest(Dataset):\n",
    "    def __init__(self,data):\n",
    "        self.data=data\n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx,0:2], self.data[idx,2]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "a=numpy.random.random((4,3))\n",
    "print(a,end='\\n\\n') \n",
    "testdata=mytest(a)\n",
    "\n",
    "\n",
    "def my_fn(x):\n",
    "    y=[]\n",
    "    for i in x: \n",
    "        i=list(i)\n",
    "        i[1]+=100\n",
    "        i=tuple(i) \n",
    "        y.append(i) \n",
    "    return y \n",
    "\n",
    "把a複製出來而已='''  \n",
    "[ ([0.23314527, 0.05247574], 0.63437053) ,\n",
    "  ([0.45916101, 0.2706865],  0.4225151) , \n",
    "  ([0.25938324, 0.75807604], 0.78212523) ,\n",
    "  ([0.42055378, 0.80188126], 0.95242862 ),]\n",
    "  https://pythontutor.com/render.html#mode=display\n",
    "'''\n",
    "\n",
    "可在Python_Visualization試試_一個程失敗的寫法_弄清list跟tupe變數的指標本質='''  \n",
    "x=[ ([0.23314527, 0.05247574], 0.63437053) ,\n",
    "  ([0.45916101, 0.2706865],  0.4225151) , \n",
    "]\n",
    "  \n",
    "def my_fn(x):\n",
    "    for i in x: \n",
    "        i=list(i)\n",
    "        i[1]+=100\n",
    "        i=tuple(i) \n",
    "    return x\n",
    "    \n",
    "c=my_fn(x)\n",
    "'''\n",
    "\n",
    "dataloader = DataLoader(testdata, batch_size=2, collate_fn=my_fn)\n",
    "batch=list(dataloader)[0]\n",
    "\n",
    "mel, speaker = zip(*batch)\n",
    "# 這邊的 mel 是很多個 mel , speaker 是很多個 speaker\n",
    "# zip 有拉鏈、拉開的意思 \n",
    "# 拉鏈拉上時是左跟右，一對一對綁在一起\n",
    "# 拉鏈拉下後是左邊自己一排，右邊自己一排。 \n",
    "# *batch 的星號則應該是 python 函數裡的 function(變數, *列表或序對, **字典) 格式的第二節\n",
    "print(batch)\n",
    "print( mel, speaker) \n",
    "\n",
    "mel=torch.FloatTensor(mel)\n",
    "mel = pad_sequence(mel, batch_first=True, padding_value=-20) \n",
    "mel \n",
    "\n",
    "# 執行兩次 warning 會消失\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.1020,   0.0501,   0.3898, -20.0000, -20.0000, -20.0000, -20.0000],\n",
       "        [  0.4938,   0.1357,   0.7109,   0.7387,   0.0449, -20.0000, -20.0000],\n",
       "        [  0.7572,   0.7689,   0.2584,   0.9882,   0.8434,   0.1651,   0.3293]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.rand(3)\n",
    "b=torch.rand(5)\n",
    "c=torch.rand(7)\n",
    "pad_sequence([a,b,c],batch_first=True,padding_value=-20) \n",
    "\n",
    "# 補上 -20 而已， batch_first 差不多是直的排跟橫的排\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果有把652截取成128，那麼這一步 padding_value 就沒有用途了。 所以底下來查一下到底有沒有哪一段長度 < 128 ： "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 為了理解再下一格裡 for i in metadata 的意思"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it=iter(metadata)\n",
    "it   # <dict_keyiterator at 0x1b76634ae50>\n",
    "\n",
    "i=next(it)   # i = 'id10473'\n",
    "#list(metadata)[0]  # 'id10473'\n",
    "\n",
    "it2=iter(metadata[i] ) \n",
    "j=it2.__next__()   # 等價於 j=next(it2)\n",
    "j['mel_len']>=128  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 尋找長度 < 150 的 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id10986 uttr-c1f39fbeff39408684786b10595439fe.pt 129\n",
      "id11111 uttr-7d9b6e1c007b41419915a21b3946b626.pt 146\n",
      "id11111 uttr-c230eb5bd3a0400e900b08b18c265db8.pt 148\n",
      "id11060 uttr-f71b58229faf4c808b634b2512ed3e42.pt 96\n",
      "id10220 uttr-d0e4b0c8a4334ba9b6a5c2638550ce9d.pt 129\n"
     ]
    }
   ],
   "source": [
    "'''  \n",
    "找出長度小於指定數的所有 mel 和它的 speaker\n",
    "'''\n",
    "\n",
    "for i in metadata: #0['speakers']:\n",
    "    for j in metadata[i]:\n",
    "        if j['mel_len']<150 :   # 原是 128\n",
    "            print(i, j['feature_path'] ,j['mel_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 進入模型架構的部份"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假設以 [652,40] 的那個 mel 為例，在資料處理的階段已經把它截成 [128,40] 的尺寸。 底下的NN結構是先把 [128,40] 中的每個 $\\mathbb R^{40}$ 線性地變成 $\\mathbb R^{80}$，再丟進 attention 架構，不論是內積自專注或是並行自專注(multihead)。 \n",
    "\n",
    "因此 $\\rm{d}_{\\rm{model}}$ 所指的意思是進入 attention 的向量序列中，每一個向量的維度。 而根據 [原始論文](.....)，公式是\n",
    "\n",
    "$$\\rm{sfmx}\\it\\left(  \\frac{QK^{\\rm{T}}}{\\sqrt{d_k}} \\right)V,$$\n",
    "\n",
    "不過這裡的向量是列向量，李宏毅老師 [上課](https://www.youtube.com/watch?v=gmsMY5kc-zw#t=14m7s) 慣用的是行向量。假設輸入的向量串是 \n",
    "$$I = \\begin{pmatrix}|&&|\\\\a_1&\\cdots&a_n\\\\|&&|\\end{pmatrix}\\in \\mathbb R^{  d_{\\rm model}      \\times \\it n}$$ \n",
    "因此\n",
    "$$Q= \\begin{pmatrix}|&&|\\\\q_1&\\cdots&q_n\\\\|&&|\\end{pmatrix},\\,\\,\\,K=\\begin{pmatrix}|&&|\\\\k_1&\\cdots&k_n\\\\|&&|\\end{pmatrix},\\,\\,\\,V=\\begin{pmatrix}|&&|\\\\v_1&\\cdots&v_n\\\\|&&|\\end{pmatrix},$$ \n",
    "分別可表示為\n",
    "$$Q=W^{\\rm{q}}I,\\,\\,K=W^{\\rm{k}}I, \\,\\, V=W^{\\rm{v}}I\\text{。}$$\n",
    "這裡的 $W^{\\rm q},\\it W^{\\rm k} \\in\\mathbb R^{ d_1 \\times d_{\\rm model \\it}}$，而$W^{\\rm v}\\in\\mathbb R^{ d_{\\rm output} \\times d_{\\rm model \\it}}$。 下一步計算\n",
    "\n",
    "$$A=K^{\\rm T}Q, \\quad \\it A' = \\rm sfmx \\it(A),\\quad O =  V A' \\text{，}$$ \n",
    "\n",
    "此即輸出。 如果我們只關注 $Q,K,V$ 如何變成 $O$，那答案就會是論文裡的轉置： \n",
    "$$ V\\cdot \\rm sfmx(\\it K^{\\rm T\\it}V).$$ \n",
    "\n",
    ".\n",
    "\n",
    "有趣的是，向量的個數 n 一直都沒有在考量的範圍內。自專注就是這麼專注在其他變量上！ 話雖如此，但底下的程式碼根本不甩基本款的 Scalar Product Self-Attention： \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['self_attn.in_proj_weight',\n",
       " 'self_attn.in_proj_bias',\n",
       " 'self_attn.out_proj.weight',\n",
       " 'self_attn.out_proj.bias',\n",
       " 'linear1.weight',\n",
       " 'linear1.bias',\n",
       " 'linear2.weight',\n",
       " 'linear2.bias',\n",
       " 'norm1.weight',\n",
       " 'norm1.bias',\n",
       " 'norm2.weight',\n",
       " 'norm2.bias']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "  def __init__(self, d_model=80, n_spks=600, dropout=0.1):\n",
    "    super().__init__()\n",
    "    # Project the dimension of features from that of input into d_model.\n",
    "    self.prenet = nn.Linear(40, d_model)\n",
    "    # TODO:\n",
    "    #   Change Transformer to Conformer.\n",
    "    #   https://arxiv.org/abs/2005.08100\n",
    "    self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "      d_model=d_model, dim_feedforward=256, nhead=2\n",
    "    )\n",
    "    # self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n",
    "\n",
    "    # Project the the dimension of features from d_model into speaker nums.\n",
    "    self.pred_layer = nn.Sequential(\n",
    "      nn.Linear(d_model, d_model),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(d_model, n_spks),\n",
    "    )\n",
    "\n",
    "  def forward(self, mels):\n",
    "    \"\"\"\n",
    "    args:\n",
    "      mels: (batch size, length, 40)\n",
    "    return:\n",
    "      out: (batch size, n_spks)\n",
    "    \"\"\"\n",
    "    # out: (batch size, length, d_model)\n",
    "    out = self.prenet(mels)\n",
    "    # out: (length, batch size, d_model)\n",
    "    out = out.permute(1, 0, 2)\n",
    "    # The encoder layer expect features in the shape of (length, batch size, d_model).\n",
    "    out = self.encoder_layer(out)\n",
    "    # out: (batch size, length, d_model)\n",
    "    out = out.transpose(0, 1)\n",
    "    # mean pooling\n",
    "    stats = out.mean(dim=1)\n",
    "\n",
    "    # out: (batch, n_spks)\n",
    "    out = self.pred_layer(stats)\n",
    "    return out\n",
    "\n",
    "speaker_num=600 \n",
    "device='cpu'\n",
    "model = Classifier(n_spks=speaker_num).to(device)\n",
    "\n",
    "list(model.encoder_layer.state_dict()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# self attention 層"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那麼， TransformerEncoderLayer 究竟裝啥？ dim_feedforward 是什麼？ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    " # dim_feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoderLayer(\n",
       "  (self_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=80, out_features=80, bias=True)\n",
       "  )\n",
       "  (linear1): Linear(in_features=80, out_features=256, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (linear2): Linear(in_features=256, out_features=80, bias=True)\n",
       "  (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "d_model = 80 \n",
    "encoder_layer = nn.TransformerEncoderLayer(\n",
    "      d_model=d_model, dim_feedforward=256, nhead=2\n",
    "    )\n",
    "\n",
    "I = torch.rand(128,80) \n",
    "I.shape   # torch.Size([128, 80])\n",
    "\n",
    "dir(encoder_layer)\n",
    "dir(encoder_layer.self_attn.out_proj) \n",
    "encoder_layer.self_attn.out_proj.weight.data.shape \n",
    "encoder_layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不做不知道，一做嚇一跳。這些 linear, dropout , linear 的是什麼？ 還好幾個.... 這個測試還把一個 $\\mathbb R^{128\\times 80}$ 的向量序列丟進去，發現得到的輸出也是 $\\mathbb R^{128\\times 80}$ 。 從概念上合理，但技術上卻看不出與 \n",
    "\n",
    "        dim_feedforward=256 \n",
    "\n",
    "\n",
    "的關聯。 \n",
    "\n",
    "進去看 TransformerEncoderLayer 的程式碼吧。 一些簡要結果紀錄在 \n",
    "\n",
    "        torch的一堆transformer.ipynb \n",
    "        \n",
    "中， 遇到第一個比較大的困擾是 Dropout 是什麼，因為它沒在 [老師的投影片](https://www.youtube.com/watch?v=n9TlOhRjYoc#t=29m) 裡出現。  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout 是啥!? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tensor([[0.2376, 0.0000, 0.0000],\n",
      "        [0.3319, 0.0000, 0.0000],\n",
      "        [0.3795, 0.0000, 0.0000],\n",
      "        [0.6173, 0.0000, 0.0000],\n",
      "        [0.8108, 0.0000, 0.0000],\n",
      "        [0.6467, 1.2934, 2.0000],\n",
      "        [0.7943, 0.0000, 0.0000],\n",
      "        [0.9298, 1.8597, 2.0000],\n",
      "        [0.3702, 0.0000, 0.0000],\n",
      "        [0.7718, 0.0000, 0.0000]])\n",
      "\n",
      " tensor([[0.1940, 0.3880, 2.0000],\n",
      "        [0.2710, 0.5420, 2.0000],\n",
      "        [0.9168, 0.0000, 0.0000],\n",
      "        [0.9338, 0.0000, 0.0000],\n",
      "        [0.5225, 0.0000, 0.0000],\n",
      "        [0.3371, 0.0000, 0.0000],\n",
      "        [0.6093, 0.0000, 0.0000],\n",
      "        [0.1278, 0.2557, 2.0000],\n",
      "        [0.0063, 0.0126, 2.0000],\n",
      "        [0.9213, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# https://www.cnblogs.com/luckyplj/p/13424561.html \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "for i in range(2):   # 以下動作做兩次\n",
    "\n",
    "    a=torch.rand(10,1)   # 隨機的 10 維向量\n",
    "    b=torch.ones(10,3)   # 待會觀察結果用的\n",
    "    b[:,0]=a[:,0]\n",
    "    \n",
    "    b[:,1]=nn.Dropout(0.5)(a)[:,0]   \n",
    "    # 對 a 使用剔除機率 = 0.5 的 Dropout \n",
    "\n",
    "    b[:,2]=b[:,1]/b[:,0]\n",
    "    # 觀察保留的數據漲了幾倍\n",
    "\n",
    "    print('\\n', b)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "設0.5時，被保留的數字大致上在 $ 5\\pm 1$ 居多，$ 5\\pm 2$ 遠一點的情況較少，符合分佈。 設 0.1的話大致在 $9\\pm 1$ 左右，設 0.9 的話在 $2\\pm 2$ 。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerEncoderLayer 怎麼做到的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最主要的問題還是，中間的 dim_feedforward = 256 究竟在哪裡發生的。 畢竟就連 Encoder 的結構都知道後，看著各個有可能從 80 維漲到256維的部份主要就兩個地方： \n",
    "\n",
    "1. attention 的輸出\n",
    "2. FC 的中間層\n",
    "\n",
    "如果答案是1，那為 256維的名字是用 feedforward，覺得不合理。 若答案是 2，但 FC 是包在裡面的，我的直覺判斷是，它不會卡在裡面。 為什麼敢這麼武斷？因為認為 FC 的維度不會只變一次 (HW1, HW3 得到的認知)，再加上它沒有激發函數；另一邊，又因為 256 就出現在上面的 linear1 裡面，它並不像是 FC 裡的樣子。 \n",
    "\n",
    "所以 linear1 在哪裡就成了關鍵： "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回答： "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "它的 forward 函式主要部份是這樣寫的，輸入是 x，張量，它做了以下： \n",
    "\n",
    "        x = self.norm1(x + self._sa_block(x))\n",
    "        x = self.norm2(x + self._ff_block(x))\n",
    "\n",
    "回傳 x。 其中 _sa_block 是\n",
    "\n",
    "        def _sa_block(self, x: Tensor,\n",
    "                  attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: bool = False\n",
    "                  ) -> Tensor:\n",
    "                  \n",
    "                x = self.self_attn(x, x, x,\n",
    "                           attn_mask=attn_mask,\n",
    "                           key_padding_mask=key_padding_mask,\n",
    "                           need_weights=False, is_causal=is_causal)[0]\n",
    "                return self.dropout1(x)，\n",
    "\n",
    "而  _ff_block 是\n",
    "\n",
    "        def _ff_block(self, x: Tensor) -> Tensor:\n",
    "        \n",
    "                x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "                return self.dropout2(x)。 \n",
    "\n",
    "因此可以看出在 TransformerEncoderLayer 的輸入到輸出，這個 x 經歷了什麼。 結局是，linear1, linear2 還真的是 FC 裡的東西。 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 研究蟲蟲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 雜亂無章未旅順 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先拿到一個 batch 的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([0.71511142, 0.73934188]), 100.1994760943883),\n",
       " (array([0.64381424, 0.03567123]), 100.29782309383232)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_iterator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Documents\\myvscode\\231113-self attention\\230924 test\\231115 慢速拆解.ipynb Cell 47\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/myvscode/231113-self%20attention/230924%20test/231115%20%E6%85%A2%E9%80%9F%E6%8B%86%E8%A7%A3.ipynb#Y102sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m Classifier(n_spks\u001b[39m=\u001b[39mspeaker_num)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/myvscode/231113-self%20attention/230924%20test/231115%20%E6%85%A2%E9%80%9F%E6%8B%86%E8%A7%A3.ipynb#Y102sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(train_iterator)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/myvscode/231113-self%20attention/230924%20test/231115%20%E6%85%A2%E9%80%9F%E6%8B%86%E8%A7%A3.ipynb#Y102sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/myvscode/231113-self%20attention/230924%20test/231115%20%E6%85%A2%E9%80%9F%E6%8B%86%E8%A7%A3.ipynb#Y102sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m optimizer \u001b[39m=\u001b[39m AdamW(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_iterator' is not defined"
     ]
    }
   ],
   "source": [
    "model = Classifier(n_spks=speaker_num).to(device)\n",
    "\n",
    "\n",
    "batch = next(train_iterator)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mels, labels = batch\n",
    "mels = mels.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "outs = model(mels)\n",
    "\n",
    "loss = criterion(outs, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    " \n",
    " \n",
    "class myDataset(Dataset):\n",
    "  def __init__(self, data_dir, segment_len=128):\n",
    "    self.data_dir = data_dir\n",
    "    self.segment_len = segment_len\n",
    " \n",
    "    # Load the mapping from speaker neme to their corresponding id. \n",
    "    mapping_path = Path(data_dir) / \"mapping.json\"\n",
    "    mapping = json.load(mapping_path.open())\n",
    "    self.speaker2id = mapping[\"speaker2id\"]\n",
    " \n",
    "    # Load metadata of training data.\n",
    "    metadata_path = Path(data_dir) / \"metadata.json\"\n",
    "    metadata = json.load(open(metadata_path))[\"speakers\"]\n",
    " \n",
    "    # Get the total number of speaker.\n",
    "    self.speaker_num = len(metadata.keys())\n",
    "    self.data = []\n",
    "    for speaker in metadata.keys():\n",
    "      for utterances in metadata[speaker]:\n",
    "        self.data.append([utterances[\"feature_path\"], self.speaker2id[speaker]])\n",
    " \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    " \n",
    "  def __getitem__(self, index):\n",
    "    feat_path, speaker = self.data[index]\n",
    "    # Load preprocessed mel-spectrogram.\n",
    "    mel = torch.load(os.path.join(self.data_dir, feat_path))\n",
    " \n",
    "    # Segmemt mel-spectrogram into \"segment_len\" frames.\n",
    "    if len(mel) > self.segment_len:\n",
    "      # Randomly get the starting point of the segment.\n",
    "      start = random.randint(0, len(mel) - self.segment_len)\n",
    "      # Get a segment with \"segment_len\" frames.\n",
    "      mel = torch.FloatTensor(mel[start:start+self.segment_len])\n",
    "    else:\n",
    "      mel = torch.FloatTensor(mel)\n",
    "    # Turn the speaker id into long for computing loss later.\n",
    "    speaker = torch.FloatTensor([speaker]).long()\n",
    "    return mel, speaker\n",
    " \n",
    "  def get_speaker_number(self):\n",
    "    return self.speaker_num\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "  # Process features within a batch.\n",
    "  \"\"\"Collate a batch of data.\"\"\"\n",
    "  mel, speaker = zip(*batch)\n",
    "  # Because we train the model batch by batch, we need to pad the features in the same batch to make their lengths the same.\n",
    "  mel = pad_sequence(mel, batch_first=True, padding_value=-20)    # pad log 10^(-20) which is very small value.\n",
    "  # mel: (batch size, length, 40)\n",
    "  return mel, torch.FloatTensor(speaker).long()\n",
    "\n",
    "\n",
    "def get_dataloader(data_dir, batch_size, n_workers):\n",
    "  \"\"\"Generate dataloader\"\"\"\n",
    "  dataset = myDataset(data_dir)\n",
    "  speaker_num = dataset.get_speaker_number()\n",
    "  # Split dataset into training dataset and validation dataset\n",
    "  trainlen = int(0.9 * len(dataset))\n",
    "  lengths = [trainlen, len(dataset) - trainlen]\n",
    "  trainset, validset = random_split(dataset, lengths)\n",
    "\n",
    "  train_loader = DataLoader(\n",
    "    trainset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=n_workers,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_batch,\n",
    "  )\n",
    "  valid_loader = DataLoader(\n",
    "    validset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=n_workers,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_batch,\n",
    "  )\n",
    "\n",
    "  return train_loader, valid_loader, speaker_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "  def __init__(self, d_model=80, n_spks=600, dropout=0.1):\n",
    "    super().__init__()\n",
    "    # Project the dimension of features from that of input into d_model.\n",
    "    self.prenet = nn.Linear(40, d_model)\n",
    "    # TODO:\n",
    "    #   Change Transformer to Conformer.\n",
    "    #   https://arxiv.org/abs/2005.08100\n",
    "    self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "      d_model=d_model, dim_feedforward=256, nhead=2\n",
    "    )\n",
    "    # self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n",
    "\n",
    "    # Project the the dimension of features from d_model into speaker nums.\n",
    "    self.pred_layer = nn.Sequential(\n",
    "      nn.Linear(d_model, d_model),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(d_model, n_spks),\n",
    "    )\n",
    "\n",
    "  def forward(self, mels):\n",
    "    \"\"\"\n",
    "    args:\n",
    "      mels: (batch size, length, 40)\n",
    "    return:\n",
    "      out: (batch size, n_spks)\n",
    "    \"\"\"\n",
    "    # out: (batch size, length, d_model)\n",
    "    out = self.prenet(mels)\n",
    "    # out: (length, batch size, d_model)\n",
    "    out = out.permute(1, 0, 2)\n",
    "    # The encoder layer expect features in the shape of (length, batch size, d_model).\n",
    "    out = self.encoder_layer(out)\n",
    "    # out: (batch size, length, d_model)\n",
    "    out = out.transpose(0, 1)\n",
    "    # mean pooling\n",
    "    stats = out.mean(dim=1)\n",
    "\n",
    "    # out: (batch, n_spks)\n",
    "    out = self.pred_layer(stats)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "def get_cosine_schedule_with_warmup(\n",
    "  optimizer: Optimizer,\n",
    "  num_warmup_steps: int,\n",
    "  num_training_steps: int,\n",
    "  num_cycles: float = 0.5,\n",
    "  last_epoch: int = -1,\n",
    "):\n",
    "  \"\"\"\n",
    "  Create a schedule with a learning rate that decreases following the values of the cosine function between the\n",
    "  initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n",
    "  initial lr set in the optimizer.\n",
    "\n",
    "  Args:\n",
    "    optimizer (:class:`~torch.optim.Optimizer`):\n",
    "      The optimizer for which to schedule the learning rate.\n",
    "    num_warmup_steps (:obj:`int`):\n",
    "      The number of steps for the warmup phase.\n",
    "    num_training_steps (:obj:`int`):\n",
    "      The total number of training steps.\n",
    "    num_cycles (:obj:`float`, `optional`, defaults to 0.5):\n",
    "      The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0\n",
    "      following a half-cosine).\n",
    "    last_epoch (:obj:`int`, `optional`, defaults to -1):\n",
    "      The index of the last epoch when resuming training.\n",
    "\n",
    "  Return:\n",
    "    :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n",
    "  \"\"\"\n",
    "\n",
    "  def lr_lambda(current_step):\n",
    "    # Warmup\n",
    "    if current_step < num_warmup_steps:\n",
    "      return float(current_step) / float(max(1, num_warmup_steps))\n",
    "    # decadence\n",
    "    progress = float(current_step - num_warmup_steps) / float(\n",
    "      max(1, num_training_steps - num_warmup_steps)\n",
    "    )\n",
    "    return max(\n",
    "      0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n",
    "    )\n",
    "\n",
    "  return LambdaLR(optimizer, lr_lambda, last_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def model_fn(batch, model, criterion, device):\n",
    "  \"\"\"Forward a batch through the model.\"\"\"\n",
    "\n",
    "  mels, labels = batch\n",
    "  mels = mels.to(device)\n",
    "  labels = labels.to(device)\n",
    "\n",
    "  outs = model(mels)\n",
    "\n",
    "  loss = criterion(outs, labels)\n",
    "\n",
    "  # Get the speaker id with highest probability.\n",
    "  preds = outs.argmax(1)\n",
    "  # Compute accuracy.\n",
    "  accuracy = torch.mean((preds == labels).float())\n",
    "\n",
    "  return loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "def valid(dataloader, model, criterion, device): \n",
    "  \"\"\"Validate on validation set.\"\"\"\n",
    "\n",
    "  model.eval()\n",
    "  running_loss = 0.0\n",
    "  running_accuracy = 0.0\n",
    "  pbar = tqdm(total=len(dataloader.dataset), ncols=0, desc=\"Valid\", unit=\" uttr\")\n",
    "\n",
    "  for i, batch in enumerate(dataloader):\n",
    "    with torch.no_grad():\n",
    "      loss, accuracy = model_fn(batch, model, criterion, device)\n",
    "      running_loss += loss.item()\n",
    "      running_accuracy += accuracy.item()\n",
    "\n",
    "    pbar.update(dataloader.batch_size)\n",
    "    pbar.set_postfix(\n",
    "      loss=f\"{running_loss / (i+1):.2f}\",\n",
    "      accuracy=f\"{running_accuracy / (i+1):.2f}\",\n",
    "    )\n",
    "\n",
    "  pbar.close()\n",
    "  model.train()\n",
    "\n",
    "  return running_accuracy / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info]: Use cuda now!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "  \"\"\"arguments\"\"\"\n",
    "  config = {\n",
    "    \"data_dir\": r\"c:\\Users\\User\\Desktop\\Dataset\",\n",
    "    \"save_path\": \"model.ckpt\",\n",
    "    \"batch_size\": 32,\n",
    "    \"n_workers\": 8,\n",
    "    \"valid_steps\": 2000,\n",
    "    \"warmup_steps\": 1000,\n",
    "    \"save_steps\": 10000,\n",
    "    \"total_steps\": 70000,\n",
    "  }\n",
    "\n",
    "  return config\n",
    "\n",
    "\n",
    "def main(\n",
    "  data_dir,\n",
    "  save_path,\n",
    "  batch_size,\n",
    "  n_workers,\n",
    "  valid_steps,\n",
    "  warmup_steps,\n",
    "  total_steps,\n",
    "  save_steps,\n",
    "):\n",
    "  \"\"\"Main function.\"\"\"\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  print(f\"[Info]: Use {device} now!\")\n",
    "\n",
    "  train_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n",
    "  train_iterator = iter(train_loader)\n",
    "  print(f\"[Info]: Finish loading data!\",flush = True)\n",
    "\n",
    "  model = Classifier(n_spks=speaker_num).to(device)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = AdamW(model.parameters(), lr=1e-3)\n",
    "  scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "  print(f\"[Info]: Finish creating model!\",flush = True)\n",
    "\n",
    "  best_accuracy = -1.0\n",
    "  best_state_dict = None\n",
    "\n",
    "  pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n",
    "\n",
    "  for step in range(total_steps):\n",
    "    # Get data\n",
    "    try:\n",
    "      batch = next(train_iterator)\n",
    "    except StopIteration:\n",
    "      train_iterator = iter(train_loader)\n",
    "      batch = next(train_iterator)\n",
    "\n",
    "    loss, accuracy = model_fn(batch, model, criterion, device)\n",
    "    batch_loss = loss.item()\n",
    "    batch_accuracy = accuracy.item()\n",
    "\n",
    "    # Updata model\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Log\n",
    "    pbar.update()\n",
    "    pbar.set_postfix(\n",
    "      loss=f\"{batch_loss:.2f}\",\n",
    "      accuracy=f\"{batch_accuracy:.2f}\",\n",
    "      step=step + 1,\n",
    "    )\n",
    "\n",
    "    # Do validation\n",
    "    if (step + 1) % valid_steps == 0:\n",
    "      pbar.close()\n",
    "\n",
    "      valid_accuracy = valid(valid_loader, model, criterion, device)\n",
    "\n",
    "      # keep the best model\n",
    "      if valid_accuracy > best_accuracy:\n",
    "        best_accuracy = valid_accuracy\n",
    "        best_state_dict = model.state_dict()\n",
    "\n",
    "      pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n",
    "\n",
    "    # Save the best model so far.\n",
    "    if (step + 1) % save_steps == 0 and best_state_dict is not None:\n",
    "      torch.save(best_state_dict, save_path)\n",
    "      pbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n",
    "\n",
    "  pbar.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main(**parse_args())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penguin_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
