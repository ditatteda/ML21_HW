{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HW04.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC5KwRyl6Flp"
      },
      "source": [
        "# Task description\n",
        "- Classify the speakers of given features.\n",
        "- Main goal: Learn how to use transformer.\n",
        "- Baselines:\n",
        "  - Easy: Run sample code and know how to use transformer.\n",
        "  - Medium: Know how to adjust parameters of transformer.\n",
        "  - Hard: Construct [conformer](https://arxiv.org/abs/2005.08100) which is a variety of transformer.\n",
        "\n",
        "- Other links\n",
        "  - Kaggle: [link](https://www.kaggle.com/t/859c9ca9ede14fdea841be627c412322)\n",
        "  - Slide: [link](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/hw/HW04/HW04.pdf)\n",
        "  - Data: [link](https://drive.google.com/file/d/1T0RPnu-Sg5eIPwQPfYysipfcz81MnsYe/view?usp=sharing)\n",
        "  - Video (Chinese): [link](https://www.youtube.com/watch?v=EPerg2UnGaI)\n",
        "  - Video (English): [link](https://www.youtube.com/watch?v=Gpz6AUvCak0)\n",
        "  - Solution for downloading dataset fail.: [link](https://drive.google.com/drive/folders/13T0Pa_WGgQxNkqZk781qhc5T9-zfh19e?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import shutil\n",
        "shutil.copyfile('/content/drive/MyDrive/Colab Notebooks/self_attention/Dataset.zip','/content/Dataset.zip')\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile('/content/Dataset.zip','r') as zip_ref:\n",
        "  zip_ref.extractall('/content/')"
      ],
      "metadata": {
        "id": "YV7LBTjr02EO",
        "outputId": "3707430c-2bf9-4744-fb7c-33788f314ebb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPDoreyypeJE"
      },
      "source": [
        "# Download dataset\n",
        "- Please follow [here](https://drive.google.com/drive/folders/13T0Pa_WGgQxNkqZk781qhc5T9-zfh19e?usp=sharing) to download data\n",
        "- Data is [here](https://drive.google.com/file/d/1gaFy8RaQVUEXo2n0peCBR5gYKCB-mNHc/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvpaILXnJIcw",
        "outputId": "b926731f-128f-4567-ba32-6e7a355263f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!gdown --id 'https://drive.google.com/file/d/1gaFy8RaQVUEXo2n0peCBR5gYKCB-mNHc/view' --output Dataset.zip\n",
        "!unzip Dataset.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Access denied with the following error:\n",
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=https://drive.google.com/file/d/1gaFy8RaQVUEXo2n0peCBR5gYKCB-mNHc/view \n",
            "\n",
            "unzip:  cannot find or open Dataset.zip, Dataset.zip.zip or Dataset.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1gYr_aoNDue"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz_NpuAipk3h"
      },
      "source": [
        "## Dataset\n",
        "- Original dataset is [Voxceleb1](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/).\n",
        "- The [license](https://creativecommons.org/licenses/by/4.0/) and [complete version](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/files/license.txt) of Voxceleb1.\n",
        "- We randomly select 600 speakers from Voxceleb1.\n",
        "- Then preprocess the raw waveforms into mel-spectrograms.\n",
        "\n",
        "- Args:\n",
        "  - data_dir: The path to the data directory.\n",
        "  - metadata_path: The path to the metadata.\n",
        "  - segment_len: The length of audio segment for training.\n",
        "- The architecture of data directory \\\\\n",
        "  - data directory \\\\\n",
        "  |---- metadata.json \\\\\n",
        "  |---- testdata.json \\\\\n",
        "  |---- mapping.json \\\\\n",
        "  |---- uttr-{random string}.pt \\\\\n",
        "\n",
        "- The information in metadata\n",
        "  - \"n_mels\": The dimention of mel-spectrogram.\n",
        "  - \"speakers\": A dictionary.\n",
        "    - Key: speaker ids.\n",
        "    - value: \"feature_path\" and \"mel_len\"\n",
        "\n",
        "\n",
        "For efficiency, we segment the mel-spectrograms into segments in the traing step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd7hoGhYtbXQ"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "class myDataset(Dataset):\n",
        "  def __init__(self, data_dir, segment_len=128):\n",
        "    self.data_dir = data_dir\n",
        "    self.segment_len = segment_len\n",
        "\n",
        "    # Load the mapping from speaker neme to their corresponding id.\n",
        "    mapping_path = Path(data_dir) / \"mapping.json\"\n",
        "    mapping = json.load(mapping_path.open())\n",
        "    self.speaker2id = mapping[\"speaker2id\"]\n",
        "\n",
        "    # Load metadata of training data.\n",
        "    metadata_path = Path(data_dir) / \"metadata.json\"\n",
        "    metadata = json.load(open(metadata_path))[\"speakers\"]\n",
        "\n",
        "    # Get the total number of speaker.\n",
        "    self.speaker_num = len(metadata.keys())\n",
        "    self.data = []\n",
        "    for speaker in metadata.keys():\n",
        "      for utterances in metadata[speaker]:\n",
        "        self.data.append([utterances[\"feature_path\"], self.speaker2id[speaker]])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    feat_path, speaker = self.data[index]\n",
        "    # Load preprocessed mel-spectrogram.\n",
        "    mel = torch.load(os.path.join(self.data_dir, feat_path))\n",
        "\n",
        "    # Segmemt mel-spectrogram into \"segment_len\" frames.\n",
        "    if len(mel) > self.segment_len:\n",
        "      # Randomly get the starting point of the segment.\n",
        "      start = random.randint(0, len(mel) - self.segment_len)\n",
        "      # Get a segment with \"segment_len\" frames.\n",
        "      mel = torch.FloatTensor(mel[start:start+self.segment_len])\n",
        "    else:\n",
        "      mel = torch.FloatTensor(mel)\n",
        "    # Turn the speaker id into long for computing loss later.\n",
        "    speaker = torch.FloatTensor([speaker]).long()\n",
        "    return mel, speaker\n",
        "\n",
        "  def get_speaker_number(self):\n",
        "    return self.speaker_num"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqJxjoi_NGnB"
      },
      "source": [
        "## Dataloader\n",
        "- Split dataset into training dataset(90%) and validation dataset(10%).\n",
        "- Create dataloader to iterate the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuT1AuFENI8t"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "def collate_batch(batch):\n",
        "  # Process features within a batch.\n",
        "  \"\"\"Collate a batch of data.\"\"\"\n",
        "  mel, speaker = zip(*batch)\n",
        "  # Because we train the model batch by batch, we need to pad the features in the same batch to make their lengths the same.\n",
        "  mel = pad_sequence(mel, batch_first=True, padding_value=-20)    # pad log 10^(-20) which is very small value.\n",
        "  # mel: (batch size, length, 40)\n",
        "  return mel, torch.FloatTensor(speaker).long()\n",
        "\n",
        "\n",
        "def get_dataloader(data_dir, batch_size, n_workers):\n",
        "  \"\"\"Generate dataloader\"\"\"\n",
        "  dataset = myDataset(data_dir)\n",
        "  speaker_num = dataset.get_speaker_number()\n",
        "  # Split dataset into training dataset and validation dataset\n",
        "  trainlen = int(0.9 * len(dataset))\n",
        "  lengths = [trainlen, len(dataset) - trainlen]\n",
        "  trainset, validset = random_split(dataset, lengths)\n",
        "\n",
        "  train_loader = DataLoader(\n",
        "    trainset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=n_workers,\n",
        "    pin_memory=True,\n",
        "    collate_fn=collate_batch,\n",
        "  )\n",
        "  valid_loader = DataLoader(\n",
        "    validset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=n_workers,\n",
        "    drop_last=True,\n",
        "    pin_memory=True,\n",
        "    collate_fn=collate_batch,\n",
        "  )\n",
        "\n",
        "  return train_loader, valid_loader, speaker_num\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0x6eXiHpr4R"
      },
      "source": [
        "# Model\n",
        "- TransformerEncoderLayer:\n",
        "  - Base transformer encoder layer in [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
        "  - Parameters:\n",
        "    - d_model: the number of expected features of the input (required).\n",
        "\n",
        "    - nhead: the number of heads of the multiheadattention models (required).\n",
        "\n",
        "    - dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
        "\n",
        "    - dropout: the dropout value (default=0.1).\n",
        "\n",
        "    - activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
        "\n",
        "- TransformerEncoder:\n",
        "  - TransformerEncoder is a stack of N transformer encoder layers\n",
        "  - Parameters:\n",
        "    - encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n",
        "\n",
        "    - num_layers: the number of sub-encoder-layers in the encoder (required).\n",
        "\n",
        "    - norm: the layer normalization component (optional).\n",
        "\n",
        "- Self-Attention-Pooling: (Code Ref: Shopping)\n",
        "  - [Self-Attentive Speaker Embeddings](https://www.danielpovey.com/files/2018_interspeech_xvector_attention.pdf)\n",
        "  - Instead of using mean pooling, we use self-attentive pooling.\n",
        "\n",
        "- To view model in details: may use `torchinfo.summary`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHX4eVj4tjtd"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "  def __init__(self, d_model=128, n_spks=600, dropout=0.2, dr=1):\n",
        "    super().__init__()\n",
        "    # Project the dimension of features from that of input into d_model.\n",
        "    self.prenet = nn.Linear(40, d_model)\n",
        "    # TODO:\n",
        "    #   Change Transformer to Conformer.\n",
        "    #   https://arxiv.org/abs/2005.08100\n",
        "    self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "      d_model=d_model, dim_feedforward=256, nhead=2\n",
        "    )\n",
        "    self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=4)\n",
        "\n",
        "    # self-attentive pooling\n",
        "    self.self_attn_pool = nn.Sequential(\n",
        "        nn.Linear(d_model, d_model),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(d_model, dr),\n",
        "        nn.Softmax(dim=1)\n",
        "    )\n",
        "\n",
        "    # Project the the dimension of features from d_model into speaker nums.\n",
        "    self.pred_layer = nn.Sequential(\n",
        "      #nn.Linear(d_model, d_model),\n",
        "      #nn.ReLU(),\n",
        "      nn.Linear(d_model, n_spks),\n",
        "    )\n",
        "\n",
        "  def forward(self, mels):\n",
        "    \"\"\"\n",
        "    args:\n",
        "      mels: (batch size, length, 40)\n",
        "    return:\n",
        "      out: (batch size, n_spks)\n",
        "    \"\"\"\n",
        "    # out: (batch size, length, d_model)\n",
        "    out = self.prenet(mels)\n",
        "    # out: (length, batch size, d_model)\n",
        "    out = out.permute(1, 0, 2)\n",
        "    # The encoder layer expect features in the shape of (length, batch size, d_model).\n",
        "    #out = self.encoder_layer(out)\n",
        "    out = self.encoder(out)\n",
        "    # out: (batch size, length, d_model)\n",
        "    out = out.transpose(0, 1)\n",
        "    # self-attentive pooling: annotation matrix (batch size, length, dr)\n",
        "    annoMat = self.self_attn_pool(out)\n",
        "    # stats: (batch size, dr, d_model)\n",
        "    stats = torch.bmm(annoMat.transpose(1, 2), out)\n",
        "    # stats: (batch size, d_model)\n",
        "    stats = torch.squeeze(stats, dim=1)\n",
        "    # mean pooling\n",
        "    #stats = out.mean(dim=1)\n",
        "\n",
        "    # out: (batch, n_spks)\n",
        "    out = self.pred_layer(stats)\n",
        "    return out\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-__DolPGpvDZ"
      },
      "source": [
        "# Learning rate schedule\n",
        "- For transformer architecture, the design of learning rate schedule is different from that of CNN.\n",
        "- Previous works show that the warmup of learning rate is useful for training models with transformer architectures.\n",
        "- The warmup schedule\n",
        "  - Set learning rate to 0 in the beginning.\n",
        "  - The learning rate increases linearly from 0 to initial learning rate during warmup period."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-0816BntqT9"
      },
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "\n",
        "def get_cosine_schedule_with_warmup(\n",
        "  optimizer: Optimizer,\n",
        "  num_warmup_steps: int,\n",
        "  num_training_steps: int,\n",
        "  num_cycles: float = 0.5,\n",
        "  last_epoch: int = -1,\n",
        "):\n",
        "  \"\"\"\n",
        "  Create a schedule with a learning rate that decreases following the values of the cosine function between the\n",
        "  initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n",
        "  initial lr set in the optimizer.\n",
        "\n",
        "  Args:\n",
        "    optimizer (:class:`~torch.optim.Optimizer`):\n",
        "      The optimizer for which to schedule the learning rate.\n",
        "    num_warmup_steps (:obj:`int`):\n",
        "      The number of steps for the warmup phase.\n",
        "    num_training_steps (:obj:`int`):\n",
        "      The total number of training steps.\n",
        "    num_cycles (:obj:`float`, `optional`, defaults to 0.5):\n",
        "      The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0\n",
        "      following a half-cosine).\n",
        "    last_epoch (:obj:`int`, `optional`, defaults to -1):\n",
        "      The index of the last epoch when resuming training.\n",
        "\n",
        "  Return:\n",
        "    :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n",
        "  \"\"\"\n",
        "\n",
        "  def lr_lambda(current_step):\n",
        "    # Warmup\n",
        "    if current_step < num_warmup_steps:\n",
        "      return float(current_step) / float(max(1, num_warmup_steps))\n",
        "    # decadence\n",
        "    progress = float(current_step - num_warmup_steps) / float(\n",
        "      max(1, num_training_steps - num_warmup_steps)\n",
        "    )\n",
        "    return max(\n",
        "      0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n",
        "    )\n",
        "\n",
        "  return LambdaLR(optimizer, lr_lambda, last_epoch)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP03FFo9K8DS"
      },
      "source": [
        "# Model Function\n",
        "- Model forward function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fohaLEFJK9-t"
      },
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def model_fn(batch, model, criterion, device):\n",
        "  \"\"\"Forward a batch through the model.\"\"\"\n",
        "\n",
        "  mels, labels = batch\n",
        "  mels = mels.to(device)\n",
        "  labels = labels.to(device)\n",
        "\n",
        "  outs = model(mels)\n",
        "\n",
        "  loss = criterion(outs, labels)\n",
        "\n",
        "  # Get the speaker id with highest probability.\n",
        "  preds = outs.argmax(1)\n",
        "  # Compute accuracy.\n",
        "  accuracy = torch.mean((preds == labels).float())\n",
        "\n",
        "  return loss, accuracy\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7cg-YrzLQcf"
      },
      "source": [
        "# Validate\n",
        "- Calculate accuracy of the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mD-_p6nWLO2L"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "\n",
        "def valid(dataloader, model, criterion, device):\n",
        "  \"\"\"Validate on validation set.\"\"\"\n",
        "\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  running_accuracy = 0.0\n",
        "  pbar = tqdm(total=len(dataloader.dataset), ncols=0, desc=\"Valid\", unit=\" uttr\")\n",
        "\n",
        "  for i, batch in enumerate(dataloader):\n",
        "    with torch.no_grad():\n",
        "      loss, accuracy = model_fn(batch, model, criterion, device)\n",
        "      running_loss += loss.item()\n",
        "      running_accuracy += accuracy.item()\n",
        "\n",
        "    pbar.update(dataloader.batch_size)\n",
        "    pbar.set_postfix(\n",
        "      loss=f\"{running_loss / (i+1):.2f}\",\n",
        "      accuracy=f\"{running_accuracy / (i+1):.2f}\",\n",
        "    )\n",
        "\n",
        "  pbar.close()\n",
        "  model.train()\n",
        "\n",
        "  return running_accuracy / len(dataloader)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noHXyal5p1W5"
      },
      "source": [
        "# Main function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracies Records\n",
        "Store the records of accuracy of training and validating, which are used in plotting."
      ],
      "metadata": {
        "id": "1ZtLMxxYp-rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class AccuRecords:\n",
        "  def __init__(self):\n",
        "    self.train_accus = []\n",
        "    self.valid_accus = []\n",
        "\n",
        "  def record_train_accu(self, accu):\n",
        "    self.train_accus.append(accu)\n",
        "\n",
        "  def record_valid_accu(self, accu):\n",
        "    self.valid_accus.append(accu)\n",
        "\n",
        "  def get_best_accu(self):\n",
        "    \"\"\"Get the best accuracies of training and validating stored.\"\"\"\n",
        "    return max(self.train_accus), max(self.valid_accus)\n",
        "\n",
        "  def get_records_for_plot(self):\n",
        "    \"\"\"\n",
        "    Get accuracies for plotting.\n",
        "\n",
        "    Return:\n",
        "      (gap between consecutive records, records of train, records of validation)\n",
        "    \"\"\"\n",
        "    train_accus = np.array(self.train_accus)\n",
        "    valid_accus = np.array(self.valid_accus)\n",
        "    # The length of training-accuracies to mean so that the outputs of train-\n",
        "    #   and valid-accuracies be the same.\n",
        "    gap = (train_accus.size // valid_accus.size) + 1\n",
        "\n",
        "    out_train = []\n",
        "    out_valid = valid_accus.tolist()\n",
        "\n",
        "    for i in range(valid_accus.size - 1):\n",
        "      out_train.append(train_accus[i * gap : (i + 1) * gap].mean())\n",
        "    out_train.append(train_accus[(valid_accus.size - 1) * gap :].mean())\n",
        "\n",
        "    return gap, out_train, out_valid\n",
        "\n",
        "# instanization\n",
        "accus_records = AccuRecords()"
      ],
      "metadata": {
        "id": "G0A1PAWsqHl4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "pPGNJ04CqD8-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chRQE7oYtw62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60695cda-8d19-4c0c-a28a-d7a91857eace"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "  \"\"\"arguments\"\"\"\n",
        "  config = {\n",
        "    \"data_dir\": \"./Dataset\",\n",
        "    \"save_path\": \"model.ckpt\",\n",
        "    \"batch_size\": 32,\n",
        "    \"n_workers\": 8,\n",
        "    \"valid_steps\": 2000,\n",
        "    \"warmup_steps\": 1000,\n",
        "    \"save_steps\": 10000,\n",
        "    \"total_steps\": 100000,\n",
        "  }\n",
        "\n",
        "  return config\n",
        "\n",
        "\n",
        "def main(\n",
        "  data_dir,\n",
        "  save_path,\n",
        "  batch_size,\n",
        "  n_workers,\n",
        "  valid_steps,\n",
        "  warmup_steps,\n",
        "  total_steps,\n",
        "  save_steps,\n",
        "):\n",
        "  \"\"\"Main function.\"\"\"\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(f\"[Info]: Use {device} now!\")\n",
        "\n",
        "  train_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n",
        "  train_iterator = iter(train_loader)\n",
        "  print(f\"[Info]: Finish loading data!\",flush = True)\n",
        "\n",
        "  model = Classifier(n_spks=speaker_num).to(device)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = AdamW(model.parameters(), lr=1e-3)\n",
        "  scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "  print(f\"[Info]: Finish creating model!\",flush = True)\n",
        "\n",
        "  best_accuracy = -1.0\n",
        "  best_state_dict = None\n",
        "\n",
        "  pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n",
        "\n",
        "  for step in range(total_steps):\n",
        "    # Get data\n",
        "    try:\n",
        "      batch = next(train_iterator)\n",
        "    except StopIteration:\n",
        "      train_iterator = iter(train_loader)\n",
        "      batch = next(train_iterator)\n",
        "\n",
        "    loss, accuracy = model_fn(batch, model, criterion, device)\n",
        "    batch_loss = loss.item()\n",
        "    batch_accuracy = accuracy.item()\n",
        "    accus_records.record_train_accu(batch_accuracy)\n",
        "\n",
        "    # Updata model\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Log\n",
        "    pbar.update()\n",
        "    pbar.set_postfix(\n",
        "      loss=f\"{batch_loss:.2f}\",\n",
        "      accuracy=f\"{batch_accuracy:.2f}\",\n",
        "      step=step + 1,\n",
        "    )\n",
        "\n",
        "    # Do validation\n",
        "    if (step + 1) % valid_steps == 0:\n",
        "      pbar.close()\n",
        "\n",
        "      valid_accuracy = valid(valid_loader, model, criterion, device)\n",
        "      accus_records.record_valid_accu(valid_accuracy)\n",
        "\n",
        "      # keep the best model\n",
        "      if valid_accuracy > best_accuracy:\n",
        "        best_accuracy = valid_accuracy\n",
        "        best_state_dict = model.state_dict()\n",
        "\n",
        "      pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n",
        "\n",
        "    # Save the best model so far.\n",
        "    if (step + 1) % save_steps == 0 and best_state_dict is not None:\n",
        "      torch.save(best_state_dict, save_path)\n",
        "      pbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n",
        "\n",
        "  pbar.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main(**parse_args())\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info]: Use cuda now!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info]: Finish loading data!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info]: Finish creating model!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:  98% 1952/2000 [02:37<00:02, 20.02 step/s, accuracy=0.28, loss=3.03, step=1952]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Train: 100% 2000/2000 [02:40<00:00, 12.45 step/s, accuracy=0.28, loss=3.39, step=2000]\n",
            "Valid: 100% 6944/6944 [00:16<00:00, 415.85 uttr/s, accuracy=0.27, loss=3.44]\n",
            "Train: 100% 2000/2000 [02:11<00:00, 15.17 step/s, accuracy=0.59, loss=2.11, step=4000]\n",
            "Valid: 100% 6944/6944 [00:15<00:00, 434.55 uttr/s, accuracy=0.41, loss=2.64]\n",
            "Train: 100% 2000/2000 [02:15<00:00, 14.77 step/s, accuracy=0.62, loss=1.88, step=6000]\n",
            "Valid: 100% 6944/6944 [00:16<00:00, 431.31 uttr/s, accuracy=0.53, loss=2.06]\n",
            "Train: 100% 2000/2000 [02:12<00:00, 15.12 step/s, accuracy=0.62, loss=1.70, step=8000]\n",
            "Valid: 100% 6944/6944 [00:16<00:00, 433.99 uttr/s, accuracy=0.59, loss=1.77]\n",
            "Train: 100% 2000/2000 [02:12<00:00, 15.15 step/s, accuracy=0.72, loss=1.35, step=1e+4]\n",
            "Valid: 100% 6944/6944 [00:16<00:00, 414.82 uttr/s, accuracy=0.62, loss=1.57]\n",
            "Train:   0% 4/2000 [00:00<01:51, 17.86 step/s, accuracy=0.44, loss=2.01, step=1e+4]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 10000, best model saved. (accuracy=0.6201)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100% 2000/2000 [02:13<00:00, 14.99 step/s, accuracy=0.62, loss=1.67, step=12000]\n",
            "Valid: 100% 6944/6944 [00:16<00:00, 422.64 uttr/s, accuracy=0.66, loss=1.43]\n",
            "Train: 100% 2000/2000 [02:15<00:00, 14.72 step/s, accuracy=0.78, loss=0.99, step=14000]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 403.36 uttr/s, accuracy=0.67, loss=1.39]\n",
            "Train: 100% 2000/2000 [02:15<00:00, 14.81 step/s, accuracy=0.69, loss=1.10, step=16000]\n",
            "Valid: 100% 6944/6944 [00:16<00:00, 410.22 uttr/s, accuracy=0.68, loss=1.31]\n",
            "Train: 100% 2000/2000 [02:15<00:00, 14.80 step/s, accuracy=0.72, loss=0.86, step=18000]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 393.46 uttr/s, accuracy=0.72, loss=1.20]\n",
            "Train: 100% 2000/2000 [02:15<00:00, 14.72 step/s, accuracy=0.72, loss=0.80, step=2e+4]\n",
            "Valid: 100% 6944/6944 [00:16<00:00, 410.26 uttr/s, accuracy=0.73, loss=1.14]\n",
            "Train:   0% 6/2000 [00:00<01:11, 27.95 step/s, accuracy=0.62, loss=1.39, step=2e+4]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 20000, best model saved. (accuracy=0.7310)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100% 2000/2000 [02:13<00:00, 14.96 step/s, accuracy=0.72, loss=1.18, step=22000]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 404.12 uttr/s, accuracy=0.73, loss=1.11]\n",
            "Train: 100% 2000/2000 [02:14<00:00, 14.92 step/s, accuracy=0.81, loss=0.61, step=24000]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 407.12 uttr/s, accuracy=0.74, loss=1.09]\n",
            "Train: 100% 2000/2000 [02:13<00:00, 14.99 step/s, accuracy=0.88, loss=0.36, step=26000]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 403.91 uttr/s, accuracy=0.74, loss=1.11]\n",
            "Train: 100% 2000/2000 [02:13<00:00, 15.00 step/s, accuracy=0.81, loss=1.05, step=28000]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 396.92 uttr/s, accuracy=0.75, loss=1.04]\n",
            "Train: 100% 2000/2000 [02:11<00:00, 15.22 step/s, accuracy=0.72, loss=1.26, step=3e+4]\n",
            "Valid: 100% 6944/6944 [00:16<00:00, 410.74 uttr/s, accuracy=0.76, loss=1.00]\n",
            "Train:   0% 9/2000 [00:00<00:52, 38.25 step/s, accuracy=0.84, loss=0.73, step=3e+4]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 30000, best model saved. (accuracy=0.7562)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100% 2000/2000 [02:09<00:00, 15.40 step/s, accuracy=0.81, loss=0.61, step=32000]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 396.37 uttr/s, accuracy=0.77, loss=0.96]\n",
            "Train: 100% 2000/2000 [02:09<00:00, 15.41 step/s, accuracy=0.75, loss=1.35, step=34000]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 398.19 uttr/s, accuracy=0.77, loss=0.96]\n",
            "Train: 100% 2000/2000 [02:09<00:00, 15.39 step/s, accuracy=0.78, loss=0.75, step=36000]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 397.12 uttr/s, accuracy=0.79, loss=0.89]\n",
            "Train: 100% 2000/2000 [02:08<00:00, 15.51 step/s, accuracy=0.81, loss=0.47, step=38000]\n",
            "Valid: 100% 6944/6944 [00:18<00:00, 383.08 uttr/s, accuracy=0.80, loss=0.86]\n",
            "Train: 100% 2000/2000 [02:13<00:00, 14.98 step/s, accuracy=0.69, loss=0.78, step=4e+4]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 389.79 uttr/s, accuracy=0.80, loss=0.87]\n",
            "Train:   0% 9/2000 [00:00<00:51, 38.88 step/s, accuracy=0.81, loss=0.74, step=4e+4]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 40000, best model saved. (accuracy=0.7959)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100% 2000/2000 [02:10<00:00, 15.38 step/s, accuracy=0.97, loss=0.24, step=42000]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 397.36 uttr/s, accuracy=0.81, loss=0.82]\n",
            "Train: 100% 2000/2000 [02:07<00:00, 15.63 step/s, accuracy=0.91, loss=0.40, step=44000]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 389.85 uttr/s, accuracy=0.80, loss=0.85]\n",
            "Train: 100% 2000/2000 [02:09<00:00, 15.47 step/s, accuracy=0.94, loss=0.43, step=46000]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 393.20 uttr/s, accuracy=0.81, loss=0.77]\n",
            "Train: 100% 2000/2000 [02:10<00:00, 15.31 step/s, accuracy=0.88, loss=0.42, step=48000]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 394.69 uttr/s, accuracy=0.81, loss=0.79]\n",
            "Train: 100% 2000/2000 [02:12<00:00, 15.08 step/s, accuracy=0.91, loss=0.35, step=5e+4]\n",
            "Valid: 100% 6944/6944 [00:18<00:00, 382.30 uttr/s, accuracy=0.82, loss=0.78]\n",
            "Train:   0% 7/2000 [00:00<01:03, 31.52 step/s, accuracy=0.78, loss=0.78, step=5e+4]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 50000, best model saved. (accuracy=0.8171)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100% 2000/2000 [02:11<00:00, 15.25 step/s, accuracy=0.88, loss=0.52, step=52000]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 390.28 uttr/s, accuracy=0.82, loss=0.75]\n",
            "Train: 100% 2000/2000 [02:15<00:00, 14.78 step/s, accuracy=1.00, loss=0.13, step=54000]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 390.83 uttr/s, accuracy=0.83, loss=0.73]\n",
            "Train: 100% 2000/2000 [02:08<00:00, 15.52 step/s, accuracy=0.91, loss=0.30, step=56000]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 396.22 uttr/s, accuracy=0.83, loss=0.69]\n",
            "Train: 100% 2000/2000 [02:10<00:00, 15.27 step/s, accuracy=0.94, loss=0.29, step=58000]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 386.51 uttr/s, accuracy=0.84, loss=0.65]\n",
            "Train: 100% 2000/2000 [02:15<00:00, 14.81 step/s, accuracy=0.94, loss=0.27, step=6e+4]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 404.04 uttr/s, accuracy=0.85, loss=0.65]\n",
            "Train:   0% 7/2000 [00:00<01:00, 32.81 step/s, accuracy=0.94, loss=0.28, step=6e+4]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 60000, best model saved. (accuracy=0.8461)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100% 2000/2000 [02:15<00:00, 14.78 step/s, accuracy=0.88, loss=0.36, step=62000]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 395.26 uttr/s, accuracy=0.84, loss=0.66]\n",
            "Train: 100% 2000/2000 [02:15<00:00, 14.72 step/s, accuracy=0.94, loss=0.27, step=64000]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 393.43 uttr/s, accuracy=0.85, loss=0.63]\n",
            "Train: 100% 2000/2000 [02:17<00:00, 14.54 step/s, accuracy=0.91, loss=0.28, step=66000]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 398.51 uttr/s, accuracy=0.86, loss=0.62]\n",
            "Train: 100% 2000/2000 [02:15<00:00, 14.75 step/s, accuracy=0.97, loss=0.24, step=68000]\n",
            "Valid: 100% 6944/6944 [00:16<00:00, 410.13 uttr/s, accuracy=0.86, loss=0.59]\n",
            "Train: 100% 2000/2000 [02:15<00:00, 14.77 step/s, accuracy=1.00, loss=0.08, step=7e+4]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 395.59 uttr/s, accuracy=0.86, loss=0.57]\n",
            "Train:   0% 6/2000 [00:00<01:12, 27.49 step/s, accuracy=0.91, loss=0.28, step=7e+4]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 70000, best model saved. (accuracy=0.8649)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100% 2000/2000 [02:18<00:00, 14.48 step/s, accuracy=0.97, loss=0.10, step=72000]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 406.46 uttr/s, accuracy=0.87, loss=0.55]\n",
            "Train: 100% 2000/2000 [02:15<00:00, 14.78 step/s, accuracy=0.97, loss=0.12, step=74000]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 405.94 uttr/s, accuracy=0.87, loss=0.55]\n",
            "Train: 100% 2000/2000 [02:17<00:00, 14.57 step/s, accuracy=0.91, loss=0.21, step=76000]\n",
            "Valid: 100% 6944/6944 [00:16<00:00, 408.49 uttr/s, accuracy=0.87, loss=0.55]\n",
            "Train: 100% 2000/2000 [02:15<00:00, 14.72 step/s, accuracy=1.00, loss=0.10, step=78000]\n",
            "Valid: 100% 6944/6944 [00:16<00:00, 412.56 uttr/s, accuracy=0.88, loss=0.52]\n",
            "Train: 100% 2000/2000 [02:16<00:00, 14.62 step/s, accuracy=0.97, loss=0.12, step=8e+4]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 408.13 uttr/s, accuracy=0.88, loss=0.51]\n",
            "Train:   0% 6/2000 [00:00<01:15, 26.39 step/s, accuracy=0.97, loss=0.08, step=8e+4]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 80000, best model saved. (accuracy=0.8844)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100% 2000/2000 [02:16<00:00, 14.64 step/s, accuracy=0.94, loss=0.13, step=82000]\n",
            "Valid: 100% 6944/6944 [00:15<00:00, 436.07 uttr/s, accuracy=0.88, loss=0.51]\n",
            "Train: 100% 2000/2000 [02:17<00:00, 14.56 step/s, accuracy=1.00, loss=0.06, step=84000]\n",
            "Valid: 100% 6944/6944 [00:16<00:00, 420.90 uttr/s, accuracy=0.89, loss=0.49]\n",
            "Train: 100% 2000/2000 [02:15<00:00, 14.71 step/s, accuracy=1.00, loss=0.05, step=86000]\n",
            "Valid: 100% 6944/6944 [00:16<00:00, 409.45 uttr/s, accuracy=0.89, loss=0.49]\n",
            "Train: 100% 2000/2000 [02:18<00:00, 14.43 step/s, accuracy=0.97, loss=0.13, step=88000]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 407.11 uttr/s, accuracy=0.89, loss=0.51]\n",
            "Train: 100% 2000/2000 [02:16<00:00, 14.63 step/s, accuracy=0.94, loss=0.22, step=9e+4]\n",
            "Valid: 100% 6944/6944 [00:16<00:00, 429.04 uttr/s, accuracy=0.89, loss=0.48]\n",
            "Train:   0% 8/2000 [00:00<00:54, 36.39 step/s, accuracy=1.00, loss=0.03, step=9e+4]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 90000, best model saved. (accuracy=0.8897)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100% 2000/2000 [02:12<00:00, 15.09 step/s, accuracy=0.97, loss=0.13, step=92000]\n",
            "Valid: 100% 6944/6944 [00:16<00:00, 417.49 uttr/s, accuracy=0.89, loss=0.47]\n",
            "Train: 100% 2000/2000 [02:11<00:00, 15.22 step/s, accuracy=1.00, loss=0.06, step=94000]\n",
            "Valid: 100% 6944/6944 [00:17<00:00, 400.72 uttr/s, accuracy=0.89, loss=0.47]\n",
            "Train: 100% 2000/2000 [02:12<00:00, 15.09 step/s, accuracy=0.91, loss=0.25, step=96000]\n",
            "Valid: 100% 6944/6944 [00:16<00:00, 413.87 uttr/s, accuracy=0.89, loss=0.47]\n",
            "Train: 100% 2000/2000 [02:12<00:00, 15.05 step/s, accuracy=1.00, loss=0.14, step=98000]\n",
            "Valid: 100% 6944/6944 [00:16<00:00, 423.02 uttr/s, accuracy=0.89, loss=0.48]\n",
            "Train: 100% 2000/2000 [02:13<00:00, 15.01 step/s, accuracy=0.97, loss=0.08, step=1e+5]\n",
            "Valid: 100% 6944/6944 [00:16<00:00, 409.31 uttr/s, accuracy=0.89, loss=0.50]\n",
            "Train:   0% 0/2000 [00:00<?, ? step/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100000, best model saved. (accuracy=0.8934)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting"
      ],
      "metadata": {
        "id": "LujR3Hv1VGBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "\n",
        "gap, train_accus, valid_accus = accus_records.get_records_for_plot()\n",
        "print(train_accus[-1], valid_accus[-1])\n",
        "\n",
        "figure(figsize=(6, 4))\n",
        "x = [gap * (i+1) for i in range(len(valid_accus))]\n",
        "plt.plot(x, train_accus, c=\"tab:red\", label=\"train\")\n",
        "plt.plot(x, valid_accus, c=\"tab:cyan\", label=\"valid\")\n",
        "plt.ylim(0.0, 1.0)\n",
        "plt.xlabel(\"steps\")\n",
        "plt.ylabel(\"accuracies\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "t-EwK8igVNCy",
        "outputId": "a821de37-8905-4a12-d786-d6aac8385a12"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9741959251665813 0.8870967741935484\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R2rx3AyHpQ-"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSuI3WY9Fz78"
      },
      "source": [
        "## Dataset of inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4evns0055Dsx"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class InferenceDataset(Dataset):\n",
        "  def __init__(self, data_dir):\n",
        "    testdata_path = Path(data_dir) / \"testdata.json\"\n",
        "    metadata = json.load(testdata_path.open())\n",
        "    self.data_dir = data_dir\n",
        "    self.data = metadata[\"utterances\"]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    utterance = self.data[index]\n",
        "    feat_path = utterance[\"feature_path\"]\n",
        "    mel = torch.load(os.path.join(self.data_dir, feat_path))\n",
        "\n",
        "    return feat_path, mel\n",
        "\n",
        "\n",
        "def inference_collate_batch(batch):\n",
        "  \"\"\"Collate a batch of data.\"\"\"\n",
        "  feat_paths, mels = zip(*batch)\n",
        "\n",
        "  return feat_paths, torch.stack(mels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAinHBG1GIWv"
      },
      "source": [
        "## Main funcrion of Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQaTt7VDHoRI"
      },
      "source": [
        "import json\n",
        "import csv\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def parse_args():\n",
        "  \"\"\"arguments\"\"\"\n",
        "  config = {\n",
        "    \"data_dir\": \"./Dataset\",\n",
        "    \"model_path\": \"./model.ckpt\",\n",
        "    \"output_path\": \"./output.csv\",\n",
        "  }\n",
        "\n",
        "  return config\n",
        "\n",
        "\n",
        "def main(\n",
        "  data_dir,\n",
        "  model_path,\n",
        "  output_path,\n",
        "):\n",
        "  \"\"\"Main function.\"\"\"\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(f\"[Info]: Use {device} now!\")\n",
        "\n",
        "  mapping_path = Path(data_dir) / \"mapping.json\"\n",
        "  mapping = json.load(mapping_path.open())\n",
        "\n",
        "  dataset = InferenceDataset(data_dir)\n",
        "  dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=8,\n",
        "    collate_fn=inference_collate_batch,\n",
        "  )\n",
        "  print(f\"[Info]: Finish loading data!\",flush = True)\n",
        "\n",
        "  speaker_num = len(mapping[\"id2speaker\"])\n",
        "  model = Classifier(n_spks=speaker_num).to(device)\n",
        "  model.load_state_dict(torch.load(model_path))\n",
        "  model.eval()\n",
        "  print(f\"[Info]: Finish creating model!\",flush = True)\n",
        "\n",
        "  results = [[\"Id\", \"Category\"]]\n",
        "  for feat_paths, mels in tqdm(dataloader):\n",
        "    with torch.no_grad():\n",
        "      mels = mels.to(device)\n",
        "      outs = model(mels)\n",
        "      preds = outs.argmax(1).cpu().numpy()\n",
        "      for feat_path, pred in zip(feat_paths, preds):\n",
        "        results.append([feat_path, mapping[\"id2speaker\"][str(pred)]])\n",
        "\n",
        "  with open(output_path, 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerows(results)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main(**parse_args())\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}