{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW6 SNGAN\n",
    "\n",
    "Resource:\n",
    "\n",
    "- https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.php\n",
    "\n",
    "- https://arxiv.org/pdf/1704.00028.pdf\n",
    "\n",
    "Structure:\n",
    "\n",
    "- Colab (Do this block if is running on `Colab`)\n",
    "\n",
    "- Dataset & Dataloader\n",
    "\n",
    "- Model\n",
    "\n",
    "- Training\n",
    "\n",
    "- Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab\n",
    "\n",
    "Mount drive and extract data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('/content/data', exist_ok=True)\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import shutil\n",
    "shutil.copyfile('/content/drive/MyDrive/ML2021/hw6/data/crypko_data.zip','/content/data/crypko_data.zip')\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile('/content/data/crypko_data.zip','r') as zip_ref:\n",
    "  zip_ref.extractall('/content/data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install qqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset/Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "1. Resize images to (64, 64).\n",
    "2. Linearly map values from [0, 1] to [-1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CrypkoDataset(Dataset):\n",
    "    def __init__(self, fnames, transform):\n",
    "        self.transform = transform\n",
    "        self.fnames = fnames\n",
    "        self.n_samples = len(self.fnames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.fnames[idx]\n",
    "        # load image\n",
    "        img = torchvision.io.read_image(fname)\n",
    "        # image transform\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "def get_dataset(root):\n",
    "    # get paths of images\n",
    "    fnames = glob.glob(os.path.join(root, \"*\"))\n",
    "    compose = [\n",
    "        v2.ToPILImage(),\n",
    "        # 1. Resize image to (64, 64)\n",
    "        v2.Resize((64, 64)),\n",
    "        # map values to [0, 1]\n",
    "        v2.ToTensor(),\n",
    "        # 2. Linearly map values to [-1, 1]\n",
    "        v2.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "    ]\n",
    "    transform = v2.Compose(compose)\n",
    "    dataset = CrypkoDataset(fnames, transform)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Some Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = get_dataset(\"./data/faces\")\n",
    "\n",
    "print(\"values in [-1, 1]:\")\n",
    "images = [dataset[i] for i in range(16)]\n",
    "grid_img = torchvision.utils.make_grid(images, nrow=4)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(grid_img.permute(1,2,0))\n",
    "plt.show()\n",
    "\n",
    "print(\"values in [0, 1]:\")\n",
    "images_origin = [(dataset[i]+1)/2 for i in range(16)]\n",
    "grid_img_origin = torchvision.utils.make_grid(images_origin, nrow=4)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(grid_img_origin.permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def get_dataloader(dataset, batch_size, n_workers):\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=n_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "`N` of the input/output shape stands for batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Normalisation\n",
    "\n",
    "- Reference:\n",
    "\n",
    "    - https://paperswithcode.com/method/spectral-normalization#:~:text=Spectral%20Normalization%20is%20a%20normalization,hyper%2Dparameter%20to%20be%20tuned.\n",
    "\n",
    "    - https://xiaosean5408.medium.com/sn-gan簡介-spectral-normalization-for-generative-adversarial-networks-f8fd784f2ad\n",
    "\n",
    "- Code:\n",
    "\n",
    "    - https://github.com/christiancosgrove/pytorch-spectral-normalization-gan/blob/12dcf945a6359301d63d1e0da3708cd0f0590b19/spectral_normalization.py#L14\n",
    "\n",
    "    - https://github.com/pfnet-research/sngan_projection/?tab=readme-ov-file#references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch.linalg import vector_norm\n",
    "\n",
    "\n",
    "def l2normalize(v, eps=1e-12):\n",
    "    return v / (vector_norm(v) + eps)\n",
    "\n",
    "\n",
    "class SpectralNorm(nn.Module):\n",
    "    def __init__(self, module, name='weight', power_iteration=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.module = module\n",
    "        self.name = name\n",
    "        self.power_iteration = power_iteration\n",
    "        if not self._made_params():\n",
    "            self._make_params()\n",
    "            self._update_u_v()\n",
    "\n",
    "    def _update_u_v(self):\n",
    "        u = getattr(self.module, self.name + '_u')\n",
    "        v = getattr(self.module, self.name + '_v')\n",
    "        w = getattr(self.module, self.name + '_bar')\n",
    "\n",
    "        height = w.shape[0]\n",
    "        for _ in range(self.power_iteration):\n",
    "            v.data = l2normalize(torch.mv(torch.t(w.view(height, -1).data), u.data))\n",
    "            u.data = l2normalize(torch.mv(w.view(height, -1).data, v.data))\n",
    "\n",
    "        sigma = u.dot(w.view(height, -1).mv(v))\n",
    "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
    "\n",
    "    def _made_params(self):\n",
    "        try:\n",
    "            u = getattr(self.module, self.name + '_u')\n",
    "            v = getattr(self.module, self.name + '_v')\n",
    "            w = getattr(self.module, self.name + '_bar')\n",
    "            return True\n",
    "        except AttributeError:\n",
    "            return False\n",
    "\n",
    "    def _make_params(self):\n",
    "        w = getattr(self.module, self.name)\n",
    "\n",
    "        height = w.shape[0]\n",
    "        width = w.view(height, -1).shape[1]\n",
    "\n",
    "        u = torch.tensor(w.data.new(height).normal_(0, 1))\n",
    "        v = torch.tensor(w.data.new(width).normal_(0.1))\n",
    "        #u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
    "        #v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
    "\n",
    "        u.data = l2normalize(u.data)\n",
    "        v.data = l2normalize(v.data)\n",
    "        w_bar = Parameter(w.data)\n",
    "\n",
    "        del self.module._parameters[self.name]\n",
    "\n",
    "        self.module.register_buffer(self.name + '_u', u)\n",
    "        self.module.register_buffer(self.name + '_v', v)\n",
    "        #self.module.register_parameter(self.name + '_u', u)\n",
    "        #self.module.register_parameter(self.name + '_v', v)\n",
    "        self.module.register_parameter(self.name + '_bar', w_bar)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._update_u_v()\n",
    "        return self.module.forward(*args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator & Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0.0)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Input shape: (N, in_dim)\n",
    "    Output shape: (N, 3, 64, 64)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, dim=64):\n",
    "        super().__init__()\n",
    "\n",
    "        def dconv_bn_relu(in_dim, out_dim):\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_dim, out_dim, 5, 2,\n",
    "                                   padding=2, output_padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(in_dim, dim * 8 * 4 * 4, bias=False),\n",
    "            nn.BatchNorm1d(dim * 8 * 4 * 4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.l2_5 = nn.Sequential(\n",
    "            dconv_bn_relu(dim * 8, dim * 4),\n",
    "            dconv_bn_relu(dim * 4, dim * 2),\n",
    "            dconv_bn_relu(dim * 2, dim),\n",
    "            nn.ConvTranspose2d(dim, 3, 5, 2, padding=2, output_padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.l1(x)\n",
    "        y = y.view(y.size(0), -1, 4, 4)\n",
    "        y = self.l2_5(y)\n",
    "        return y\n",
    "    \n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Input shape: (N, 3, 64, 64)\n",
    "    Output shape: (N, )\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, model_type : str, dim=64):\n",
    "        super().__init__()\n",
    "\n",
    "        def conv_sn_lrelu(in_dim, out_dim):\n",
    "            return nn.Sequential(\n",
    "                SpectralNorm(nn.Conv2d(in_dim, out_dim, 5, 2, 2)),\n",
    "                nn.LeakyReLU(0.2)\n",
    "            )\n",
    "        # K: kernel size, P: padding, S: stride\n",
    "        # x: (N, C, H, W)\n",
    "        # after one Conv2d(C, C_out, K, S, P)\n",
    "        # x: (N, C_out, (H + 2*P - K) // 2 + 1, (W + 2*P - K) // 2 + 1)\n",
    "        self.ls = nn.Sequential(\n",
    "            # x: (N, 3, 64, 64)\n",
    "            conv_sn_lrelu(in_dim, dim),\n",
    "\n",
    "            # x: (N, dim, 32, 32)\n",
    "            conv_sn_lrelu(dim, dim * 2),\n",
    "\n",
    "            # x:(N, dim * 2, 16, 16)\n",
    "            conv_sn_lrelu(dim * 2, dim * 4),\n",
    "\n",
    "            # x:(N, dim * 4, 8, 8)\n",
    "            conv_sn_lrelu(dim * 4, dim * 8),\n",
    "\n",
    "            # x: (N, dim * 8, 4, 4)\n",
    "            SpectralNorm(nn.Conv2d(dim * 8, 1, 4)),\n",
    "\n",
    "            # x: (N, 1, 1, 1)\n",
    "        )\n",
    "\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.ls(x)\n",
    "        y = y.view(-1)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools Kit\n",
    "\n",
    "Common tools kit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "#import datetime\n",
    "#from pathlib import Path\n",
    "\n",
    "def set_logger(log_name : str):\n",
    "    #\"\"\"write log under given log_path\"\"\"\n",
    "    #log_dir = Path(\"./log\")\n",
    "    #log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    #log_path = log_dir / f\"{datetime.datetime.now().date()}.log\"\n",
    "\n",
    "    logger = logging.getLogger(log_name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formater_s = logging.Formatter(\"%(name)s [%(levelname)s] %(message)s\")\n",
    "    #formater_f = logging.Formatter(\"%(asctime)s - %(name)s [%(levelname)s] %(message)s\")\n",
    "    \n",
    "    if not logger.handlers:\n",
    "        ## logging to file\n",
    "        #file_handler = logging.FileHandler(log_path)\n",
    "        #file_handler.setFormatter(formater_f)\n",
    "        #logger.addHandler(file_handler)\n",
    "\n",
    "        # logging to console\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        stream_handler.setFormatter(formater_s)\n",
    "        logger.addHandler(stream_handler)\n",
    "    \n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed\n",
    "\n",
    "Fix random seed for reproduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device\n",
    "\n",
    "Get running device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    return \"cuda\" if torch.cuda.is_available() \\\n",
    "      else \"mps\" if torch.backends.mps.is_available() \\\n",
    "      else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Save/Load Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ModelSaveLoad:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "    def save(self, G_state_dict, D_state_dict, epoch):\n",
    "        torch.save({\n",
    "            \"G_state_dict\" : G_state_dict,\n",
    "            \"D_state_dict\" : D_state_dict,\n",
    "            \"epoch\" : epoch\n",
    "        }, self.path)\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Return checkpoint.\"\"\"\n",
    "        return torch.load(self.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "- Classic Adversarial Loss\n",
    "\n",
    "- Hinge Loss\n",
    "\n",
    "Reference:\n",
    "https://github.com/pfnet-research/sngan_projection/blob/master/updater.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# classic adversarial loss\n",
    "def dc_loss_D(d_real, d_fake):\n",
    "    loss_r = torch.mean(F.softplus(-d_real))\n",
    "    loss_f = torch.mean(F.softplus(d_fake))\n",
    "    loss = loss_r + loss_f\n",
    "    return loss\n",
    "\n",
    "def dc_loss_G(d_fake):\n",
    "    loss = torch.mean(F.softplus(-d_fake))\n",
    "    return loss\n",
    "\n",
    "# hinge loss\n",
    "def hinge_loss_D(d_real, d_fake):\n",
    "    loss_r = torch.mean(F.relu(1.0 - d_real))\n",
    "    loss_f = torch.mean(F.relu(1.0 + d_fake))\n",
    "    loss = loss_r + loss_f\n",
    "    return loss\n",
    "\n",
    "def hinge_loss_G(d_fake):\n",
    "    loss = -torch.mean(d_fake)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from qqdm.notebook import qqdm\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    # loss_type = 'dc_loss' or 'hinge_loss'\n",
    "    config = {\n",
    "        \"loss_type\": 'dc_loss',\n",
    "        \"seed\": 2021,\n",
    "        \"batch_size\": 64,\n",
    "        \"z_dim\": 100,\n",
    "        \"n_workers\": 0,\n",
    "        \"n_epoch\": 1,\n",
    "        \"n_critic\": 5,\n",
    "        \"lr\": 1e-4,\n",
    "        \"onColab\": False,\n",
    "        \"load_ckpt\": False,\n",
    "        \"workspace_dir\": \"./\",\n",
    "        \"ckpt_dir_config\": {\n",
    "            \"local\": \"./checkpoints\",\n",
    "            \"drive\" : \"/content/drive/MyDrive/ML2021/hw6/checkpoints\"\n",
    "        },\n",
    "    }\n",
    "    return config\n",
    "\n",
    "def main(\n",
    "        loss_type,\n",
    "        seed,\n",
    "        batch_size,\n",
    "        z_dim,\n",
    "        n_workers,\n",
    "        n_epoch,\n",
    "        n_critic,\n",
    "        lr,\n",
    "        ckpt_dir_config,\n",
    "        onColab,\n",
    "        load_ckpt,\n",
    "        workspace_dir\n",
    "):\n",
    "    # check loss type\n",
    "    if loss_type != 'dc_loss' and loss_type != 'hinge_loss':\n",
    "        raise ValueError(f\"not configured input loss type: {loss_type}. | Choose 'dc_loss' or 'hinge_loss'.\")\n",
    "\n",
    "    model_type = 'sngan'\n",
    "    set_seed(seed)\n",
    "    logger = set_logger(model_type)\n",
    "\n",
    "    device = get_device()\n",
    "    logger.info(f'{device} is used.')\n",
    "\n",
    "    if onColab:\n",
    "        ckpt_dir = ckpt_dir_config['drive']\n",
    "    else:\n",
    "        ckpt_dir = ckpt_dir_config['local']\n",
    "    log_dir = os.path.join(workspace_dir, \"logs\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    # dataset/dataloader\n",
    "    data_dir = os.path.join(workspace_dir, \"data/faces\")\n",
    "    dataset = get_dataset(data_dir)\n",
    "    dataloader = get_dataloader(dataset, batch_size, n_workers)\n",
    "    logger.info('Data loaded.')\n",
    "\n",
    "    # parameter\n",
    "    pre_epoch = 0\n",
    "\n",
    "    # model\n",
    "    SL_handler = ModelSaveLoad(os.path.join(ckpt_dir, f'{model_type}.ckpt'))\n",
    "    G = Generator(in_dim=z_dim)\n",
    "    D = Discriminator(3, model_type)\n",
    "    if load_ckpt:\n",
    "        checkpoint = SL_handler.load()\n",
    "        G.load_state_dict(checkpoint['G_state_dict'])\n",
    "        D.load_state_dict(checkpoint['D_state_dict'])\n",
    "        pre_epoch = checkpoint['epoch']\n",
    "        logger.info(f'Checkpoint loaded. | Epoch: {pre_epoch}.')\n",
    "    else:\n",
    "        logger.info('New model set.')\n",
    "    G.to(device)\n",
    "    D.to(device)\n",
    "\n",
    "    # loss function\n",
    "    if loss_type == 'dc_loss':\n",
    "        criterion_G = dc_loss_G\n",
    "        criterion_D = dc_loss_D\n",
    "    elif loss_type == 'hinge_loss':\n",
    "        criterion_G = hinge_loss_G\n",
    "        criterion_D = hinge_loss_D\n",
    "    \n",
    "    # optimizer\n",
    "    optD = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optG = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    logger.info('Optimizer set.')\n",
    "\n",
    "    # distribution samples for generator to produce images\n",
    "    z_sample = torch.randn((100, z_dim)).to(device)\n",
    "\n",
    "    # training\n",
    "    G.train()\n",
    "    D.train()\n",
    "    logger.info('================ Start Training ================')\n",
    "    for epoch in range(pre_epoch, n_epoch):\n",
    "        progress_bar = qqdm(dataloader)\n",
    "        for i, data in enumerate(progress_bar):\n",
    "            imgs = data.to(device)\n",
    "\n",
    "            # batch size of imgs\n",
    "            bs = imgs.size(0)\n",
    "            \n",
    "            # ============================================\n",
    "            # Train Discriminator\n",
    "            # ============================================\n",
    "            z = torch.randn(bs, z_dim).to(device)\n",
    "            r_imgs = imgs\n",
    "            f_imgs = G(z)\n",
    "\n",
    "            d_real = D(r_imgs)\n",
    "            d_fake = D(f_imgs)\n",
    "\n",
    "            # loss for the discriminator\n",
    "            loss_D = criterion_D(d_real, d_fake)\n",
    "\n",
    "            # model backward\n",
    "            D.zero_grad()\n",
    "            loss_D.backward()\n",
    "\n",
    "            # update discriminator\n",
    "            optD.step()\n",
    "            \n",
    "            # ============================================\n",
    "            # Train Generator\n",
    "            # ============================================\n",
    "            # Viewing Generator + Discriminator as a compund model.\n",
    "            if i % n_critic == 0:\n",
    "                # generate some fake images\n",
    "                z = torch.randn((bs, z_dim)).to(device)\n",
    "                f_imgs = G(z)\n",
    "                \n",
    "                d_fake = D(f_imgs)\n",
    "\n",
    "                # loss for the generator\n",
    "                loss_G = criterion_G(d_fake)\n",
    "\n",
    "                # model backward\n",
    "                G.zero_grad()\n",
    "                loss_G.backward()\n",
    "\n",
    "                # update generator\n",
    "                optG.step()\n",
    "        \n",
    "            # set info for the progress bar\n",
    "            #   Note that the value of GAN loss is not directly related to\n",
    "            #   the quality of the generated images.\n",
    "            progress_bar.set_infos({\n",
    "                'Loss_D' : round(loss_D.item(), 4),\n",
    "                'Loss_G' : round(loss_G.item(), 4),\n",
    "                'Epoch' : epoch+1,\n",
    "                'Step' : i+1\n",
    "            })\n",
    "\n",
    "        G.eval()\n",
    "        f_imgs_sample = (G(z_sample).data + 1) / 2.0\n",
    "        filename = os.path.join(log_dir, f'Epoch_{epoch+1:03d}_{model_type}.jpg')\n",
    "        torchvision.utils.save_image(f_imgs_sample, filename, nrow=10)\n",
    "        print(f' | Save some samples to {filename}.')\n",
    "\n",
    "        # show generated images in jupyter notebook\n",
    "        grid_img = torchvision.utils.make_grid(f_imgs_sample.cpu(), nrow=10)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(grid_img.permute(1, 2, 0))\n",
    "        plt.show()\n",
    "        G.train()\n",
    "\n",
    "        if (epoch+1) % 5 == 0 or epoch == 0:\n",
    "            # save checkpoints\n",
    "            SL_handler.save(G.state_dict(), D.state_dict(), epoch)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(**parse_args())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    config = {\n",
    "        \"n_imgs\": 100,\n",
    "        \"z_dim\": 100,\n",
    "        \"onColab\": False,\n",
    "        \"workspace_dir\": \"./\",\n",
    "        \"ckpt_dir_config\": {\n",
    "            \"local\": \"./checkpoints\",\n",
    "            \"drive\" : \"/content/drive/MyDrive/ML2021/hw6/checkpoints\"\n",
    "        }\n",
    "    }\n",
    "    return config\n",
    "\n",
    "def main(\n",
    "        n_imgs,\n",
    "        z_dim,\n",
    "        onColab,\n",
    "        ckpt_dir_config,\n",
    "        workspace_dir\n",
    "):\n",
    "    model_type = 'sngan'\n",
    "    \n",
    "    device = get_device()\n",
    "\n",
    "    if onColab:\n",
    "        ckpt_dir = ckpt_dir_config['drive']\n",
    "    else:\n",
    "        ckpt_dir = ckpt_dir_config['local']\n",
    "\n",
    "    G = Generator(z_dim)\n",
    "    checkpoint = torch.load(os.path.join(ckpt_dir, f'{model_type}.ckpt'))\n",
    "    epoch = checkpoint['epoch']\n",
    "    G.load_state_dict(checkpoint['G_state_dict'])\n",
    "    G.to(device)\n",
    "    G.eval()\n",
    "\n",
    "    # generate n_imgs images and make a grid to save them\n",
    "    z_sample = torch.randn((n_imgs, z_dim)).to(device)\n",
    "    imgs_sample = (G(z_sample).data + 1) / 2.0\n",
    "    log_dir = os.path.join(workspace_dir, 'logs')\n",
    "    filename = os.path.join(log_dir, f'result_E{epoch+1:03d}_{model_type}.jpg')\n",
    "    torchvision.utils.save_image(imgs_sample, filename, nrow=10)\n",
    "\n",
    "    # show some images\n",
    "    n_display = min(int(n_imgs / 10), 32)\n",
    "    grid_img = torchvision.utils.make_grid(imgs_sample[:n_display].cpu(), nrow=10)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(**parse_args())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complexa_ML21_3a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
