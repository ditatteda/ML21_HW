{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW6 GAN\n",
    "\n",
    "Modification needed to achieve Medium baseline:\n",
    "\n",
    "- Model\n",
    "\n",
    "- Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset/Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "1. Resize images to (64, 64).\n",
    "2. Linearly map values from [0, 1] to [-1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CrypkoDataset(Dataset):\n",
    "    def __init__(self, fnames, transform):\n",
    "        self.transform = transform\n",
    "        self.fnames = fnames\n",
    "        self.n_samples = len(self.fnames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.fnames[idx]\n",
    "        # load image\n",
    "        img = torchvision.io.read_image(fname)\n",
    "        # image transform\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "def get_dataset(root):\n",
    "    # get paths of images\n",
    "    fnames = glob.glob(os.path.join(root, \"*\"))\n",
    "    compose = [\n",
    "        v2.ToPILImage(),\n",
    "        # 1. Resize image to (64, 64)\n",
    "        v2.Resize((64, 64)),\n",
    "        # map values to [0, 1]\n",
    "        v2.ToTensor(),\n",
    "        # 2. Linearly map values to [-1, 1]\n",
    "        v2.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "    ]\n",
    "    transform = v2.Compose(compose)\n",
    "    dataset = CrypkoDataset(fnames, transform)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Some Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = get_dataset(\"./data/faces\")\n",
    "\n",
    "print(\"values in [-1, 1]:\")\n",
    "images = [dataset[i] for i in range(16)]\n",
    "grid_img = torchvision.utils.make_grid(images, nrow=4)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(grid_img.permute(1,2,0))\n",
    "plt.show()\n",
    "\n",
    "print(\"values in [0, 1]:\")\n",
    "images_origin = [(dataset[i]+1)/2 for i in range(16)]\n",
    "grid_img_origin = torchvision.utils.make_grid(images_origin, nrow=4)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(grid_img_origin.permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def get_dataloader(dataset, batch_size, n_workers):\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=n_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "`N` of the input/output shape stands for batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0.0)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Input shape: (N, in_dim)\n",
    "    Output shape: (N, 3, 64, 64)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, dim=64):\n",
    "        super().__init__()\n",
    "\n",
    "        def dconv_bn_relu(in_dim, out_dim):\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_dim, out_dim, 5, 2,\n",
    "                                   padding=2, output_padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(in_dim, dim * 8 * 4 * 4, bias=False),\n",
    "            nn.BatchNorm1d(dim * 8 * 4 * 4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.l2_5 = nn.Sequential(\n",
    "            dconv_bn_relu(dim * 8, dim * 4),\n",
    "            dconv_bn_relu(dim * 4, dim * 2),\n",
    "            dconv_bn_relu(dim * 2, dim),\n",
    "            nn.ConvTranspose2d(dim, 3, 5, 2, padding=2, output_padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.l1(x)\n",
    "        y = y.view(y.size(0), -1, 4, 4)\n",
    "        y = self.l2_5(y)\n",
    "        return y\n",
    "    \n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Input shape: (N, 3, 64, 64)\n",
    "    Output shape: (N, )\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, dim=64):\n",
    "        super().__init__()\n",
    "\n",
    "        def conv_bn_lrelu(in_dim, out_dim):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_dim, out_dim, 5, 2, 2),\n",
    "                nn.BatchNorm2d(out_dim),\n",
    "                nn.LeakyReLU(0.2)\n",
    "            )\n",
    "        \n",
    "        \"\"\" Medium: Remove Sigmoid Layer for WGAN. \"\"\"        \n",
    "        self.ls = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, dim, 5, 2, 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            conv_bn_lrelu(dim, dim * 2),\n",
    "            conv_bn_lrelu(dim * 2, dim * 4),\n",
    "            conv_bn_lrelu(dim * 4, dim * 8),\n",
    "            nn.Conv2d(dim * 8, 1, 4),\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.ls(x)\n",
    "        y = y.view(-1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools Kit\n",
    "\n",
    "Common tools kit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed\n",
    "\n",
    "Fix random seed for reproduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device\n",
    "\n",
    "Get running device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    return \"cuda\" if torch.cuda.is_available() \\\n",
    "      else \"mps\" if torch.backends.mps.is_available() \\\n",
    "      else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load/Save Checkpoint\n",
    "\n",
    "Class that handles loading/saving checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ModelSaveLoad:\n",
    "    def __init__(self, save_path, load_path):\n",
    "        self.save_path = save_path\n",
    "        self.load_path = load_path\n",
    "\n",
    "    def save(self, modelG_state_dict, modelD_state_dict):\n",
    "        torch.save({\n",
    "            \"modelG_state_dict\" : modelG_state_dict,\n",
    "            \"modelD_state_dict\" : modelD_state_dict,\n",
    "        }, self.save_path)\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Return checkpoint.\"\"\"\n",
    "        return torch.load(self.load_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from qqdm.notebook import qqdm\n",
    "\n",
    "\n",
    "\"\"\" Medium: n_epoch = 50, n_critic = 5, clip_value = 0.01 \"\"\"\n",
    "def parse_args():\n",
    "    config = {\n",
    "        \"seed\": 1,\n",
    "        \"batch_size\": 64,\n",
    "        \"z_dim\": 100,\n",
    "        \"n_workers\": 0,\n",
    "        \"n_epoch\": 1,\n",
    "        \"n_critic\": 1,\n",
    "        #\"clip_value\": 0.01,\n",
    "        \"lr\": 1e-4,\n",
    "        \"onColab\": False,\n",
    "        \"load_ckpt\": False,\n",
    "        \"workspace_dir_config\": {\n",
    "            \"local\": \"./\",\n",
    "            \"drive\" : \"/content/drive/MyDrive/ML2021_data/hw6/\"\n",
    "        },\n",
    "    }\n",
    "    return config\n",
    "\n",
    "def main(\n",
    "        seed,\n",
    "        batch_size,\n",
    "        z_dim,\n",
    "        n_workers,\n",
    "        n_epoch,\n",
    "        n_critic,\n",
    "        #clip_value,\n",
    "        lr,\n",
    "        workspace_dir_config,\n",
    "        onColab,\n",
    "        load_ckpt\n",
    "):\n",
    "    device = get_device()\n",
    "\n",
    "    if onColab:\n",
    "        workspace_dir = workspace_dir_config['drive']\n",
    "    else:\n",
    "        workspace_dir = workspace_dir_config['local']\n",
    "\n",
    "    log_dir = os.path.join(workspace_dir, \"logs\")\n",
    "    ckpt_dir = os.path.join(workspace_dir, \"checkpoints\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    ckpt_path = os.path.join(ckpt_dir, \"model.ckpt\")\n",
    "    modelSL = ModelSaveLoad(ckpt_path, ckpt_path)\n",
    "\n",
    "    # dataset/dataloader\n",
    "    data_dir = os.path.join(workspace_dir, \"data/faces\")\n",
    "    dataset = get_dataset(data_dir)\n",
    "    dataloader = get_dataloader(dataset, batch_size, n_workers)\n",
    "\n",
    "    # model\n",
    "    modelG = Generator(in_dim=z_dim)\n",
    "    modelD = Discriminator(3)\n",
    "    if load_ckpt:\n",
    "        checkpoint = modelSL.load()\n",
    "        modelG.load_state_dict(checkpoint['modelG_state_dict'])\n",
    "        modelD.load_state_dict(checkpoint['modelD_state_dict'])\n",
    "    modelG.to(device)\n",
    "    modelD.to(device)\n",
    "\n",
    "    # loss\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # optimizer\n",
    "    optG = torch.optim.Adam(modelG.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optD = torch.optim.Adam(modelD.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \"\"\" Medium: Use RMSprop for WGAN. \"\"\"\n",
    "    #optG = torch.optim.RMSprop(modelG.parameters(), lr=lr)\n",
    "    #optD = torch.optim.RMSprop(modelD.parameters(), lr=lr)\n",
    "\n",
    "    # distribution samples for generator to produce images\n",
    "    z_sample = torch.randn((100, z_dim)).to(device)\n",
    "    \n",
    "    # training\n",
    "    modelG.train()\n",
    "    modelD.train()\n",
    "    for e, epoch in enumerate(range(n_epoch)):\n",
    "        progress_bar = qqdm(dataloader)\n",
    "        for i, data in enumerate(progress_bar):\n",
    "            imgs = data.to(device)\n",
    "\n",
    "            # batch size of imgs\n",
    "            bs = imgs.size(0)\n",
    "            \n",
    "            # ============================================\n",
    "            # Train Discriminator\n",
    "            # ============================================\n",
    "            z = torch.randn(bs, z_dim).to(device)\n",
    "            r_imgs = imgs\n",
    "            f_imgs = modelG(z)\n",
    "\n",
    "            \"\"\" Medium: Use WGAN Loss. \"\"\"\n",
    "            # label\n",
    "            r_label = torch.ones((bs)).to(device)\n",
    "            f_label = torch.zeros((bs)).to(device)\n",
    "\n",
    "            # model forwarding\n",
    "            r_logits = modelD(r_imgs)\n",
    "            f_logits = modelD(f_imgs)\n",
    "\n",
    "            # compute the loss for the discriminator\n",
    "            r_loss = criterion(r_logits, r_label)\n",
    "            f_loss = criterion(f_logits, f_label)\n",
    "            loss_D = (r_loss + f_loss) / 2\n",
    "            # WGAN Loss\n",
    "            #loss_D = -torch.mean(modelD(r_imgs)) + torch.mean(modelD(f_imgs))\n",
    "\n",
    "            # model backward\n",
    "            modelD.zero_grad()\n",
    "            loss_D.backward()\n",
    "\n",
    "            # update discriminator\n",
    "            optD.step()\n",
    "\n",
    "            \"\"\" Medium: Clip weights of Discriminator. \"\"\"\n",
    "            #for p in modelD.parameters():\n",
    "            #    p.data.clamp_(-clip_value, clip_value)\n",
    "            \n",
    "            # ============================================\n",
    "            # Train Generator\n",
    "            # ============================================\n",
    "            # Viewing Generator + Discriminator as a compund model.\n",
    "            if i % n_critic == 0:\n",
    "                # generate some fake images\n",
    "                z = torch.randn((bs, z_dim)).to(device)\n",
    "                f_imgs = modelG(z)\n",
    "\n",
    "                # Discriminator score images from Generator\n",
    "                f_logits = modelD(f_imgs)\n",
    "\n",
    "                \"\"\" Mediun: Use WGAN Loss. \"\"\"\n",
    "                # compute the loss for the generator (infact is the loss of generator + discriminator)\n",
    "                loss_G = criterion(f_logits, r_label)\n",
    "                # WGAN Loss\n",
    "                #loss_G = -torch.mean(modelD(f_imgs))\n",
    "\n",
    "                # model backward\n",
    "                modelG.zero_grad()\n",
    "                loss_G.backward()\n",
    "\n",
    "                # update generator\n",
    "                optG.step()\n",
    "        \n",
    "            # set info for the progress bar\n",
    "            #   Note that the value of GAN loss is not directly related to\n",
    "            #   the quality of the generated images.\n",
    "            progress_bar.set_infos({\n",
    "                'Loss_D' : round(loss_D.item(), 4),\n",
    "                'Loss_G' : round(loss_G.item(), 4),\n",
    "                'Epoch' : e+1,\n",
    "                'Step' : i+1\n",
    "            })\n",
    "\n",
    "        modelG.eval()\n",
    "        f_imgs_sample = (modelG(z_sample).data + 1) / 2.0\n",
    "        filename = os.path.join(log_dir, f'Epoch_{epoch+1:03d}.jpg')\n",
    "        torchvision.utils.save_image(f_imgs_sample, filename, nrow=10)\n",
    "        print(f' | Save some samples to {filename}.')\n",
    "\n",
    "        # show generated images in jupyter notebook\n",
    "        grid_img = torchvision.utils.make_grid(f_imgs_sample.cpu(), nrow=10)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(grid_img.permute(1, 2, 0))\n",
    "        plt.show()\n",
    "        modelG.train()\n",
    "\n",
    "        if (e+1) % 5 == 0 or e == 0:\n",
    "            # save checkpoints\n",
    "            modelSL.save(modelG_state_dict=modelG.state_dict(), modelD_state_dict=modelD.state_dict())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(**parse_args())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    config = {\n",
    "        \"n_imgs\": 100,\n",
    "        \"z_dim\": 100,\n",
    "        \"onColab\": False,\n",
    "        \"workspace_dir_config\": {\n",
    "            \"local\": \"./\",\n",
    "            \"drive\" : \"/content/drive/MyDrive/ML2021/hw6/\"\n",
    "        }\n",
    "    }\n",
    "    return config\n",
    "\n",
    "def main(\n",
    "        n_imgs,\n",
    "        z_dim,\n",
    "        onColab,\n",
    "        workspace_dir_config\n",
    "):\n",
    "    device = get_device()\n",
    "\n",
    "    if onColab:\n",
    "        workspace_dir = workspace_dir_config['drive']\n",
    "    else:\n",
    "        workspace_dir = workspace_dir_config['local']\n",
    "\n",
    "    ckpt_dir = os.path.join(workspace_dir, 'checkpoints')\n",
    "    checkpoint = torch.load(os.path.join(ckpt_dir, 'model.ckpt'))\n",
    "    G = Generator(z_dim)\n",
    "    G.load_state_dict(checkpoint['modelG_state_dict'])\n",
    "    G.to(device)\n",
    "    G.eval()\n",
    "\n",
    "    # generate n_imgs images and make a grid to save them\n",
    "    z_sample = torch.randn((n_imgs, z_dim)).to(device)\n",
    "    imgs_sample = (G(z_sample).data + 1) / 2.0\n",
    "    log_dir = os.path.join(workspace_dir, 'logs')\n",
    "    filename = os.path.join(log_dir, 'result.jpg')\n",
    "    torchvision.utils.save_image(imgs_sample, filename, nrow=10)\n",
    "\n",
    "    # show some images\n",
    "    n_display = min(int(n_imgs / 10), 32)\n",
    "    grid_img = torchvision.utils.make_grid(imgs_sample[:n_display].cpu(), nrow=10)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(**parse_args())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complexa_ML21_3a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
