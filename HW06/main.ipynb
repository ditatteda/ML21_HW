{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW6 GAN\n",
    "\n",
    "Structure:\n",
    "\n",
    "- Colab (Do this block if is running on `Colab`)\n",
    "\n",
    "- Dataset & Dataloader\n",
    "\n",
    "- Model\n",
    "\n",
    "- Training\n",
    "\n",
    "- Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab\n",
    "\n",
    "Mount drive and extract data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('/content/data', exist_ok=True)\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import shutil\n",
    "shutil.copyfile('/content/drive/MyDrive/ML2021/hw6/data/crypko_data.zip','/content/data/crypko_data.zip')\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile('/content/data/crypko_data.zip','r') as zip_ref:\n",
    "  zip_ref.extractall('/content/data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install qqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset/Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "1. Resize images to (64, 64).\n",
    "2. Linearly map values from [0, 1] to [-1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CrypkoDataset(Dataset):\n",
    "    def __init__(self, fnames, transform):\n",
    "        self.transform = transform\n",
    "        self.fnames = fnames\n",
    "        self.n_samples = len(self.fnames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.fnames[idx]\n",
    "        # load image\n",
    "        img = torchvision.io.read_image(fname)\n",
    "        # image transform\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "def get_dataset(root):\n",
    "    # get paths of images\n",
    "    fnames = glob.glob(os.path.join(root, \"*\"))\n",
    "    compose = [\n",
    "        v2.ToPILImage(),\n",
    "        # 1. Resize image to (64, 64)\n",
    "        v2.Resize((64, 64)),\n",
    "        # map values to [0, 1]\n",
    "        v2.ToTensor(),\n",
    "        # 2. Linearly map values to [-1, 1]\n",
    "        v2.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "    ]\n",
    "    transform = v2.Compose(compose)\n",
    "    dataset = CrypkoDataset(fnames, transform)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Some Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = get_dataset(\"./data/faces\")\n",
    "\n",
    "print(\"values in [-1, 1]:\")\n",
    "images = [dataset[i] for i in range(16)]\n",
    "grid_img = torchvision.utils.make_grid(images, nrow=4)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(grid_img.permute(1,2,0))\n",
    "plt.show()\n",
    "\n",
    "print(\"values in [0, 1]:\")\n",
    "images_origin = [(dataset[i]+1)/2 for i in range(16)]\n",
    "grid_img_origin = torchvision.utils.make_grid(images_origin, nrow=4)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(grid_img_origin.permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def get_dataloader(dataset, batch_size, n_workers):\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=n_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "`N` of the input/output shape stands for batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0.0)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Input shape: (N, in_dim)\n",
    "    Output shape: (N, 3, 64, 64)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, dim=64):\n",
    "        super().__init__()\n",
    "\n",
    "        def dconv_bn_relu(in_dim, out_dim):\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_dim, out_dim, 5, 2,\n",
    "                                   padding=2, output_padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(in_dim, dim * 8 * 4 * 4, bias=False),\n",
    "            nn.BatchNorm1d(dim * 8 * 4 * 4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.l2_5 = nn.Sequential(\n",
    "            dconv_bn_relu(dim * 8, dim * 4),\n",
    "            dconv_bn_relu(dim * 4, dim * 2),\n",
    "            dconv_bn_relu(dim * 2, dim),\n",
    "            nn.ConvTranspose2d(dim, 3, 5, 2, padding=2, output_padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.l1(x)\n",
    "        y = y.view(y.size(0), -1, 4, 4)\n",
    "        y = self.l2_5(y)\n",
    "        return y\n",
    "    \n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Input shape: (N, 3, 64, 64)\n",
    "    Output shape: (N, )\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, dim=64):\n",
    "        super().__init__()\n",
    "\n",
    "        def conv_bn_lrelu(in_dim, out_dim):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_dim, out_dim, 5, 2, 2),\n",
    "                nn.BatchNorm2d(out_dim),\n",
    "                nn.LeakyReLU(0.2)\n",
    "            )\n",
    "        \n",
    "        \"\"\" Medium: Remove Sigmoid Layer for WGAN. \"\"\"        \n",
    "        self.ls = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, dim, 5, 2, 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            conv_bn_lrelu(dim, dim * 2),\n",
    "            conv_bn_lrelu(dim * 2, dim * 4),\n",
    "            conv_bn_lrelu(dim * 4, dim * 8),\n",
    "            nn.Conv2d(dim * 8, 1, 4),\n",
    "            #nn.Sigmoid() \n",
    "        )\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.ls(x)\n",
    "        y = y.view(-1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools Kit\n",
    "\n",
    "Common tools kit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "#import datetime\n",
    "#from pathlib import Path\n",
    "\n",
    "def set_logger(log_name : str):\n",
    "    #\"\"\"write log under given log_path\"\"\"\n",
    "    #log_dir = Path(\"./log\")\n",
    "    #log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    #log_path = log_dir / f\"{datetime.datetime.now().date()}.log\"\n",
    "\n",
    "    logger = logging.getLogger(log_name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formater_s = logging.Formatter(\"%(name)s [%(levelname)s] %(message)s\")\n",
    "    #formater_f = logging.Formatter(\"%(asctime)s - %(name)s [%(levelname)s] %(message)s\")\n",
    "    \n",
    "    if not logger.handlers:\n",
    "        ## logging to file\n",
    "        #file_handler = logging.FileHandler(log_path)\n",
    "        #file_handler.setFormatter(formater_f)\n",
    "        #logger.addHandler(file_handler)\n",
    "\n",
    "        # logging to console\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        stream_handler.setFormatter(formater_s)\n",
    "        logger.addHandler(stream_handler)\n",
    "    \n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed\n",
    "\n",
    "Fix random seed for reproduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device\n",
    "\n",
    "Get running device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    return \"cuda\" if torch.cuda.is_available() \\\n",
    "      else \"mps\" if torch.backends.mps.is_available() \\\n",
    "      else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Save/Load Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ModelSaveLoad:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "    def save(self, G_state_dict, D_state_dict, epoch):\n",
    "        torch.save({\n",
    "            \"G_state_dict\" : G_state_dict,\n",
    "            \"D_state_dict\" : D_state_dict,\n",
    "            \"epoch\" : epoch\n",
    "        }, self.path)\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Return checkpoint.\"\"\"\n",
    "        return torch.load(self.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from qqdm.notebook import qqdm\n",
    "\n",
    "\n",
    "\"\"\" Medium: n_epoch = 50, n_critic = 5, clip_value = 0.01 \"\"\"\n",
    "def parse_args():\n",
    "    config = {\n",
    "        \"seed\": 2021,\n",
    "        \"batch_size\": 64,\n",
    "        \"z_dim\": 100,\n",
    "        \"n_workers\": 0,\n",
    "        \"n_epoch\": 50,\n",
    "        \"n_critic\": 5,\n",
    "        \"clip_value\": 0.01,\n",
    "        \"lr\": 1e-4,\n",
    "        \"onColab\": False,\n",
    "        \"load_ckpt\": False,\n",
    "        \"workspace_dir\": \"./\",\n",
    "        \"ckpt_dir_config\": {\n",
    "            \"local\": \"./checkpoints\",\n",
    "            \"drive\" : \"/content/drive/MyDrive/ML2021/hw6/checkpoints\"\n",
    "        },\n",
    "    }\n",
    "    return config\n",
    "\n",
    "def main(\n",
    "        seed,\n",
    "        batch_size,\n",
    "        z_dim,\n",
    "        n_workers,\n",
    "        n_epoch,\n",
    "        n_critic,\n",
    "        clip_value,\n",
    "        lr,\n",
    "        ckpt_dir_config,\n",
    "        onColab,\n",
    "        load_ckpt,\n",
    "        workspace_dir\n",
    "):\n",
    "    logger = set_logger('GAN')\n",
    "\n",
    "    device = get_device()\n",
    "    logger.info(f'{device} is used.')\n",
    "\n",
    "    if onColab:\n",
    "        ckpt_dir = ckpt_dir_config['drive']\n",
    "    else:\n",
    "        ckpt_dir = ckpt_dir_config['local']\n",
    "    log_dir = os.path.join(workspace_dir, \"logs\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    # dataset/dataloader\n",
    "    data_dir = os.path.join(workspace_dir, \"data/faces\")\n",
    "    dataset = get_dataset(data_dir)\n",
    "    dataloader = get_dataloader(dataset, batch_size, n_workers)\n",
    "    logger.info('Data loaded.')\n",
    "\n",
    "    # parameter\n",
    "    pre_epoch = 0\n",
    "\n",
    "    # model\n",
    "    SL_handler = ModelSaveLoad(os.path.join(ckpt_dir, 'model.ckpt'))\n",
    "    G = Generator(in_dim=z_dim)\n",
    "    D = Discriminator(3)\n",
    "    if load_ckpt:\n",
    "        checkpoint = SL_handler.load()\n",
    "        G.load_state_dict(checkpoint['G_state_dict'])\n",
    "        D.load_state_dict(checkpoint['D_state_dict'])\n",
    "        pre_epoch = checkpoint['epoch']\n",
    "        logger.info(f'Checkpoint loaded. | Epoch: {pre_epoch}.')\n",
    "    else:\n",
    "        logger.info('New model set.')\n",
    "    G.to(device)\n",
    "    D.to(device)\n",
    "\n",
    "    ## loss\n",
    "    #criterion = nn.BCELoss()\n",
    "    #logger.info('Criterion set.')\n",
    "    ## optimizer\n",
    "    #optG = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    #optD = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    #logger.info('Optimizer set.')\n",
    "\n",
    "    \"\"\" Medium: Use RMSprop for WGAN. \"\"\"\n",
    "    optG = torch.optim.RMSprop(G.parameters(), lr=lr)\n",
    "    optD = torch.optim.RMSprop(D.parameters(), lr=lr)\n",
    "    logger.info('RMSprop set.')\n",
    "\n",
    "    # distribution samples for generator to produce images\n",
    "    z_sample = torch.randn((100, z_dim)).to(device)\n",
    "    \n",
    "    # training\n",
    "    G.train()\n",
    "    D.train()\n",
    "    logger.info('================ Start Training ================')\n",
    "    for epoch in range(pre_epoch, n_epoch):\n",
    "        progress_bar = qqdm(dataloader)\n",
    "        for i, data in enumerate(progress_bar):\n",
    "            imgs = data.to(device)\n",
    "\n",
    "            # batch size of imgs\n",
    "            bs = imgs.size(0)\n",
    "            \n",
    "            # ============================================\n",
    "            # Train Discriminator\n",
    "            # ============================================\n",
    "            z = torch.randn(bs, z_dim).to(device)\n",
    "            r_imgs = imgs\n",
    "            f_imgs = G(z)\n",
    "\n",
    "            \"\"\" Medium: Use WGAN Loss. \"\"\"\n",
    "            ## label\n",
    "            #r_label = torch.ones((bs)).to(device)\n",
    "            #f_label = torch.zeros((bs)).to(device)\n",
    "\n",
    "            ## model forwarding\n",
    "            #r_logits = D(r_imgs)\n",
    "            #f_logits = D(f_imgs)\n",
    "\n",
    "            ## compute the loss for the discriminator\n",
    "            #r_loss = criterion(r_logits, r_label)\n",
    "            #f_loss = criterion(f_logits, f_label)\n",
    "            #loss_D = (r_loss + f_loss) / 2\n",
    "\n",
    "            # WGAN Loss\n",
    "            loss_D = -torch.mean(D(r_imgs)) + torch.mean(D(f_imgs))\n",
    "\n",
    "            # model backward\n",
    "            D.zero_grad()\n",
    "            loss_D.backward()\n",
    "\n",
    "            # update discriminator\n",
    "            optD.step()\n",
    "\n",
    "            \"\"\" Medium: Clip weights of Discriminator. \"\"\"\n",
    "            for p in D.parameters():\n",
    "                p.data.clamp_(-clip_value, clip_value)\n",
    "            \n",
    "            # ============================================\n",
    "            # Train Generator\n",
    "            # ============================================\n",
    "            # Viewing Generator + Discriminator as a compund model.\n",
    "            if i % n_critic == 0:\n",
    "                # generate some fake images\n",
    "                z = torch.randn((bs, z_dim)).to(device)\n",
    "                f_imgs = G(z)\n",
    "\n",
    "                # Discriminator score images from Generator\n",
    "                #f_logits = D(f_imgs)\n",
    "\n",
    "                \"\"\" Mediun: Use WGAN Loss. \"\"\"\n",
    "                # compute the loss for the generator (infact is the loss of generator + discriminator)\n",
    "                #loss_G = criterion(f_logits, r_label)\n",
    "                # WGAN Loss\n",
    "                loss_G = -torch.mean(D(f_imgs))\n",
    "\n",
    "                # model backward\n",
    "                G.zero_grad()\n",
    "                loss_G.backward()\n",
    "\n",
    "                # update generator\n",
    "                optG.step()\n",
    "        \n",
    "            # set info for the progress bar\n",
    "            #   Note that the value of GAN loss is not directly related to\n",
    "            #   the quality of the generated images.\n",
    "            progress_bar.set_infos({\n",
    "                'Loss_D' : round(loss_D.item(), 4),\n",
    "                'Loss_G' : round(loss_G.item(), 4),\n",
    "                'Epoch' : epoch+1,\n",
    "                'Step' : i+1\n",
    "            })\n",
    "\n",
    "        G.eval()\n",
    "        f_imgs_sample = (G(z_sample).data + 1) / 2.0\n",
    "        filename = os.path.join(log_dir, f'Epoch_{epoch+1:03d}.jpg')\n",
    "        torchvision.utils.save_image(f_imgs_sample, filename, nrow=10)\n",
    "        print(f' | Save some samples to {filename}.')\n",
    "\n",
    "        # show generated images in jupyter notebook\n",
    "        grid_img = torchvision.utils.make_grid(f_imgs_sample.cpu(), nrow=10)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(grid_img.permute(1, 2, 0))\n",
    "        plt.show()\n",
    "        G.train()\n",
    "\n",
    "        if (epoch+1) % 5 == 0 or epoch == 0:\n",
    "            # save checkpoints\n",
    "            SL_handler.save(G.state_dict(), D.state_dict(), epoch)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(**parse_args())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    config = {\n",
    "        \"n_imgs\": 100,\n",
    "        \"z_dim\": 100,\n",
    "        \"onColab\": False,\n",
    "        \"workspace_dir\": \"./\",\n",
    "        \"ckpt_dir_config\": {\n",
    "            \"local\": \"./checkpoints\",\n",
    "            \"drive\" : \"/content/drive/MyDrive/ML2021/hw6/checkpoints\"\n",
    "        }\n",
    "    }\n",
    "    return config\n",
    "\n",
    "def main(\n",
    "        n_imgs,\n",
    "        z_dim,\n",
    "        onColab,\n",
    "        ckpt_dir_config,\n",
    "        workspace_dir\n",
    "):\n",
    "    device = get_device()\n",
    "\n",
    "    if onColab:\n",
    "        ckpt_dir = ckpt_dir_config['drive']\n",
    "    else:\n",
    "        ckpt_dir = ckpt_dir_config['local']\n",
    "\n",
    "    G = Generator(z_dim)\n",
    "    checkpoint = torch.load(os.path.join(ckpt_dir, 'model.ckpt'))\n",
    "    G.load_state_dict(checkpoint['G_state_dict'])\n",
    "    G.to(device)\n",
    "    G.eval()\n",
    "\n",
    "    # generate n_imgs images and make a grid to save them\n",
    "    z_sample = torch.randn((n_imgs, z_dim)).to(device)\n",
    "    imgs_sample = (G(z_sample).data + 1) / 2.0\n",
    "    log_dir = os.path.join(workspace_dir, 'logs')\n",
    "    filename = os.path.join(log_dir, 'result.jpg')\n",
    "    torchvision.utils.save_image(imgs_sample, filename, nrow=10)\n",
    "\n",
    "    # show some images\n",
    "    n_display = min(int(n_imgs / 10), 32)\n",
    "    grid_img = torchvision.utils.make_grid(imgs_sample[:n_display].cpu(), nrow=10)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(**parse_args())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complexa_ML21_3a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
